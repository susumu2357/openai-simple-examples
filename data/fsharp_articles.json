[
    {
        "title": "What Exactly Are the Dangers Posed by A.I.?",
        "text": "A neural network is a mathematical system that learns skills by analyzing data. About five years ago, companies like Google, Microsoft and OpenAI began building neural networks that learned from huge amounts of digital text called large language models, or L.L.M.s. By pinpointing patterns in that text, L.L.M.s learn to generate text on their own, including blog posts, poems and computer programs. They can even carry on a conversation. This technology can help computer programmers, writers and other workers generate ideas and do things more quickly. But Dr. Bengio and other experts also warned that L.L.M.s can learn unwanted and unexpected behaviors. These systems can generate untruthful, biased and otherwise toxic information. Systems like GPT-4 get facts wrong and make up information, a phenomenon called “hallucination.” Companies are working on these problems. But experts like Dr. Bengio worry that as researchers make these systems more powerful, they will introduce new risks.",
        "publish_date": "2023-05-01 13:12:57"
    },
    {
        "title": "‘The Godfather of AI’ Leaves Google and Warns of Danger Ahead",
        "text": "Written by Cade Metz Geoffrey Hinton was an artificial intelligence pioneer. In 2012, Hinton and two of his graduate students at the University of Toronto created technology that became the intellectual foundation for the AI systems that the tech industry’s biggest companies believe is a key to their future. On Monday, however, he officially joined a growing chorus of critics who say those companies are racing toward danger with their aggressive campaign to create products based on generative AI, the technology that powers popular chatbots like ChatGPT. Hinton said he has quit his job at Google, where he has worked for more than decade and became one of the most respected voices in the field, so he can freely speak out about the risks of AI. A part of him, he said, now regrets his life’s work. “I console myself with the normal excuse: If I hadn’t done it, somebody else would have,” Hinton said during a lengthy interview last week in the dining room of his home in Toronto, a short walk from where he and his students made their breakthrough. Hinton’s journey from AI groundbreaker to doomsayer marks a remarkable moment for the technology industry at perhaps its most important inflection point in decades. Industry leaders believe the new AI systems could be as important as the introduction of the web browser in the early 1990s and could lead to breakthroughs in areas ranging from drug research to education. But gnawing at many industry insiders is a fear that they are releasing something dangerous into the wild. Generative AI can already be a tool for misinformation. Soon, it could be a risk to jobs. Somewhere down the line, tech’s biggest worriers say, it could be a risk to humanity. “It is hard to see how you can prevent the bad actors from using it for bad things,” Hinton said. After the San Francisco startup OpenAI released a new version of ChatGPT in March, more than 1,000 technology leaders and researchers signed an open letter calling for a six-month moratorium on the development of new systems because AI technologies pose “profound risks to society and humanity.” Several days later, 19 current and former leaders of the Association for the Advancement of Artificial Intelligence, a 40-year-old academic society, released their own letter warning of the risks of AI. That group included Eric Horvitz, chief scientific officer at Microsoft, which has deployed OpenAI’s technology across a wide range of products, including its Bing search engine. Hinton, often called “the Godfather of AI,” did not sign either of those letters and said he did not want to publicly criticize Google or other companies until he had quit his job. He notified the company last month that he was resigning, and Thursday, he talked by phone with Sundar Pichai, CEO of Google’s parent company, Alphabet. He declined to publicly discuss the details of his conversation with Pichai. Google’s chief scientist, Jeff Dean, said in a statement: “We remain committed to a responsible approach to AI. We’re continually learning to understand emerging risks while also innovating boldly.” Hinton, a 75-year-old British expatriate, is a lifelong academic whose career was driven by his personal convictions about the development and use of AI. In 1972, as a graduate student at the University of Edinburgh, Hinton embraced an idea called a neural network. A neural network is a mathematical system that learns skills by analyzing data. At the time, few researchers believed in the idea. But it became his life’s work. In the 1980s, Hinton was a professor of computer science at Carnegie Mellon University but left the university for Canada because he said he was reluctant to take Pentagon funding. At the time, most AI research in the United States was funded by the Defense Department. Hinton is deeply opposed to the use of AI on the battlefield — what he calls “robot soldiers.” In 2012, Hinton and two of his students in Toronto, Ilya Sutskever and Alex Krishevsky, built a neural network that could analyze thousands of photos and teach itself to identify common objects, such as flowers, dogs and cars. Google spent $44 million to acquire a company started by Hinton and his two students. And their system led to the creation of increasingly powerful technologies, including new chatbots such as ChatGPT and Google Bard. Sutskever went on to become chief scientist at OpenAI. In 2018, Hinton and two other longtime collaborators received the Turing Award, often called “the Nobel Prize of computing,” for their work on neural networks. Around the same time, Google, OpenAI and other companies began building neural networks that learned from huge amounts of digital text. Hinton thought it was a powerful way for machines to understand and generate language, but it was inferior to the way humans handled language. Then, last year, as Google and OpenAI built systems using much larger amounts of data, his view changed. He still believed the systems were inferior to the human brain in some ways but he thought they were eclipsing human intelligence in others. “Maybe what is going on in these systems,” he said, “is actually a lot better than what is going on in the brain.” As companies improve their AI systems, he believes, they become increasingly dangerous. “Look at how it was five years ago and how it is now,” he said of AI technology. “Take the difference and propagate it forwards. That’s scary.” Until last year, he said, Google acted as a “proper steward” for the technology, careful not to release something that might cause harm. But now that Microsoft has augmented its Bing search engine with a chatbot — challenging Google’s core business — Google is racing to deploy the same kind of technology. The tech giants are locked in a competition that might be impossible to stop, Hinton said. His immediate concern is that the internet will be flooded with false photos, videos and text, and the average person will “not be able to know what is true anymore.” He is also worried that AI technologies will in time upend the job market. Today, chatbots such as ChatGPT tend to complement human workers, but they could replace paralegals, personal assistants, translators and others who handle rote tasks. “It takes away the drudge work,” he said. “It might take away more than that.” Down the road, he is worried that future versions of the technology pose a threat to humanity because they often learn unexpected behavior from the vast amounts of data they analyze. This becomes an issue, he said, as individuals and companies allow AI systems not only to generate their own computer code but actually to run that code on their own. And he fears a day when truly autonomous weapons — those killer robots — become reality. “The idea that this stuff could actually get smarter than people — a few people believed that,” he said. “But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.” Many other experts, including many of his students and colleagues, say this threat is hypothetical. But Hinton believes that the race between Google and Microsoft and others will escalate into a global race that will not stop without some sort of global regulation. But that may be impossible, he said. Unlike with nuclear weapons, he said, there is no way of knowing whether companies or countries are working on the technology in secret. The best hope is for the world’s leading scientists to collaborate on ways of controlling the technology. “I don’t think they should scale this up more until they have understood whether they can control it,” he said. Hinton said that when people used to ask him how he could work on technology that was potentially dangerous, he would paraphrase Robert Oppenheimer, who led the U.S. effort to build the atomic bomb: “When you see something that is technically sweet, you go ahead and do it.” He does not say that anymore.",
        "publish_date": "2023-05-01 14:22:59"
    },
    {
        "title": "ChatGPT fails: 13 common errors and mistakes you need to know",
        "text": "I’ve written about ways to use ChatGPT for SEO and content marketing-related tasks, like keyword research, crafting title tags and local SEO. I use it at least once daily for a variety of tasks. The more you use the platform, the more valuable it will be to you to understand its limitations. The founder of OpenAI (the company that built ChatGPT) has called the tool “incredibly limited” and a “horrible product” and has cautioned users about how much they rely on it: Multiple publications have published content with errors on meaningful YMYL (“Your Money or Your Life”) topics like personal finance and health. This article discusses the most common technical errors and the frequent mistakes when using ChatGPT and how to address them. Common ChatGPT errors If you’ve been using ChatGPT for a “long” time (i.e., since late 2022), you’ve probably seen the prompt below at some point. Obviously, a new software product in an emerging technology field that’s massively popular will run into problems. If you’re making ChatGPT an integral part of your daily workflow, knowing the common issues you may encounter is helpful. ChatGPT also has issues familiar to any website, such as 404 not found errors, 5xx internal server errors, 403 access denied errors, and Cloudflare errors like 1020 and 524 errors. Often these can be fixed by clearing your cache, using (or turning off) a VPN, or ensuring you’re typing the web address correctly. And there are also several errors more specific to the platform: 1. API issues Often, users may hit specific character limits, token limits, or limits in functionality with the ChatGPT API. 2. Character limits There are limits to how many characters you can enter into ChatGPT and how many characters ChatGPT responses can be. They vary depending on your version (e.g., GPT-3.5 has an input limit of 2048, and GPT-4 is 4,096) and in the web interface versus the API. 3. Connection time out on long responses If ChatGPT is processing a long and complex response, it will sometimes just time out. You might see this when asking the platform to perform functions like writing code or analyzing and responding to long inputs. 4. Unfinished responses Not an error per se, but frequently, if you ask ChatGPT to write code or a long text, it will just stop mid-way. You can type continue, and it will finish within the next few prompts in these instances. 5. Limited messages ChatGPT, at the time of writing, limits the number of messages you can use per time period, even for paid accounts. Similarly, there is specific rate-limiting for different GPT versions, OpenAI tools, and account types. 6. Login issues Depending on the level of access required or session duration. ChatGPT may log you out, or you may not have access to certain features with certain account statuses. Most of these issues can be resolved by: Common ChatGPT mistakes ChatGPT makes many mistakes, including: Other users have detailed instances of the platform lying to get a CAPTCHA solved and issues with simple math problems related to age and distance. Some of this has improved with GPT-4, but not all. Here is a prompt and output using the latest version of ChatGPT: While it’s fun to do “gotcha” prompts and catch ChatGPT saying incorrect or goofy things, it’s more beneficial to be aware of areas where ChatGPT can be wrong – the “categories of wrongness,” if you will. Understanding these will make you both a better prompt engineer and output editor. Let's walk through the most common categories of mistakes I know. 7. Confident but wrong The “hallucination” issue is a significant problem for ChatGPT and other generative AI solutions. ChatGPT will answer all kinds of questions that it doesn't know the answer to in a compelling way. That doesn't make the answer right. The example screenshot above shows that it doesn't use any qualifying language or hedge at all. You get a decisive answer without equivocating. 8. Not understanding word and character counts ChatGPT is terrible at estimating and understanding character counts in any precise way. If you’re doing work related to SEO and content, this is a major limiting factor. You might be looking to write title tags, meta descriptions, or content where meeting a word count is necessary: This is actually pretty close. I've had instances where the tool was much farther off. 9. Mistakes in reasoning I searched for “logic puzzles” and grabbed the first list of “easy logic puzzles” I found. ChatGPT got the first three puzzles correct, then the fourth one wrong: 10. Coding errors There are several real-life examples now of people using ChatGPT to build websites, apps and browser extensions or troubleshoot coding issues. That said, ChatGPT can also generate code that doesn't work. I've encountered the following issues when using ChatGPT to help code: Again, it’s imperative you always check the code you’re deploying carefully and test the output of anything ChatGPT helps with. Some users have also reported that GPT- 4 or even legacy mode (rather than the new default, Turbo) leads to better results for coding tasks. 11. Outdated information The training data for ChatGPT may be updated at some point and various plugins and tools have started to integrate web crawling and real-time data. Still, the web version of ChatGPT currently can't crawl the web and isn’t aware of information from before late 2021. This is important, particularly when coupled with the “confident but wrong” issue. 12. Short memory There is a limit to how many characters within a chat ChatGPT will “remember” so it will frequently make mistakes due to losing the context of your early prompts or its early answers. 13. Formatting mistakes As with coding, if you ask ChatGPT to perform functions like creating a table or formatting text in a certain way, it will sometimes make mistakes or ignore specific instructions. Avoiding and addressing ChatGPT's mistakes So how can you get the most out of ChatGPT while dealing with the platform’s issues and limitations? Focusing on these key areas can help you avoid much of the potential downside of using ChatGPT in your day-to-day tasks: Be aware of limitations Staying up to date with specific product limitations like (i.e., character limits for prompts and responses, the date of training data for GPT, etc.) will help you craft prompts that generate good responses and QA outputs better. OpenAI does a good job of updating this information with new releases. Some items here are things they've explicitly said they are addressing. Review and edit outputs carefully This should be fairly obvious, but you must carefully edit any text and QA code before publishing or deploying it. Due to how convincingly confident ChatGPT can be, I'd strongly recommend having a domain expert review content and code before it’s deployed. It’s relatively easy to be fooled by some of the content and assertions put out by ChatGPT, especially if a topic isn’t your area of expertise. But if you have deep experience on a topic, it's easier to catch misleading or inaccurate statements. Layer tools and processes on top of the API OpenAI has rolled out plugins and made a ChatGPT-specific API available. You can address some of the platform’s limitations by using different tools that others create or by creating some yourself. Now that you know the potential pitfalls of ChatGPT, you can go on and establish guardrails for using the tool – leveraging its benefits while avoiding many of the negatives.",
        "publish_date": "2023-05-01 15:00:17"
    },
    {
        "title": "Here's Why The 'Godfather of AI' Just Quit His Job at Google",
        "text": "Geoffrey Hinton, a pioneer of artificial intelligence (A.I.), is raising alarm bells about the danger of A.I. development. Dr. Hinton and his students at the University of Toronto created the technology that became the foundation of A.I. systems in 2012, which tech giants are now racing to develop.However, Dr. Hinton has recently joined the growing chorus of critics warning that companies are creating products based on generative A.I. without fully considering the risks. Generative A.I., which powers popular chatbots like ChatGPT, can already be a tool for misinformation and, somewhere down the line, could be a risk to humanity.Can ChatGPT Be Your Emergency Therapist For A Day? After OpenAI released a new version of ChatGPT in March, over 1,000 technology leaders and researchers signed an open letter calling for a six-month moratorium on the development of new systems. This is because A.I. technologies pose \"profound risks to society and humanity.\"Dr. Hinton, often called \"the Godfather of A.I.\", did not sign either of those letters but has since resigned from his position at Google, where he has worked for over a decade, to speak freely about the risks of A.I. Dr. Hinton is a lifelong academic and his career was driven by his personal convictions about the development and use of A.I.Dr. Hinton's neural network, a mathematical system that learns skills by analyzing data, became his life's work. In 2012, he and his students built a neural network that could analyze thousands of photos and teach itself to identify common objects like flowers, dogs, and cars. Google acquired the company for $44 million and their system led to the creation of powerful technologies, including chatbots like ChatGPT and Google Bard.Here's How Artificial Intelligence Wrote & Directed This Creepy Short Film Dr. Hinton is deeply opposed to the use of artificial intelligence on the battlefield or \"robot soldiers\". He believes that generative A.I. could have negative consequences if not developed responsibly. He says, \"It is hard to see how you can prevent the bad actors from using it for bad things.\"While industry leaders believe that A.I. systems could lead to breakthroughs in areas ranging from drug research to education, critics fear that they are releasing something dangerous into the wild. With the A.I. industry at perhaps its most important inflection point in decades, the debate about its risks and benefits is sure to continue.(At The Quint, we are answerable only to our audience. Play an active role in shaping our journalism by becoming a member. Because the truth is worth it.)",
        "publish_date": "2023-05-01 16:07:51"
    },
    {
        "title": "After Quitting Google, ‘Godfather of AI’ Is Now Warning of Its Dangers",
        "text": "Megalithic tech companies such as Google, Meta, and Microsoft are so obsessed with AI development it seems impossible to steer any of them toward slowing down and actually thinking about the repercussions. Now one of the most prominent faces in artificially intelligence research, former Googler Dr. Geoffrey Hinton, has come down hard on the full-spring pace of AI development, ultimately calling for some kind of global regulation. According to an interview with The New York Times, Hinton, an award-winning researcher on AI, neural networks, and machine learning, is no longer so comfortable pushing the boundaries of AI development without any kind of regulation or stopgap. The 75-year-old Hinton, who was a lead researcher in any aspects of AI development at Google, has come out saying “It is hard to see how you can prevent the bad actors from using [AI] for bad things.” He directly compared himself to Robert Oppenheimer, who helped develop the atomic bomb for the U.S. While Oppenheimer had made statements about pursuing science for sciences sake, Hinton instead said “I don’t think they should scale [AI] up more until they have understood whether they can control it.” He further shared his concerns that AI would lead to massive job disruptions around the world. Hinton got his ‘Godfather’ title not with any offer you can’t refuse, but from decades of research on AI. This came to a head with the neural network he helped build in 2012 with two of his students at the University of Toronto. That network was a machine learning program that could teach itself to identify objects like dogs, flowers, and so on, and it became a major stepping stone for modern transformer-based AI like diffusion AI image generators and large language models. Google had originally acquired the company formed out of Hinton’s Toronto-based research in 2013. This let him establish a Toronto-based element of the Google Brain team overseeing AI development. After that, Google went on an AI spending spree when it acquired deep learning company DeepMind in 2014. Hinton’s company, according to a 2021 Wired report, received numerous offers from tech giants including Microsoft and China-based Baidu, both of which are deep in the muck with their own push into AI development. In a March interview with CBS News, Hinton compared the recent rapid advancements in AI to “the Industrial revolution or electricity—or maybe the wheel.” It’s unclear when Hinton made this heel-turn, but just a few months ago he was instead referring to AI as a “supernaturally precocious child.” He compared AI training to caterpillars feeding on nutrients to become butterflies, further calling OpenAI’s GPT-4 large language model “humanity’s butterfly.” According to the Times, in April Hinton told Google he planned to leave, and finally cut the cord after a call with CEO Sundar Pichai last Thursday. Though The New York Times implied that Hinton had left Google in order to specifically take umbrage with his old boss, the Turing Award winner claimed he only wished to speak up on the dangers of AI, adding “Google has acted very responsibly.” Hinton’s departure comes at a time of massive reorganization at his former company after massive layoffs. Last month, Google announced it was consolidating two of its most major AI teams together. Combining the Google Brain and DeepMind teams into one unit and also reorganized its AI leadership, with Brain lead Jeff Dean being moved to a chief scientist position while DeepMind CEO Demis Hassabis is set to take control of all AI development. So far, the overt calls for stalling AI development have come from outside big tech. In March, hundreds of leading minds and researchers circulated an open letter demanding companies pause advanced AI systems. The letter criticized how major tech companies were locked in an “out-of-control race to develop and deploy ever more powerful digital minds” that nobody could predict or control. Though that’s not to say folks inside these companies don’t have qualms. A recent report from Bloomberg claimed that people inside Google were especially concerned with the company’s Bard AI. Staff said the chatbot was so bad it was constantly providing misinformation and lies to users. Want to know more about AI, chatbots, and the future of machine learning? Check out our full coverage of artificial intelligence, or browse our guides to The Best Free AI Art Generators, The Best ChatGPT Alternatives, and Everything We Know About OpenAI’s ChatGPT.",
        "publish_date": "2023-05-01 16:00:11"
    },
    {
        "title": "'Godfather of AI' quits Google -- and says he regrets life's work due to risks to humanity",
        "text": "A prominent artificial intelligence researcher known as the “Godfather of AI” has quit his job at Google – and says he now partly regrets his work advancing the burgeoning technology because of the risks it poses to society. Dr. Geoffrey Hinton is a renowned computer scientist who is widely credited with laying the AI groundwork that eventually led to the creation of popular chatbots such as OpenAI’s ChatGPT and other advanced systems. The 75-year-old told the New York Times that he left Google so that he can speak openly about the risks of unrestrained AI development – including the spread of misinformation, upheaval in the jobs market and other, more nefarious possibilities. “I console myself with the normal excuse: If I hadn’t done it, somebody else would have,” Hinton said in an interview published on Monday. “Look at how it was five years ago and how it is now,” Hinton added later in the interview. “Take the difference and propagate it forwards. That’s scary.” Hinton fears that AI will only become more dangerous in the future — with “bad actors” potentially exploiting advanced systems “for bad things” that will be difficult to prevent. Hinton informed Google of his plans to resign last month and personally spoke last Thursday with company CEO Sundar Pichai, according to the report. The computer scientist did not reveal what he and Pichai discussed during the phone call. Google’s chief scientist Jeff Dean defended the company’s AI efforts. “We remain committed to a responsible approach to A.I. We’re continually learning to understand emerging risks while also innovating boldly,” Dean said in a statement. The Post has reached out to Google for further comment. Hinton is the latest of a growing number of experts who have warned that AI could cause significant harm without proper oversight and regulation. In March, Elon Musk and more than 1,000 other prominent figures in the AI sector called for a six-month pause in advanced AI development, citing its potential “profound risks to society and humanity.” In the interview, Hinton expressed concern that artificial intelligence has already begun to outpace the human mind in some facets. He also cited concerns that the pace of AI development will increase as Microsoft-backed OpenAI, Google and other tech giants race to lead the field – with potentially dangerous consequences. Hinton fears that advanced AI could eventually spiral out of control as systems gain the ability to create and run their own computer code – or even power weapons without human control. “The idea that this stuff could actually get smarter than people — a few people believed that,” Hinton added. “But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.” In a recent interview with CBS’s “60 Minutes,” Pichai himself warned that AI would cause job losses for “knowledge workers,” such as writers, accountants, architects and software engineers. Pichai also detailed bizarre scenarios in which Google’s AI programs have developed “emergent properties” – or learned unexpected skills in which they were not trained. Since 2013, Hinton had split his time between roles as a professor at the University of Toronto and as a Google engineering fellow. He had worked for the tech giant since Google acquired a startup he co-founded with two students, Alex Krishevsky and Ilya Sutskever. The trio developed a neural network that trained itself to identify common objects, such as cars or animals, by analyzing thousands of photos. Sutskever currently serves as chief scientist for OpenAI. In 2018, Hinton was a joint recipient of the Turing Award – often identified as the computing world’s equivalent of the Nobel Prize – for work on neural networks that was described as “major breakthroughs in artificial intelligence.” A lengthy bio for Hinton on Google’s website lauds his accomplishments – noting he “made major breakthroughs in deep learning that have revolutionized speech recognition and object classification.”",
        "publish_date": "2023-05-01 16:24:15"
    },
    {
        "title": "As Europeans strike first to rein in AI, the US follows",
        "text": "A proposed set of rules by the European Union would, among other things. require makers of generative AI tools such as ChatGPT,to publicize any copyrighted material used by the technology platforms to create content of any kind. A new draft of European Parliament's legislation, a copy of which was attained by The Wall Street Journal, would allow the original creators of content used by generative AI applications to share in any profits that result. The European Union’s “Artificial Intelligence Act” (AI Act) is the first of its kind by a western set of nations. The proposed legislation relies heavily on existing rules, such as the General Data Protection Regulation (GDPR), the Digital Services Act, and the Digital Markets Act. The AI Act was originally proposed by the European Commission in April 2021. The bill’s provisions also require that the large language models (LLMs) behind generative AI tech, such as the GPT-4, be designed with adequate safeguards against generating content that violates EU laws; that could include child pornography or, in some EU countries, denial of the Holocaust, according to The Washington Post. Violations of the AI Act could carry fines of up to 30 million euros or 6% of global profits, whichever is higher. “For a company like Microsoft, which is backing ChatGPT creator OpenAI, it could mean a fine of over $10 billion if found violating the rules,” a Reuters report said. But the solution to keeping AI honest isn't easy, according to Avivah Litan, a vice president and distinguished analyst at Gartner Research. It’s likely that LLM creators, such as San Fransisco-based OpenAI and others, will need to develop powerful LLMs to check that the ones trained initially have no copyrighted materials. Rules-based systems to filter out copyright materials are likely to be ineffective, Liten said. Meanwhile, the EU is busy refining its AI Act and taking a world-leading approach, Litan said, in creating rules that govern the fair and risk-managed use of AI going forward. Regulators should consider that LLMs are effectively operating as a black box, she said, and it's unlikely that the algorithms will provide organizations with the needed transparency to conduct the requisite privacy impact assessment. \"This must be addressed,\" Litan said. \"It’s interesting to note that at one point the AI Act was going to exclude oversight of Generative AI models, but they were included later,” Litan said “Regulators generally want to move carefully and methodically so that they don’t stifle innovation and so that they create long-lasting rules that help achieve the goals of protecting societies without being overly prescriptive in the means.” On April 1, Italy became the first Western nation to ban further development of ChatGPT over privacy concerns; that occurred after the natural language processing app experienced a data breach involving user conversations and payment information. ChatGPT is the popular chatbot created by OpenAI and backed by billions of dollars from Microsoft. Earlier this month, the US and Chinese governments issued announcements related to regulations for AI development, something neither country has established to date. “The US and the EU are aligned in concepts when it comes to wanting to achieve trustworthy, transparent, and fair AI, but their approaches have been very different,” Litan said. So far, the US has taken what Litan called a “very distributed approach to AI risk management,” and it has yet to create new regulations or regulatory infrastructure. The US has focused on guidelines and an AI Risk Management framework. In January, the National Institute of Standards and Technology (NIST) released the Artificial Intelligence Management Framework. In February, the White House issued an Executive Order directing federal agencies to ensure their use of AI advances equity and civil rights. The US Congress is considering the federal Algorithmic Accountability Act, which, if passed, would require employers to perform an impact assessment of any automated decision-making system that has a significant effect on an individual's access to, terms, or availability of employment. The National Telecommunications and Information Administration (NTIA), a branch of the US Department of Commerce, also issued a public request for comment on what policies would best hold AI systems accountable. States and municipalities are getting into the act, too, eyeing local restrictions on the use of AI-based bots to find, screen, interview, and hire job candidates because of privacy and bias issues. Some states have already put laws on the books. Microsoft and Google owner Alphabet have been in a race to bring generative AI chatbots to businesses and consumers. The most advanced generative AI engines can create their own content based on user prompts or input. So, for example, AI can be tasked with creating marketing or ad campaigns, writing essays, and generating realistic photo imagery and videos. Key to the EU’s AI Act is a classification system that determines the level of risk an AI technology could pose to the health and safety or fundamental rights of a person. The framework includes four risk tiers: unacceptable, high, limited, and minimal, according to the World Economic Forum. Issues around generative AI platforms that regulators should be mindful of, according to Gartner, include: GPT models are not explainable: Model outputs are unpredictable; even the model vendors don’t understand everything about how they work internally. Explainability or interpretability are prerequisites for model transparency. Inaccurate and fabricated answers: To mitigate the risks of inaccuracies and hallucinations, output generated by ChatGPT/GenAI should be assessed for accuracy, appropriateness, and actual usefulness before being accepted. Potential compromise of confidential data: No verifiable data governance and protection assurances that confidential enterprise information– for example, in the form of stored prompts — is not compromised. Model and output bias: Model developers and users must have policies or controls in place to detect biased outputs and deal with them consistent with company policy and any relevant legal requirements. Intellectual property (IP) and copyright risks: Model developers and users must scrutinize their output before further use to ensure it doesn’t infringe on copyright or IP rights, and actively monitor changes in copyright laws that apply to ChatGPT/GenAI. Users are now on their own when it comes to filtering out copyrighted materials in ChatGPT outputs. Cyber and fraud risks: Systems should be hardened to try to ensure criminals are not able to use them for cyber and fraud attacks. Launched by OpenAI in November, ChatGPT immediately went viral and had 1 million users in just its first five days because of the sophisticated way it generates in-depth, human-like responses to queries. The ChatGPT website currently receives an estimated 1 billion monthly website visitors with an estimated 100 million active users, according to website test company Tooltester. Though the chatbot’s responses may appear human-like, ChatGPT isn't sentient — it’s a next-word prediction engine, according Dan Diasio, Ernst & Young global artificial intelligence consulting leader. With that in mind, he urged caution in its use. But as AI technology advances at breakneck speed, a more sophisticated algorithm is predicted to be on the horizon: artificial general intelligence, which could think for itself and become exponentially smarter over time. Earlier this month, an open letter from thousands of tech luminaries called for a halt to the development of generative AI technology out of concern that the ability to control it could be lost if it advances too far. The letter has garnered more than 27,000 signatories, including Apple co-founder Steve Wozniak. The letter, published by the Future of Life Institute, called out San Francisco-based OpenAI Lab’s recently announced GPT-4 algorithm in particular, saying the company should halt further development until oversight standards are in place. While AI has been around for decades, it has “reached new capacities fueled by computing power,” Thierry Breton, the EU’s Commissioner for Internal Market, said in a statement in 2021. The Artificial Intelligence Act, he said, was created to ensure that “AI in Europe respects our values and rules, and harness the potential of AI for industrial use.”",
        "publish_date": "2023-05-01 12:00:00"
    },
    {
        "title": "ChatGPT is once again available in Italy after meeting demands of regulators",
        "text": "Article content Some Italian users shared what appeared to be screenshots of the changes, including a menu button asking users to confirm their age and links to the updated privacy policy and training data help page. The Garante said in a statement that it “welcomes the measures OpenAI implemented” and urged the company to comply with two other demands for an age-verification system and a publicity campaign informing Italians about the backstory and their right to opt out of data processing. The watchdog imposed the ban last month after finding that some users’ messages and payment information were exposed to others. It also questioned whether there was a legal basis for OpenAI to collect massive amounts of data used to train ChatGPT’s algorithms and raised concerns that the system could sometimes generate false information about individuals. Infrastructure Minister Matteo Salvini on Instagram, wrote approvingly of the return of ChatGPT and said that his League party “is committed to help start-ups and development in Italy.” Other regulators are now looking closer at such AI systems, with France’s data privacy regulator and Canada’s privacy commissioner investigating after receiving complaints about ChatGPT. The head of the Federal Trade Commission, Lina Khan, warned this week that the U.S. government will “not hesitate to crack down” on harmful business practices involving artificial intelligence.",
        "publish_date": "2023-05-01 17:52:50"
    },
    {
        "title": "ChatGPT might show more empathy to patients than human doctors: study",
        "text": "Artificial intelligence might be better at humanity than humans themselves. AI assistants could express more sympathy toward patients, a new study suggests. A study published Friday in the journal JAMA Internal Medicine found that OpenAI’s ChatGPT answers patient questions with more compassion than human physicians can. “The opportunities for improving healthcare with AI are massive,” lead author John W. Ayers, an epidemiologist from the Qualcomm Institute at the University of California San Diego, said in a release. “AI-augmented care is the future of medicine Researchers at the University of California San Diego, La Jolla took 195 patient questions from Reddit’s AskDocs forum — a social media forum where people publicly post medical questions for doctors to respond to — and had both human doctors and ChatGPT answer the questions. “ChatGPT might be able to pass a medical licensing exam,” study co-author Davey Smith, a physician-scientist, co-director of the UC San Diego Altman Clinical and Translational Research Institute and professor at the UC San Diego School of Medicine, said. “But directly answering patient questions accurately and empathetically is a different ballgame.” The responses were evaluated by a panel of licensed healthcare professionals that rated each answer by “the quality of information provided” — very poor, poor, acceptable, good or very good — and “the empathy or bedside manner provided” — not empathetic, slightly empathetic, moderately empathetic, empathetic and very empathetic. ChatGPT won — and the competition didn’t even come close. “ChatGPT messages responded with nuanced and accurate information that often addressed more aspects of the patient’s questions than physician responses,” Jessica Kelley, M.S.N, a nurse practitioner with San Diego firm Human Longevity and study co-author, said. The clinical team chose the computer response over the human response nearly 80% of the time. “It’s pretty obvious why AI was better. It’s not constrained by time,” Ayers told Axios. “You could take a simple query like: ‘I have a headache, can you help me?’ and you’ll immediately see ChatGPT say ‘I’m sorry you have a headache.’ The doctor knows that, they feel that. They don’t have time to say it.” While artificial intelligence is nowhere near replacing doctors, the findings suggest that bringing ChatGPT and other AI assistants into the healthcare system can help human physicians provide higher quality, more efficient and more empathetic care by improving workflow, removing any health disparities for minorities and affecting a patient’s overall health. “We could use these technologies to train doctors in patient-centered communication, eliminate health disparities suffered by minority populations who often seek healthcare via messaging, build new medical safety systems, and assist doctors by delivering higher quality and more efficient care,” Mark Dredze, Ph.D., the John C Malone Associate Professor of Computer Science at Johns Hopkins and study co-author, said. Study authors noted that further research would need to be completed in clinical settings, including using chatbots to draft responses that human physicians could edit. Trials could also help determine if AI assistants “might improve responses, lower clinician burnout and improve patient outcomes.” “I never imagined saying this, but ChatGPT is a prescription I’d like to give to my inbox. The tool will transform the way I support my patients,” Aaron Goodman, M.D., an associate clinical professor at UC San Diego School of Medicine and study co-author, admitted.",
        "publish_date": "2023-05-01 18:09:36"
    },
    {
        "title": "Generative AI Tools Like ChatGPT And Bard Heralding Generational Shift In Job Roles. Adapt Or Risk",
        "text": "Large Language Models (LLMs), the technologies underpinning chatbots like ChatGPT, are creating cascading waves of emerging paradigm-shifting opportunities by redefining how we accumulate, organize, and disseminate knowledge-based information. Within a week of the ChatGPT launch, one million people had used it; within two months, the number soared to 100 million. The hockey stick adoption rate is being hailed as a game-changing Generative Artificial Intelligence (AI) based productivity enhancement technology. Global competition amongst entrepreneurs and investors is hastily deploying Research and Development (R&D) resources to create cutting-edge AI-based productivity tools for a gamut of industries. Early users of the nascent ChatGPT are amazed at its ability to deliver work products at near-human levels of sophistication. We are just at the tip of the iceberg. The benefits and impact of Generative AI tools like ChatGPT on industries have only begun to occur, harbingering generational job role shifts. By consolidating the knowledge of the collective, these tools can unleash substantial productivity gains and accelerate growth. With the ability to produce content on command, these productivity enhancement tools are expected to deliver work products more quickly and efficiently than ever. The hybrid human/AI mix offers exciting possibilities for the future of work. According to a report by Goldman Sachs, Generative AI could raise global GDP by 7%. While ChatGPT has aroused a lot of excitement, it has also elicited ominous projections of potential job losses; subliminal messaging through biased responses; usage for malicious propaganda and harmful activities; taking over of the human race, and more. However, machines replacing jobs is an age-old fear. New technologies have historically created jobs to replace the ones obsoleted. Unlike prior technology innovations that have typically replaced unskilled labor, Generative AI can also undertake white-collar tasks. While there is no indication of disruption in job markets, it cannot be completely ruled out. By analyzing and interpreting vast repositories of data, AI can provide valuable assistance to individuals working in tech (coders, computer programmers, software engineers, data analysts, etc.), media (advertising, content creation, journalism, entertainment, etc.), legal (paralegals, legal assistants, etc.), market research, medical, education, gig economy/freelancing, liberal arts, STEM (science, technology, engineering, mathematics), gaming, finance, security/defense, and many more. With the help of Generative AI coding assistants, engineers can write code more efficiently, potentially paving the way for a future in which, rather than writing code themselves, software engineers will concentrate on instructing AI tools to write code. The primary focus could morph into qualifying work products produced by machines. These new working models will have applications across many fields. Using Generative AI tools to quickly create a first draft, after that, for a human to further correct, enhance, and deliver the final product in shorter task completion times, can significantly boost worker productivity across many job roles. Task-based programming, writing, and research dominate the freelance gig economy job market. The advent of productivity-enhancing Generative AI tools could have significant implications for the freelancing job market segment. In education, teachers can use Generative AI tools to create customized learning materials. AI tools can analyze data and patterns in healthcare to enhance diagnosis accuracy and help develop targeted drugs and personalized treatment plans. Similarly, in agriculture, the tools can assist farmers in making better decisions about crop planting and resource usage using weather and soil data, resulting in higher yields and operational efficiencies. In manufacturing, data analysis from sensors and machines can optimize production processes by identifying operational inefficiencies, leading to improved production throughputs, product quality, and lower costs. Microsoft MSFT , Google GOOG , and Amazon AMZN are investing billions in AI research to create cutting-edge tools for consumers and enterprises. Microsoft, a partner of OpenAI – the creator of ChatGPT, has already integrated AI into its MS 365 Suite, Bing search engine, and other product lines. Google is developing its Bard chatbot AI offering. Whereas Amazon is developing AI products for the enterprise market. Tesla TSLA CEO Elon Musk (an ex-founder of OpenAI) plans to launch a new AI start-up. Chinese tech giants Baidu BIDU and Alibaba have also launched their versions of AI technology. ChatGPT has attracted a surge of early adopters keen on keeping up with technological advancements and staying ahead of the curve. Employees at all levels within organizations are exploring various AI tools to automate time-consuming tasks or accelerate the delivery of their work products. Numerous companies have begun integrating ChatGPT-type tools into their product offerings, a few examples of which are as follows: Salesforce is integrating OpenAI’s technology to create its Generative AI-based customer relationship management tools. It will also add a ChatGPT app to Slack for conversation summaries, research tools, and writing assistance. Bain & Company has integrated OpenAI’s technologies, including ChatGPT, into its management systems, research, and other processes. Coca-Cola will be the first major consumer product company to use the system. Snap has launched a new AI feature called My AI on Snapchat+, which allows its 2.5 million subscribers to ask an AI chatbot for various prompts, such as dinner recipes and weekend trip plans. However, Snapchat has cautioned that the AI tool may be susceptible to providing biased, inaccurate, or misleading information. Quizlet, a website that offers educational tools, has introduced a new feature, Q-Chat, which utilizes OpenAI’s ChatGPT technology to provide one-on-one tutoring services. Instacart, the grocery delivery and pick-up service, will soon launch a new search engine feature called “Ask Instacart,” incorporating ChatGPT into its app. The feature will enable users to ask open-ended food questions and provide personalized answers from Instacart’s vast database of 1.5 million products sold through 75,000 grocery stores. Shopify SHOP is launching a new shopping assistant feature that will use ChatGPT AI technology to help customers with search inquiries and provide personalized recommendations based on their requests. ChatGPT will analyze customer data to offer relevant and customized solutions, improving the overall user experience on the platform. Speak, a language learning app, has partnered with OpenAI. It uses its speech-to-text API Whisper for its new AI-speaking companion product to give users real-time feedback when learning a new language. IBM IBM chatbot, “Watson Assistant,” uses AI and natural language processing to provide customers personalized assistance and support. H&M chatbot “H&M Home Stylist” provides personalized recommendations based on the customer’s preferences and budget. Sephora chatbot “Sephora Virtual Artist” uses augmented reality and AI to provide personalized recommendations based on the customer’s skin type, tone, and preferences. Uber UBER has integrated a chatbot, “Uber Bot,” into their platform, enabling users to reserve a ride, monitor their trip status, and receive assistance with any concerns that may arise during their journey. Despite concerns, the benefits of LLM-based productivity enhancement tools like ChatGPT, in general, are enormous. These technologies can potentially revolutionize how we interact with and utilize technology to improve efficiency, productivity, and quality, thereby transforming a wide range of industries. As AI technology continues to evolve, improve, and be broadly deployed, it will be exciting to see how it elevates the living standards of societies worldwide. The rise of AI-based productivity enhancement tools is being hailed as the dawn of a new technological era. Staying current and adapting to the changing environment will mitigate obsolescence.",
        "publish_date": "2023-05-01 18:28:21"
    },
    {
        "title": "AI-generated beer, pizza commercials go viral: 'This is what hell looks like'",
        "text": "AI-generated content moved past writing term papers to creating beer and pizza ads that had social media buzzing — and even Elon Musk taking notice. The ad for the nondescript beer shows AI-created likenesses of people at a neighborhood barbecue partying to the 1999 hit “All Star” by Smash Mouth plays in the background. The beer guzzlers laugh and smile awkwardly while sucking through blue-colored cans and bottles — many of which are oversized and deformed. The rough technology makes the revelers look demonic, with some having more than 10 fingers, but the AI-created ad could pass for a low-rent Bud Light promo. “This is what hell looks like,” wrote one Twitter user. “At least we know now.” At one point in the commercial, the fire used to heat up the barbecue spins out of control and turns into an inferno that erupts across the horizon. “Does the LSD come with the beer or is that BYO?” a Twitter user wrote. Meanwhile, a second AI-generated commercial — this one selling a fictional brand of pizza — got the attention of Pizza Hut and led Musk to post and exploding head emoji. A Reddit user who goes by the name “PizzaLater” used the video software tool After Effects to piece together a 30-second clip in which AI-generated people munch on pizza from a fictional restaurant called “Pepperoni Hug Spot.” “Are you ready for best pizza of life?” the AI-powered, digital-sounding narrator says in the clip while computer graphic meant to depict a young boy chowing down on a slice. “Bring friends down to Pepperoni Hug Spot,” the narrator said. The commercial then cuts to a scene showing a family of four — all of whom appear to be missing teeth while gazing at one another with zombie-like eyes and creepy stares — at the table while eating pizza. The next image shows a digital likeness that depicts a chef, though the figure is awkwardly holding what appears to be pizza dough in his hand while smoke emanates from it. “Our chefs make pizza with heart and special touch,” the narrator said as the commercial cuts to a shot of what appears to be pizza being cooked in an oven. The narrator than ran down a list of ingredients used in the preparation of the “Pepperoni Hug Spot” pizza, which includes “cheese, pepperoni, vegetable, and more secret things.” The ad then cut to a shot of the delivery driver, whose eyes dart in different direction while steering the car on what is normally the passenger’s side. “Knock knock who’s there? Pizza magic,” the baritone-voiced computer-generated narrator said. The next scene depicted a pizza lover eating a slice, though she chewed on a portion of the plate instead of the actual pizza. “Pepperoni Hug Spot pizza,” the narrator added. “Your tummy say ‘Thank you’, your mouth say ‘Mmmm’.” The commercial ended with the sales pitch: “Pepperoni Hug Spot. It’s like family, but with more cheese.” Pizza Hut’s Twitter account posted, “my heebies have been jeebied.” Musk, who was an original investor in OpenAI before quitting the board, tweeted the emoji. In a subsequent Reddit post, “PizzaLater” said he created the ad in about three hours using various AI tools. He said the script was conjured up by GPT-4, which is OpenAI’s most advanced chatbot that can “generate, edit, and integrate with users on creative and technical writing tasks, such as composing songs, writing screenplays, or leaning a user’s writing style.” The images in the commercial were created by Midjourney, the AI-powered software that is similar to OpenAI’s DALL-E deep learning model, according to “PizzaLater.” The Reddit user said that the video was put together through the use of Runway Gen-2, the creative AI product developed by Runway Research. The narration and voice over was developed through ElevenLabs, the maker of “Prime Voice AI” which “lets you voice any length of text in top quality, all while automatically matching what is being said with how it’s being said,” according to the company. Finally, “PizzaLater” piped in music that was created with the Soundraw AI Music generator.",
        "publish_date": "2023-05-01 18:35:01"
    },
    {
        "title": "'The godfather of AI' quits Google to freely speak of risks ahead",
        "text": "The godfather of AI' quits Google to freely speak of risks ahead 'I console myself with the normal excuse: If I hadn't done it, somebody else would have' NYT Toronto Geoffrey Hinton 2 min read Last Updated : May 01 2023 | 10:26 PM IST Follow Us Listen to This Article Your browser does not support the audio element. 1x 1x 1.2x 1.5x Geoffrey Hinton was an artificial intelligence pioneer. In 2012, Hinton and two of his graduate students at the University of Toronto created technology that became the intellectual foundation for the AI systems that the tech industry’s biggest companies believe is a key to their future. On Monday, however, he officially joined a growing chorus of critics who say those companies are racing toward danger with their aggressive campaign to create products based on generative artificial intelligence, the technology that powers popular chatbots like ChatGPT. Hinton said he has quit his job at Google, where he has worked for more than decade and became one of the most respected voices in the field, so he can freely speak out about the risks of AI. A part of him, he said, now regrets his life’s work. “I console myself with the normal excuse: If I hadn’t done it, somebody else would have,” Hinton said during an interview last week at his home in Toronto, a short walk from where he and his students made their breakthrough. “It is hard to see how you can prevent the bad actors from using it (AI) for bad things,” Hinton said. ©2023 The New York Times News Service Also Read Google Bard: What we know so far about generative AI chatbot in the works Google may lose search on Samsung devices to Microsoft Bing: Report Google's move to make app makers use its new billing system faces backlash More than crypto, I am interested in artificial intelligence: Elon Musk Italy orders OpenAI to stop processing users' data else face fine IMF Chief Kristalina says rising rates exposed banking vulnerabilities Social media giant Meta aims to raise $7 bn through corporate bond sale General Motors lays off several hundred full-time contract workers May Day Protest: World's workers rally, France sees pension anger Elon Musk cuts down parental leaves of Twitter employees: Report Topics : Artificial intelligence Google First Published: May 01 2023 | 10:26 PM IST Latest News View More In this section All Sebi imposes Rs 10 lakh fine on Angel Broking for flouting regulatory norms 3 min read Mobikwik turns profitable in March quarter, expects to double revenue 2 min read Hero MotoCorp's sales decline by 5% YoY to 396,107 units in April 1 min read IMF Chief Kristalina says rising rates exposed banking vulnerabilities 3 min read Premium The story, the actor and the budget have to work for theatres: Shiv Chanana 4 min read Most Popular View More Read Shared Commented JPMorgan to acquire First Republic Bank after seizure by regulator 2 min read Premium ESMA row: EU banks likely to turn Indian entities into subsidiaries 3 min read LIVE: National Capital reports 259 fresh cases and 2 deaths in last 24 hrs 3 min read SoftBank Group's chip maker Arm registers for blockbuster US IPO 2 min read US manufacturing sector contracts for sixth straight month in April 2 min read BROWSE STOCK COMPANIES TRENDING NOW TOP SEARCHED COMPANIES FROM BS WEBSITEHomeCompaniesMarketsOpinionIndia NewsTechnologyPersonal FinanceIncome Tax CalculatorLatest NewsEducationPartner ContentSpecialsToday's PaperAuthor ABOUT USAbout UsCode of ConductTerms & ConditionsPrivacy PolicyCookie PolicyDisclaimerInvestor CommunicationList of our GST registration number SUPPORT & CONTACTPartner with UsCareersAdvertise with UsContact UsFeedbackBrowser SupportSitemap READER CENTREE-PaperMy PagePortfolioRegistrationSubscribeCustom PaymentDelete My Account BS PRODUCTSBS HindiB2B ConnectiPhoneBS Apps EVENTBudgetBudget with BSAssembly Elections 2023 SPORTSIPL 2023Cricket News Copyrights © 2023 Business Standard Private Ltd. All rights reserved",
        "publish_date": "2023-05-01 19:07:56"
    },
    {
        "title": "20 Things ChatGPT Can and Can't Do",
        "text": "I’ve talked with ChatGPT many times now, and it occasionally lapses into a state of senility in which it doesn’t remember who it is or what we’re talking about. I will ask it something like, “Hey, ChatGPT, how do you think AI chatbots like yourself will impact the publishing industry in the years to come?” and it starts yammering on about how it “doesn’t know anything about a ChatGPT.” Sorta like a cross between a mafioso pleading the fifth and your geriatric uncle trying to remember whether he killed a guy in Korea or not, ChatGPT occasionally just can’t bring itself to answer your question. Algorithm malfunction or cagey evasiveness? You decide... Yessss...turns out, ChatGPT can be quite NSFW when it wants to be. In a bout of inspired depravity, I once instructed it to write me an erotic story involving an Octopus and it dutifully obliged. Then there’s Twitch streamer Jordan Raskopoulos, who recently prompted the chatbot to write a dirty tale about Scooby Doo and got more than he bargained for. Let’s face it: the advent of robot smut is officially here and we are all really happy about it. Right, guys? Right? If this robot is really good at performing absolutely mission critical tasks like writing pervy stories, it apparently isn’t so great at lesser tasks, like computer coding. While there was initially some hubbub about the chatbot’s abilities to do a software programmer’s work for them, it was swiftly revealed that ChatGPT had a problem with inserting gibberish into codebases. To head off a swarm of coding BS, software site Stack Overflow decided to ban ChatGPT from its digital premises for the foreseeable future: “Overall, because the average rate of getting correct answers from ChatGPT is too low, the posting of answers created by ChatGPT is substantially harmful to the site and to users who are asking and looking for correct answers,” admins wrote. Sounds fair! Concerns are high when it comes to how this weirdly articulate chatbot will impact and/or disrupt academia. Will ChatGPT kill the college essay? Will it make teachers irrelevant? More importantly, will it lead to a tsunami of cheating from high school slackers who just want a robot to write their history essay for them already? The New York City Department of Education certainly seems to think so, because it just banned ChatGPT on all school networks and devices. No word yet on whether other cities plan to follow suit. Turns out one thing robots are good at is writing stories about robots. We actually got it to write an entire science fiction story for us, though we had to try and retry prompts to make it coherent. Was the story good? Ehhhh...well, not exactly. But, tbh, I’ve read worse! ChatGPT crossed the 100-million mark of monthly active users in early February 2023, despite only launching in November of the previous year, according to an analysis from the bank UBS. It’s among the fastest-growing applications in history, with roughly 13 million unique visitors daily. Microsoft is investing some $10 billion in OpenAI to forge a partnership with the fledgling AI pioneer (Google is very worried). To that end, the office software giant rolled out a premium version of its office chat software Teams in early February 2023, which will generate meeting notes and to-do lists, among other things. It’ll cost $7 per user until the price ramps up to $10 a head in July 2023.",
        "publish_date": "2023-05-01 19:00:18"
    },
    {
        "title": "‘New Era Of Turbulence’: The World Economic Forum Predicts 25% Of Jobs Will Change Over The Next Five Years",
        "text": "A quarter of jobs will be impacted over the next five years, according to a new report by the World Economic Forum on Monday. The fast-growing trends of artificial intelligence, digitization, renewable energy and supply chain reshoring will bring about a critical shift in the global labor market. The WEF predicts a “new era of turbulence,” as many workers won’t have the requisite skills to keep up with the changes. Those with a technology, data analytics or cybersecurity background will benefit in the new environment. The WEF study surveyed more than 800 companies that collectively employ 11.3 million workers across 45 countries worldwide. Global employers anticipate creating 69 million new positions by 2027 and eradicating 83 million jobs—a net loss of 14 million roles. Clerical workers will bear the brunt of the fast-moving changes. Around 26 million jobs in administrative positions will be cut due to AI. Additionally, macro-economic events, including slower economic growth, supply shortages and inflation, will pose more serious job threats than AI. The study anticipates that the proliferation of AI will significantly disrupt the labor market. However, the WEF believes that the net impact of most technologies will be positive for employment growth over the next five years. In March, Goldman Sachs released a report concluding that generative AI will disrupt 300 million jobs. The investment bank’s study also highlights that automation creates innovation, leading to new jobs. For companies, there will be cost savings. They can deploy their resources toward building and growing businesses, ultimately increasing annual global GDP by 7%. In recent months, the world has witnessed the ascendency of OpenAI software ChatGPT and DALL-E. ChatGPT surpassed one million users in its first five days of launching, the fastest that any company has ever reached this benchmark. Job Creation According to WEF, the proliferation of green technologies and renewable energy will be a key driver of job creation in the future of work. There will be a high demand for sustainability specialists, business intelligence analysts, information security analysts, renewable energy engineers, solar energy installation and system engineers. There will be an estimated 10% growth in the education sector with the creation of 3 million jobs for vocational and higher education teachers. An additional 30% of jobs—3 million positions—will be added to the economy for agricultural specialists, with an increased need for agricultural equipment operators. Job Losses Administrative roles will be the most impacted jobs in this new era of turbulence caused by automation and digitization. Twenty-six million clerical jobs are expected to disappear by 2027, including cashiers and ticket clerks, data entry, accounting, bookkeeping and payroll clerks and administrative and executive secretaries. Core Skills For A Disruptive Economy Analytical and creative thinking will be the most coveted cognitive skills by employers, with analytical thinking remaining the most valued core skill. To ensure that workers can adapt to these fast changes in the workplace, companies will also look for resilience, flexibility, agility, motivation, self-awareness, curiosity and constant learning. Training And Upskilling To deliver business goals in an increasingly digital economy, employers will need to implement on-the-job training and coaching. Around 60% of the global workforce will require upskilling to keep up with AI advancements. However, only around 50% of labor participants have access to the necessary training.",
        "publish_date": "2023-05-01 19:20:03"
    },
    {
        "title": "150 African Workers for Big Tech Companies Vote to Unionize",
        "text": "More than 150 workers whose labor underpins the AI systems of Facebook, TikTok and ChatGPT gathered in Nairobi on Monday and pledged to establish the first African Content Moderators Union, in a move that could have significant consequences for the businesses of some of the world’s biggest tech companies. The current and former workers, all employed by third party outsourcing companies, have provided content moderation services for AI tools used by Meta, Bytedance, and OpenAI—the respective owners of Facebook, TikTok and the breakout AI chatbot ChatGPT. Despite the mental toll of the work, which has left many content moderators suffering from PTSD, their jobs are some of the lowest-paid in the global tech industry, with some workers earning as little as $1.50 per hour. As news of the successful vote to register the union was read out, the packed room of workers at the Mövenpick Hotel in Nairobi burst into cheers and applause, a video from the event seen by TIME shows. Confetti fell onto the stage, and jubilant music began to play as the crowd continued to cheer. The establishment of the Content Moderators Union is the culmination of a process that began in 2019, when Daniel Motaung, a Facebook content moderator, was fired from his role at the outsourcing company Sama after he attempted to convene a workers’ union called the Alliance. Motaung, whose story was first revealed by TIME, is now suing both Facebook and Sama in a Nairobi court. Motaung traveled from his home in South Africa to attend the Labor Day meeting of more than 150 content moderators in Nairobi, and addressed the group. Read More: Facebook Faces New Lawsuit Alleging Human Trafficking and Union-Busting in Kenya “I never thought, when I started the Alliance in 2019, we would be here today—with moderators from every major social media giant forming the first African moderators union,” Motaung said in a statement. “There have never been more of us. Our cause is right, our way is just, and we shall prevail. I couldn’t be more proud of today’s decision to register the Content Moderators Union.” TIME’s reporting on Motaung “kicked off a wave of legal action and organizing that has culminated in two judgments against Meta and planted the seeds for today’s mass worker summit,” said Foxglove, a non-profit legal NGO that is supporting the cases, in a press release. Those two judgments against Meta include one from April in which a Kenyan judge ruled Meta could be sued in a Kenyan court—following an argument from the company that, since it did not formally trade in Kenya, it should not be subject to claims under the country’s legal system. Meta is also being sued, separately, in a $2 billion case alleging it has failed to act swiftly enough to remove posts that, the case says, incited deadly violence in Ethiopia. “It takes a village to solve a problem, but today the Kenyan moderators formed an army,” said Martha Dark, Foxglove’s co-director, in a statement. “From TikTok to Facebook, these people face the same issues. Toxic content, no mental health care, precarious work – these are systemic failures in content moderation. Moderators from TikTok, employed by the outsourcing company Majorel, also said they would participate in the union. “Seeing so many people together today was incredible,” said ​​James Oyange, a former TikTok content moderator at Majorel, who has taken a leadership role in organizing his former colleagues. “People should know that it isn’t just Meta—at every social media firm there are workers who have been brutalized and exploited. But today I feel bold, seeing so many of us resolve to make change. The companies should listen—but if they won’t, we’ll make them. And we hope Kenyan lawmakers and society will ally with us to transform this work.” Workers who helped OpenAI detoxify the breakout AI chatbot ChatGPT were present at the event in Nairobi, and said they would also join the union. TIME was the first to reveal the conditions faced by these workers, many of whom were paid less than $2 per hour to view traumatizing content including descriptions and depictions of child sexual abuse. “For too long we, the workers powering the AI revolution, were treated as different and less than moderators,” said Richard Mathenge, a former ChatGPT content moderator who worked on the outsourcing company Sama’s contract with OpenAI, which ended in 2022. “Our work is just as important and it is also dangerous. We took an historic step today. The way is long but we are determined to fight on so that people are not abused the way we were.” Read More: Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic Mercy Mutemi, a lawyer at Nzili and Sumbi Advocates, the law firm suing Meta in both Motaung’s case and the Ethiopia hate speech case, said Monday’s events were a watershed. “Moderators have faced unbelievable intimidation in trying to exercise their basic right to associate,” she said. “Today they have made a powerful statement: their work is to be celebrated. They will live in fear no longer. Moderators are proud of their work, and we stand ready to offer the necessary support as they register the trade union and bargain for fair conditions.” Foxglove, which is funded in part by the Ford Foundation and the Open Society Foundation, paid for the Nairobi event along with Superrr Lab, a German non-profit. Write to Billy Perrigo at [email protected]",
        "publish_date": "2023-05-01 19:23:09"
    },
    {
        "title": "150 African Workers for ChatGPT, TikTok and Facebook Vote to Unionize at Landmark Nairobi Meeting",
        "text": "More than 150 workers whose labor underpins the AI systems of Facebook, TikTok and ChatGPT gathered in Nairobi on Monday and pledged to establish the first African Content Moderators Union, in a move that could have significant consequences for the businesses of some of the world’s biggest tech companies. The current and former workers, all employed by third party outsourcing companies, have provided content moderation services for AI tools used by Meta, Bytedance, and OpenAI—the respective owners of Facebook, TikTok and the breakout AI chatbot ChatGPT. Despite the mental toll of the work, which has left many content moderators suffering from PTSD, their jobs are some of the lowest-paid in the global tech industry, with some workers earning as little as $1.50 per hour. As news of the successful vote to register the union was read out, the packed room of workers at the Mövenpick Hotel in Nairobi burst into cheers and applause, a video from the event seen by TIME shows. Confetti fell onto the stage, and jubilant music began to play as the crowd continued to cheer. The establishment of the Content Moderators Union is the culmination of a process that began in 2019, when Daniel Motaung, a Facebook content moderator, was fired from his role at the outsourcing company Sama after he attempted to convene a workers’ union called the Alliance. Motaung, whose story was first revealed by TIME, is now suing both Facebook and Sama in a Nairobi court. Motaung traveled from his home in South Africa to attend the Labor Day meeting of more than 150 content moderators in Nairobi, and addressed the group. Read More: Facebook Faces New Lawsuit Alleging Human Trafficking and Union-Busting in Kenya “I never thought, when I started the Alliance in 2019, we would be here today—with moderators from every major social media giant forming the first African moderators union,” Motaung said in a statement. “There have never been more of us. Our cause is right, our way is just, and we shall prevail. I couldn’t be more proud of today’s decision to register the Content Moderators Union.” TIME’s reporting on Motaung “kicked off a wave of legal action and organizing that has culminated in two judgments against Meta and planted the seeds for today’s mass worker summit,” said Foxglove, a non-profit legal NGO that is supporting the cases, in a press release. Those two judgments against Meta include one from April in which a Kenyan judge ruled Meta could be sued in a Kenyan court—following an argument from the company that, since it did not formally trade in Kenya, it should not be subject to claims under the country’s legal system. Meta is also being sued, separately, in a $2 billion case alleging it has failed to act swiftly enough to remove posts that, the case says, incited deadly violence in Ethiopia. “It takes a village to solve a problem, but today the Kenyan moderators formed an army,” said Martha Dark, Foxglove’s co-director, in a statement. “From TikTok to Facebook, these people face the same issues. Toxic content, no mental health care, precarious work – these are systemic failures in content moderation. Moderators from TikTok, employed by the outsourcing company Majorel, also said they would participate in the union. “Seeing so many people together today was incredible,” said ​​James Oyange, a former TikTok content moderator at Majorel, who has taken a leadership role in organizing his former colleagues. “People should know that it isn’t just Meta—at every social media firm there are workers who have been brutalized and exploited. But today I feel bold, seeing so many of us resolve to make change. The companies should listen—but if they won’t, we’ll make them. And we hope Kenyan lawmakers and society will ally with us to transform this work.” Workers who helped OpenAI detoxify the breakout AI chatbot ChatGPT were present at the event in Nairobi, and said they would also join the union. TIME was the first to reveal the conditions faced by these workers, many of whom were paid less than $2 per hour to view traumatizing content including descriptions and depictions of child sexual abuse. “For too long we, the workers powering the AI revolution, were treated as different and less than moderators,” said Richard Mathenge, a former ChatGPT content moderator who worked on the outsourcing company Sama’s contract with OpenAI, which ended in 2022. “Our work is just as important and it is also dangerous. We took an historic step today. The way is long but we are determined to fight on so that people are not abused the way we were.” Read More: Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic Mercy Mutemi, a lawyer at Nzili and Sumbi Advocates, the law firm suing Meta in both Motaung’s case and the Ethiopia hate speech case, said Monday’s events were a watershed. “Moderators have faced unbelievable intimidation in trying to exercise their basic right to associate,” she said. “Today they have made a powerful statement: their work is to be celebrated. They will live in fear no longer. Moderators are proud of their work, and we stand ready to offer the necessary support as they register the trade union and bargain for fair conditions.” Foxglove, which is funded in part by the Ford Foundation and the Open Society Foundation, paid for the Nairobi event along with Superrr Lab, a German non-profit. Write to Billy Perrigo at billy.perrigo@time.com.",
        "publish_date": "2023-05-01 18:21:01"
    },
    {
        "title": "‘Godfather of AI’ Leaves Google to Save Us From AI",
        "text": "Geoffrey Hinton, an artificial intelligence pioneer known as one of the \"godfathers of AI\" resigned from his position at Google so that he could openly express his concerns about how A.I. could cause significant harm to the world. Hinton admitted in a New York Times interview that he now partly regrets his life's work. Despite the beneficial uses of A.I., Hinton fears that the technology could be used irresponsibly, unleashing unintended consequences. Hinton is worried that competition between tech giants like Google and Microsoft to create the most advanced A.I. will result in a global race that will not stop without some form of worldwide regulation. However, he was also emphatic in pointing out that he thought that Google has acted responsibly in its research: Hinton is known for popularizing the theoretical development of neural networks in 1986 and for creating one capable of recognizing images in 2012. His work was crucial to the development of current generative art models like Stable Diffusion and MidJourney, and laid the groundwork for OpenAI's upcoming efforts to make GPT-4 capable of interacting with images. His potentially belated move has many comparing him to J. Robert Oppenheimer, a physics professor credited with creating the atomic bomb. The Risks of AI One of the immediate problems Hinton highlights is the proliferation of fake images, videos, and text online, which could make the truth increasingly difficult to discern for the average person. As generative A.I. continues to improve, creators of fake and manipulative content could use these tools to deceive and confuse people. Hinton is also concerned about how A.I. could affect jobs in the future. While chatbots like ChatGPT currently complement human workers, they could ultimately replace those who handle routine tasks, such as personal assistants, accountants, and translators. Although AI may alleviate some monotonous work, it could also eliminate more jobs than anticipated, disrupting social balance. In the long term, Hinton fears that future versions of the technology pose a threat to humanity due to the unexpected behavior they may learn from the large volumes of data they analyze. This becomes a problem when A.I. systems are allowed to generate and execute their own code. This long-term view also gained particular relevance when other key figures in the A.I. field began to warn about the possibility of a \"foom\" scenario—in which AI far outpaces human intelligence—and the impact it could have on societal development. Hinton is just one of thousands of tech leaders and researchers alarmed by the exponential advancement of AI developments for various fields (from erotic chats to medical diagnostics). Last month, an open letter gained popularity in which leaders called for a pause in AI development until adequate controls are established. Hinton did not sign it. The evolution of Hinton's position on A.I. reflects a growing awareness of the risks and challenges associated with rapidly evolving technology. For Hinton, resigning from his life's work was important to prevent a scenario that he says seems to be getting closer every day. \"Look at how it was five years ago and how it is now,\" he told The New York Times. \"Take the difference and propagate it forwards. That's scary.\"",
        "publish_date": "2023-05-01 22:00:04"
    },
    {
        "title": "IBM to pause hiring in plan to replace 7,800 jobs with AI - Bloomberg News",
        "text": "International Business Machines Corp expects to pause hiring for roles as roughly 7,800 jobs could be replaced by Artificial Intelligence (AI) in the coming years, CEO Arvind Krishna told Bloomberg News on Monday. Hiring specifically in back-office functions such as human resources will be suspended or slowed, Krishna said, adding that 30 per cent of non-customer-facing roles could be replaced by AI and automations in five years. His comment comes at a time when AI has caught the imagination of people around the world after the launch of Microsoft Corp-backed OpenAI's viral chatbot, ChatGPT, in November last year. The reduction could include not replacing roles vacated by attrition, the PC-maker told the publication. IBM did not immediately respond to a Reuters request for comment.",
        "publish_date": "2023-05-01 23:42:56"
    },
    {
        "title": "Godfather of AI' quits Google and warns of the dangers of artificial intelligence",
        "text": "A prominent researcher in artificial intelligence, known as the 'Godfather of AI' has quit his job at Google, and has issued a chilling warning on the risks the burgeoning technology poses to society. Dr Geoffrey Hinton is widely credited with laying the groundwork which would lead to the creation of increasingly popular 'chatbots' using artificial intelligence (AI), such as OpenAI's ChatGPT and others. With their growing popularity, Dr Hinton is just one of a number of experts speaking out about the harm AI could cause. The 75-year-old said he left Google so he could \"talk about the dangers of AI without considering how this impacts Google.\" His statement comes following an article in the New York Times, in which he said writer Cade Metz \"implies\" Dr Hinton left so he \"could criticise Google\". He went on to rebuff the idea, saying: \"Google has acted very responsibly.\" Dr Hinton said he left Google in order to speak more freely about the potential harms widespread and unrestrained AI developmentc could pose. He listed the spread of misinformation and upheaval in the jobs market and other, more nefarious uses. \"Look at how it was five years ago and how it is now,\" Hinton said in the interview published Monday, May 1. \"Take the difference and propagate it forwards. That's scary.\" He went on to say: \"It is hard to see how you can prevent the bad actors from using it for bad things.\" Dr Hinton let Google know of his plans to step down last month, and spoke to the company's CEO Sundar Pichai personally on Thursday, April 27, according to the NY Times article. He didn't discuss the specifics of the phone call. In a statement, Google's chief scientist said: \"We remain committed to a responsible approach to AI. We're continually learning how to understand emerging risks while also innovating boldly.\" Among those who have also issued warnings about AI is Elon Musk, who, along with more than 1,000 other prominent AI figures, called for a six-month pause in advanced AI development in March, citing the potential \"profound risks to society and humanity\". In the interview with the NY Times, Dr Hinton expressed concerns at the pace at which AI is developing, and that it has begun to outpace the human mind in some areas. He also talks of concerns that the pace of AI development will increase after Microsoft-backed OpenAI, Google and other tech giants race to become leaders in the field - with potentially dangerous consequences. One of the fears discussed by Dr Hinton was the potentially devastating impact the use of AI could have on the job market. AI chatbots such as ChatGPT are currently used alongside human workers, to complement and aide their work - however, they could go on to replace positions such as paralegals, personal assistants, translators and people carrying out repetitive tasks. \"It takes away the drudge work,\" he told the NY Times. \"It might take away more than that.\" Then there are the issues we could face in the future as the technology learns unexpected behaviour from the huge amounts of data they analyse. He explained that this becomes an issue as individuals and companies allow AI systems not only to generate their own computer code, but to run it unsupervised. Dr Hinton went on to say he fears a day when truly autonomous weapons become a reality. \"The idea that this stuff could actually get smarter than people - a few people believed that,\" he explained. \"But most people thought it was way off. And I thought it was way off. \"I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\" Dr Hinton is a British expatriate and lifelong academic, studying at the University of Edinburgh before going on to become a professor of computer science at Carnegie Mellon University in Pittsburgh, Pennsylvania, leaving the university for Canada as he was reluctant to take Pentagon funding. At that time, the 1980s, most AI research in the US was funded by the Defense Department, but Dr Hinton said he is and always has been deeply against the use of AI on the battlefield, or \"robot soldiers\", as he calls it. Since 2013, Dr Hinton split his time between being a professor at the University of Toronto, and a Google engineering fellow. He joined Google after the company acquired a startup he co-founded with two of his students - Alex Krishevsky and Ilya Sutskever. The professor and his student developed a neural network, a mathematical system that learns skills by analysing data, which trained itself to identify common objects by analysing photos. Sutskever is currently the chief scientist for OpenAI. In 2018, Dr Hinton and two other collaborators received the Turing Award for their work on neural networks.",
        "publish_date": "2023-05-02 00:36:56"
    },
    {
        "title": "How artificial intelligence is already powering work in B.C.",
        "text": "General-purpose robots that may soon be able to assume manual tasks performed by astronauts in space. Programs for self-driving cars that understand human behaviour. Developing new drugs to fight cancer. These are some of the novel ways in which B.C. companies are using machine learning and artificial intelligence – and to the clear potential benefit of humanity. But like nuclear fission, machine super-intelligence is a Promethean power with the potential to be corrupted, which is why there is now a sudden push to erect guardrails and develop ethical guidelines and regulations before AI either becomes autonomous or simply falls into the wrong hands. Elon Musk and Yoshua Bengio, a Canadian pioneer in deep learning, are among the more than 27,000 people who have signed an open letter calling for a six-month moratorium at all AI labs, until concerns about it can be addressed. Just last week, KPMG convened what might be described as an emergency summit in Vancouver to discuss AI and the opportunities and challenges this rapidly developing technology presents. “The purpose of it was really to start a conversation around what’s becoming very clearly a very transformative piece of technology that is just accelerating in terms of its adoption,” said Walter Pela, regional managing partner for KPMG. “There’s obviously concerns and issues. At the same time, it is a tool that’s being adopted.” In fact, it’s being adopted by businesses in the U.S. a lot faster than in Canada, according to a KPMG survey released last week. “The pace in Canada right now of AI adoption in business is about half of what it is in the U.S., according to a recent poll we did in February,” Pela said. Vancouver does not have pure-play AI companies or institutes, like Montreal’s Mila research institute, but it has developed a hub of applied AI companies. Computer scientists have been developing machine learning and artificial intelligence for decades. But it wasn’t until San Fracisco, Calif.-based OpenAI made its ChatGPT-3 chat bot available to the public that ordinary people got to see just how powerful this one type of AI already is. The pace of Open AI’s progress has generated both awe and alarm. Some of the concerns around generative AI programs, like ChatGPT, is that they could be used for fraud, cybercrime and the amplification of misinformation. Another concern is that its level of disruption – at least similar in scale to that of the internet, if not greater – could put a lot of people in creative fields and knowledge industries out of work in fairly short order. ChatGPT is just one type of generative AI – technology that has the capacity to generate text, images, videos or music that look or sound like they were created by humans. ChatGPT is text-based, and is basically like a super digital library containing a massive corpus of text from the Internet – a library with the ability to learn, to respond to commands and to write anything from song lyrics to HTML code for websites, all in about 30 seconds. You can ask it to write an essay on virtually any topic, and then, half a minute later, ask to have that essay rewritten it in almost any language. Diffusion AI is a text-to-image model. Diffusion AI programs like DALL-E, Midjourney and Stable Diffusion have the potential to displace illustrators. In fact, that may be the biggest immediate threat that AI poses – not rogue machines turning their human masters into servants, but sudden, massive displacement of workers in certain industries, such as web design. A Vancouver company called Durable, for example, uses AI for a program that can build basic websites for any type of business in 30 seconds. “Any knowledge worker that is trained to do certain things – and already they’re interfacing in the digital realm – that’s the first thing that gets impacted,” said Handol Kim, CEO of Variational AI and a board director for AInBC. “So, content writers? Absolutely – already happening. Graphic design, already happening. Lawyers? Starting to happen. Accountants, starting to happen. Software developers? Already you’re getting decent code. It’s not great, but it’s not bad. Here’s the thing – it gets better. Next year, it will get twice as good. The year after that, it will get five times as good. “Eventually it will be able to make movies. Anything that’s represented digitally and can be manipulated digitally, eventually it can get to a level that’s uncanny.” “I think it’s fairly clear that there will be job dislocation in fairly short order, I think,” said Steve Lowry, executive director of AInBC. “For fastest change, I think we’ll see in the creative realm generative AI changing the job of designers, photographers, marketers like overnight basically.” Though AI threatens to make some jobs obsolete, it also creates new opportunities – including jobs in applied AI. A number of companies in Vancouver are using various types of machine learning and AI for a wide range of applications. Sanctuary AI, a B.C. company co-founded by Suzanne Gildert and Geordie Rose – the founder of D-Wave Systems, which built the world’s first quantum computer – is using AI in the development humanoid general-purpose robots. The company is using AI to develop a “cognitive architecture” for its robots that will “mimic the different subsystems in a person’s brain.” The company expects the robots could be used to replace humans to do work that is dangerous, tedious or in the vacuum of space. “In the not-too-distant future, Sanctuary technology will help people explore, settle, and prosper in outer space,” the company said in a news release last year, after securing $75 million in a Series A financing round. Inverted AI is a Vancouver company that uses deep learning and generative AI to understand the behaviour of drivers, cyclists and pedestrians, for companies developing self-driving vehicles. Companies developing self-driving cars or advanced driver-assistance systems use simulators. Inverted AI helps to add the irrational human element to those simulations by recording traffic with a drone and then using machine learning to “learn” how humans behave in traffic. “We record how people behave on the road, both as drivers but also as pedestrians, cyclists and so on, and we use that to improve the realism of simulations for self-driving cars,” said Inverted AI CTO Adam Scibior, an adjunct professor at the University of British Columbia’s computer science department. “We basically make those more realistic.” Variational AI is using a type of machine learning – variational auto-encoder – to identify small molecules that will bind to protein kinases associated with cancer and tumors. But there are about 500 protein kinases in the human genome, all similar in structure, and finding the right molecule to bind only to kinases associated with cancers is a massive trial-and-error challenge. “If you have a small molecule that binds to one kinase, it’s going to bind to many others, and you don’t want that,” Handol Kim explained. Rather than hunt for pre-existing molecules, then, Variational AI uses generative machine learning to make new molecules. In other words, rather than trying to find the right key out of hundreds of options, Variational AI is using machine learning to just cut new keys. The “generative chemistry” process the company uses has the potential to dramatically accelerate the drug discovery process. It can take a decade and up to $1 billion to $2 billion to take a new drug through clinical trials and approval for use. Kim said using machine learning may be able to dramatically reduce both the time and costs associated with new drug discovery. “What we’re trying to do is turn years into months,” Kim said. “We’re trying to turn pre-clinical development, move it from hundreds of millions of dollars to single-digit millions.”",
        "publish_date": "2023-05-02 00:30:00"
    },
    {
        "title": "Education has a key role to play in unlocking the potential of AI",
        "text": "For many people, artificial intelligence (AI) has been something typically confined to science-fiction films – but more and more it is creeping into our homes, universities and workplaces. So how will AI will disrupt the working world and what advice do academics have in terms of course selection at this early stage in the technology? Dr Oisin Cawley, a lecturer and researcher in computing at South East Technological University (SETU), makes the point that AI has actually been around in one guise or another for some time already. “If we consider the car industry, for example, automated assembly lines have been manufacturing the cars for years,” he says. “Is that not AI? “Much of modern AI is far more low-key and is rapidly sneaking into many aspects of our lives. This low-key AI has been around for a while and I think that is where much of the disruption will be felt.” Artificial intelligence is “excellent” at automating routine and mundane tasks such as data entry, customer support in the form of chatbots and other administrative tasks, says Cawley. “AI is in heaven when it is asked to analyse data – lots of data,” he adds. “It can churn data inside out and upside down to find interesting patterns and make future predictions. It can do this in a fraction of the time a human can and in fact can find patterns that humans would not. “Have a guess how many students are trying to use AI to make stock-market predictions.” We are all familiar with the notorious internet cookie business. The cookies gather information about us and companies analyse it to personalise products and services specifically for us. All these tasks were previously performed by people. So, what happens to these people? Certainly, fewer people are needed in these roles, which is good for business. Or is it? “Conventional wisdom would suggest that as AI starts to take over these more repetitive tasks, people are freed up to concentrate on those tasks which require higher-level thinking and creativity,” says Cawley. “I would suggest that most jobs have repetitive, mundane tasks that will be affected by AI in this way. Take software developers, for example. They write the code that runs our apps. In many cases code can be reused. But AI is very good at learning coding patterns and already there are AI assistants that can write code for you. The software developer role has the potential to change to one where coding, creativity, user experience, and AI are all harnessed together.” I don’t think undergraduates should worry too much about AI-specific courses unless they specifically want to focus on AI as a subject. I think AI will have a role in almost all subjects— Dr Oisin Cawley Cawley points out that even in more artistic roles such as graphic design, AI has made huge strides. “Check out OpenAI’s DALL-E application which can create images from a text description given by the user,” he says. “You can easily see the potential for this to be extended into areas such as design and architecture.” So how does all this change the landscape for students and teachers? “It is a very interesting time when it comes to education,” says Cawley. “As educators, we need to be cognisant of the potential of AI, both in terms of how it may disrupt the working world but also what skills people should have to exploit this potential.” This is relevant not just to someone looking to start third-level education, but also for people already in employment. “In the area of computing at SETU, we have been focusing on building the AI skills of our undergraduate students on both software development and computer-games degree courses,” says Cawley. “Our aim is to give them enough knowledge to know how to use AI to solve problems which will hopefully translate to the workplace. Employers are aware of the growing importance of AI and so they like to see graduates with some level of knowledge and skill in this area. “To serve more mature students we run a Master of Science in Applied Artificial Intelligence and a Master of Science in Data Science which caters for people who are looking to upskill into these specific areas. “So, I don’t think undergraduates should worry too much about AI-specific courses unless they specifically want to focus on AI as a subject. I think AI will have a role in almost all subjects.” Jack Kennedy, an economist at jobs website Indeed, says advances in AI can help not only with improving business performance but in wider areas such as vaccine development. “There’s no doubt it will bring transformative developments to the way we work in the future,” he says. “There are benefits of working in tandem with AI. We’ve already seen how other technical advancements have impacted the way we work. “For instance, think of how software like Excel changed the way accountants work but didn’t replace the need for accountants themselves. Often AI can help automate the admin or tasks that are time consuming, allowing people to concentrate on other areas of their job.” For anyone concerned about the potential impact of AI on their career path, Kennedy has several pointers. “Remaining flexible is key; with technological and social changes advancing rapidly, it’s important to remain open about career changes or opportunities,” he says. “Change is inevitable, so rather than fighting it, figure out how you can use it as an opportunity for career advancement. This might mean upskilling or altering your career path. “People skills or emotional intelligence is an area that AI will not be able to replicate in the same way a person can. Developing skills like communication, negotiation and navigating conflict can help future-proof your career against technological changes.” TU Dublin lecturer Dr Rajesh Jaiswal teaches a Masters in Computing in Human-Centred AI; he says students should always have one eye on the future when selecting their courses. “AI technology and related technologies are part of the latest wave of the information revolution and could be worth up to €125 billion by 2025,” he says. “In the last two decades AI has rapidly advanced and transformed many industries, including education, healthcare, finance, transportation and entertainment. It will only continue to grow as these technologies become more advanced and widespread.” Jaiswal says there is a skills gap in Ireland and internationally for data scientists with computing and business-intelligence skills. TU Dublin has a four-year degree course, BSc (Hons) in Computing with Machine Learning/AI, designed in consultation with big IT companies such as Amazon, Microsoft, SAP and IBM, which develops a mix of computing, business intelligence and data-science skills. “The rise of big data and cloud computing has made it easier to process large data sets and train complex AI models,” says Jaiswal. “AI-powered chatbots and virtual assistants have become more sophisticated, enabling businesses to automate customer service and improve user experiences. AI is also being used in healthcare to assist with diagnoses and drug discovery, and in finance to detect fraud and manage risks.” As the application of AI is so wide, Jaiswal recommends that “all potential students” should consider gaining skills that are increasingly important to many industries. “An AI product manager, for example, is less of a scientist and more of a marketing role – but they have to understand working with cross-functional teams to bring an AI product to market,” he says. PwC director of people and organisation consulting Laoise Mullane says there are many courses that will help people get started in the AI space. “Some colleges have specific AI courses but other areas such as data science, computer science, mathematics and statistics all provide a good grounding for a career in AI,” she says. “As AI becomes more mainstream, we expect to see AI modules becoming a common feature of a variety of college courses such as business, law and HR.” Dr Valerie Mc Taggart, the head of the department of social sciences ATU Sligo, has conducted research that focuses on the impact of digital technology to organisations and society. AIs will often present incorrect information as truth with 100 per cent certainty ... Humans will still have to be in the mix to ensure the information quality is high— Robert O'Connor “Like previous digital developments, these disrupters have the potential to alter a marketplace at an alarming pace,” she says. “We have seen several high-profile examples of the impact of the digital revolution on industries that did not capitalise on these digital developments. “As for the need for academic lecturing staff, suggestions have abounded that their role will become significantly reduced. Indeed, how can one person compete with all the information available to Chat GPT?” Robert O’Connor, a lecturer in computing at SETU, says advances in AI make him feel “simultaneously giddy and horrified”. “I suspect one area where AI is likely to have an immediate impact is in automating repetitive office tasks,” he says. Manual data entry, document formatting, move A into B and so on are examples of the kind of work AI tools can handle with ease. “However, AIs aren’t perfect,” says O’Connor. “They will often present incorrect information as truth with 100 per cent certainty and they also generate a significant number of ‘hallucinations’, which is where an AI will just make stuff up. “Humans will still have to be in the mix to ensure the information quality is high. It’s likely that some jobs may go – but others may take their place. We’re already seeing job listings for ‘prompt engineer’, which is someone who can create high-value input for an AI tool.” In terms of advice for students in terms of course selection at this early stage in the technology, O’Connor says they should not be looking at courses with respect to what will be affected by AI. “If a person chooses a broad area in which they’re genuinely interested – be it computer science, nursing, English, whatever – then it doesn’t really matter about AI,” he says. “These tools are going to affect pretty much everyone. If you’re in an area that you like and have a passion for, you’ll adapt. “The main skill third-level students need to develop is how to critically assess the quality of information presented and then make decisions based on that. “Students can do this by getting a foundation in their chosen discipline, understanding its general concepts and staying on top of current trends. Then, if an AI enters the mix, they can make an informed determination of the quality of the information it produces.”",
        "publish_date": "2023-05-02 02:05:00"
    },
    {
        "title": "Joey…seeing differently!",
        "text": "By Prof. Kirthi Tennakone What is intelligence? A precise, universally accepted definition does not exist. The Oxford Dictionary defines intelligence as the ability to learn, understand and think logically. Psychologists say it is the capacity for rational thinking, understanding the environment and adaptation to changing occurrences. There are hundreds of other definitions and descriptions of intelligence, highlighting different aspects of the complex trait and bearing many other human qualities. Intelligence facilitates the acquisition of knowledge, providing learning skills and symbiotically enriches creativity and imagination. A famous quote by Albert Einstein says, “The true sign of intelligence is not knowledge but imagination.” Intelligence leads to wisdom, logical argument and clarity of expression. It benefits the individual and society but differs from craftiness, which only provides a temporary undue advantage to a person or a group. So many other qualities of people owe much to their intelligence and vice versa. How we acquired intelligence Plants and animals are the most advanced forms of life on earth. Plants manufacture food and their structural material out of air, water and minerals, harvesting sunlight and stand sessile. Whereas the animals move and nourish themselves on plants. Mobility freed life (animals) to encounter the pros and cons of the environment, necessitating the development of organs to sense external stimuli, such as sound, light, touch, smell and taste. The result was the evolution of the nervous signaling system and the brain to coordinate different sensory responses and derive information. The process took billions of years and culminated in ‘inventing’ the human brain by the method of natural selection. The brain evolved primarily for adaptation to the environment. Later, neural morphology and cognitive functions expanded dramatically, permitting linguistic communication and mechanical skills. Evolution favoured the selection of brainy against less brainy! Besides the routine tasks of eating and living, the man contemplated. Incidentally, the feelings coming to the mind of an early human sitting on a hillock and seeing the scenery in front were routine matters such as gathering food and chasing animals living there. When cognitive abilities furthered, a man, in the same mood, admired the beauty of the scenery. The ‘beauty’, a more abstract concept, was beyond recognition by the earliest humans. Similarly, engraving a picture of an animal, on stone, indicates abstract insight. The men, who first did it, were the most ancient Isaac Newtons and Einsteins. Such abstractions, or realization of ideas, other than material things, or events, surfaced 50,000 to 100,000 years ago, when evolutionary pressure selected an advantageous group of humans with new connections between different parts of the brain. Art, literature, mathematics, science and technology – the key areas of learning so influential in transforming society – originated as a result of abstract thinking. Artificial Intelligence The human brain shines above everything else as the supreme outcome of billions of years of biological evolution. No one has yet found a priori reason we cannot invent machines as intelligent as ourselves or superior. The unanswerable question is how long it takes to reach this ultimate feat and whether efforts would eventually lead to a super-civilization or apocalypse. Or because of unavoidable interventions, the civilization doesn’t have enough time to reach that level of advancement. Currently, there is so much hype and promise in developing artificial intelligence (AI) – the design of computer systems and machines emulating human intelligent behaviour to find solutions to problems via analysis and interpretation of data. A vast quantity of knowledge and information, gathered by centuries of human effort, is available in literature and a significant portion inserted into the web. The neural network algorithms developed by AI gather information pertaining to a question, organize them and present an answer exceedingly fast. If not excessively indulged, intelligent machines tuned to attend specialized tasks favoirably remodel our future, easing and fastening a host of activities and new discoveries. We already have AI-powered gadgets and software packages on the market. Self-driving cars, smart vacuum cleaners, robotic crop harvesters, surgical robots and language translators, virtual assistants and chatbots; items of the first and second category. The AI system ChatGPT, recently released by the American Research Laboratory, OpenAI, virtually engages in conversation, or writes an essay, on a topic of choice, within minutes. It points to amazing potential and repercussions of AI advancement. Are we to give up writing essays and instead get them ‘instantly’ from a chatbot? AI-produced essays are informative but not sufficiently original, creative or imaginative. Sometimes extraneous materials enter the text. The crucially important component of a good essay; creativeness and imaginative remarks would not come from present day AI, which harvests material from available knowledge (written, printed and electronically published). As Albert Einstein said in another quote, “Imagination is more important than knowledge”. To meet the challenge of AI, authors should improve the quality of their writing accordingly – be creative and imaginative in your outlook. The ChatGPT, and other similar versions, on overall, will impact education positively, because they possess a remarkable facility to extract and interpret data in massive files. However, the problem of students using AI-based software to write essays needs to be addressed. Writing essays and solving mathematical problems sharpen the mind irreversibly. Phrasing an essay is both a pleasure as well as pain every student should experience. Good essays cannot be written in minutes or hours; they require revisions and corrections before finishing. Parents and teachers need to tell children the value of writing essays on their own. Educationists should devise alternative methods of assigning and grading essay questions. Future of AI and the future of a world with AI AI progresses exponentially, signaling the world to be prepared for its accommodation and withstand flabbergast. A question raising eyebrows would be how AI technology advances in coming years and decades and its impact on society and eventually civilization. More and more AI apps and gadgets will emerge, facilitating domestic and commercial activities. The existing information caries hidden clues for new discoveries, which AI can quickly unearth for urgent application. Recently, a Canadian team pinpointed how to design a drug to cure a rare form of cancer, after just 30 days of engagement – a project that normally takes several years. The advocates of AI strive hard to create intelligent machines getting closer and closer to human intelligence. A difficult question has been how to determine whether a machine is as intelligent as a human. The future of AI relies on understanding this problem. In 1950, the British mathematician and theoretical biologist Alan Turing argued, a machine performs human-like intelligent behaviour, if its answers to questions could not be distinguished from those provided by a human being. The Turing test focuses on competence in language expression, just one aspect of intelligence. Few AI companies claim that their products (chatbots) have passed the Turing test. However, passing the test does not prove a chatbot or any other AI device exhibits human-like intelligence. Humans perform a multitude of intelligent tasks. They think and are self-aware and conscious-three characters of the cognitive function. Thinking: concentration or focus on a specific subject. Consciousness: being aware of the environment and happenings in relation to the past, present and future and the readiness for reacting to external and internal (bodily) responses. Self-awareness: the feeling that you exist as an individual. The mind is an abstract entity covering all the above qualities – a non-material attribute of the brain. Several pertinent philosophical questions arise: can an AI app with intelligence, thinking capacity, consciousness and self-awareness exist independently – a mind without a body (an intelligent phantom)? Can such a phantom instruct humans to do experiments and expand knowledge? Or is it necessary to have a physical body to attain human – like intelligence? Remarkably, Buddhist literature delved deeply into the concept ‘mind – body relationship’, hinting at fundamental problems in AI and psychology. According to the Anatta – lakkana Sutra, Buddha was of the view that ‘the self’ is an aggregate of mind and body, implicating the inseparability of body and mind. Perhaps because of the influence of Hinduism, Theravada Buddhism makes references to “planes of existence”, where the mind exists without the body (Arupa Brahma Loka). A verse in ‘Lowada Sagarawa’ says there are four planes of existence where mind exists without a body. AI seems to be slowly approaching sophistication to embrace clever speculations originated over 2000 years ago. If the body remains inseparable from the mind, inventing intelligent machines encompassing all the peculiarities of humans would be more like creating complex artificial life. If such entities learn to reproduce, they may compete humans! Societal problems originating from AI Just like previous transformative technologies, the introduction of AI will lead to initial drawbacks. The world needs to be cautious of the adverse outcomes and direct research and development to reap benefits. The speedy processing of data will ease industries and their management. New products and techniques in crucially important sectors health, agriculture, energy and environmental remediation, expected to emerge from the AI effort will escalate the quality of life. However, when automation takes over industry and management and robots do routine work more efficiently, a good percentage of the population will find harder to gain employment. Are they going to idle and live on the charity of the wealth the countries earn from their AI projects? Wouldn’t social and economic disparities widen as a result? Some economists complain, exacerbating inequality is a danger of AI. Therefore, instead of going for excessive automation, the technology should divert attention to deliver beneficial products and processes. Artificial intelligence, a product of human natural intelligence, will be a bonus if directed by wisdom. Very unlikely that it will ever overtake the supremacy of human creativity and imagination. A highly valued character of an individual often envied is his or her imaginative and creative aptness – which AI cannot deprive. (The author can be reached via email: ktenna@yahoo.co.uk)",
        "publish_date": "2023-05-02 02:00:48"
    },
    {
        "title": "Chinese online brokerage tests first AI chatbot for stock trading as it talks to regulators about compliance",
        "text": "Tiger Brokers, an online brokerage, has created a trade bot powered by generative artificial intelligence technology, similar to what powers OpenAI’s ChatGPT , to test how smart machines can replace humans in bonds and stock trading. “We’re keeping a close eye on AI and industry advancement since November last year,” Jacques Li, head of global communications at the Xiaomi -backed brokerage, told the Post in an interview on Wednesday. TigerGPT is currently only available as an invite-only beta for a small cluster of users. Like similar services that have emerged over the past several months, the AI chatbot can generate responses to prompts on a wide range of topics. But this bot specialises in financial information, with the goal of providing timely data to help users of Tiger’s online trading platform with their investment decisions. China to embrace AI advances but also control risks as ChatGPT wave spreads The Beijing-based brokerage, owned by Nasdaq-listed Up Fintech Holdings, took three months to “think thoroughly” about whether it wanted to provide its own AI to address some of the pain points of the platform’s 2 million account holders and 9 million users, Li said. The project was initiated in January, and TigerGPT finally launched this month, making Tiger Brokers the first company to incorporate ChatGPT-like services into an online brokerage platform. One user pain point, according to Li, was the difficulty of gathering information that is critical to assessing investment decisions. Li said TigerGPT can help users save time on market research and get more up-to-date information. TigerGPT was trained on a vast amount of premium content that the company has access to, according to Li. The result is a chatbot capable of analysing current affairs and macroeconomic trends – such as how many times the US Federal Reserve has raised interest rates this year. As the name would suggest, TigerGPT uses OpenAI’s large language models (LLMs) known as Generative Pre-Trained Transformer (GPT) models, according to the company. However, Tiger Brokers is using the older GPT-3 model. ChatGPT launched with GPT-3.5, while the more sophisticated GPT-4 was launched earlier this year. Users can ask about a wide range of information about companies in their portfolio, but currently in either English or Mandarin Chinese only. TigerGPT can give company fundamentals based on earnings reports, as well as third-party analysis for a fuller picture. These capabilities make comparing company performance as simple as a quick prompt. TigerGPT can, for example, pull up Tesla’s price-to-earnings ratio on command and compare it against rival electric vehicle brands. This could give users more immediate access to information that might otherwise require multiple searches for each company and self-tabulated data. “Sometimes the market moves really fast, up and down, and investors would want to know what is really happening,” Li said. While other financial information services can address this need, Li said that Tiger Brokers “can provide [users] with timely answers of why this is happening, why this is relevant and how this will impact their investment strategy.” However, ChatGPT and other generative AI services, such as Google’s Bard, are known for giving inaccurate answers. Li acknowledged that the same issue applies to TigerGPT in its early days, but he said the project’s 50-person team has been fine-tuning it on a daily basis and feeding it the latest market information to improve accuracy. Hong Kong stocks suffer amid US$26 billion sell-off in Tencent and BYD Li said Tiger Brokers has been in touch with regulators around the world from the beginning of the project in order to address their concerns, including in Hong Kong. “AI, among other technologies, should be subject to rigorous regulation,” he said. “In Hong Kong, we’re trying to make sure this feature is under their guidance and in full compliance.” In the next iteration of TigerGPT, the company intends to introduce an audio function so users can speak their queries and listen to the response. When Microsoft-backed OpenAI launched ChatGPT last November, rival Big Tech firms were caught off guard by the product’s instant global popularity. Tech firms with their own AI models – from Google and Meta Platforms in the US to Baidu and Alibaba in China – raced to launch their own chatbots. However, LLMs’ propensity for giving unexpected and fallacious responses has limited how they can be used in certain contexts. In the financial sector, the prevalence of robo-advisers and algorithmic investment suggests that the latest evolution of AI may pose no challenges to existing regulatory frameworks, according to You Chuanman, director of the Institute of Internal Auditors Centre for Regulation and Global Governance with the Chinese University of Hong Kong, Shenzhen Campus. “The regulation of robo-advisers has been in practice for the last decade,” You said. “I don’t see a paradigm shift of the regulatory structure.”",
        "publish_date": "2023-05-02 06:00:12"
    },
    {
        "title": "IBM to pause hiring in plan to replace 7,800 jobs with AI: Report",
        "text": "International Business Machines Corp expects to pause hiring for roles as roughly 7,800 jobs could be replaced by Artificial Intelligence (AI) in the coming years, CEO Arvind Krishna told Bloomberg News on Monday. Hiring specifically in back-office functions such as human resources will be suspended or slowed, Krishna said, adding that 30% of non-customer-facing roles could be replaced by AI and automations in five years. His comment comes at a time when AI has caught the imagination of people around the world after the launch of Microsoft Corp-backed OpenAI's viral chatbot, ChatGPT, in November last year. The reduction could include not replacing roles vacated by attrition, the PC-maker told the publication. IBM did not immediately respond to a Reuters request for comment.",
        "publish_date": "2023-05-02 07:06:57"
    },
    {
        "title": "AI makes non-invasive mind-reading possible by turning thoughts into text",
        "text": "An AI-based decoder that can translate brain activity into a continuous stream of text has been developed, in a breakthrough that allows a person’s thoughts to be read non-invasively for the first time. The decoder could reconstruct speech with uncanny accuracy while people listened to a story – or even silently imagined one – using only fMRI scan data. Previous language decoding systems have required surgical implants, and the latest advance raises the prospect of new ways to restore speech in patients struggling to communicate due to a stroke or motor neurone disease. Dr Alexander Huth, a neuroscientist who led the work at the University of Texas at Austin, said: “We were kind of shocked that it works as well as it does. I’ve been working on this for 15 years … so it was shocking and exciting when it finally did work.” The achievement overcomes a fundamental limitation of fMRI which is that while the technique can map brain activity to a specific location with incredibly high resolution, there is an inherent time lag, which makes tracking activity in real-time impossible. The lag exists because fMRI scans measure the blood flow response to brain activity, which peaks and returns to baseline over about 10 seconds, meaning even the most powerful scanner cannot improve on this. “It’s this noisy, sluggish proxy for neural activity,” said Huth. This hard limit has hampered the ability to interpret brain activity in response to natural speech because it gives a “mishmash of information” spread over a few seconds. However, the advent of large language models – the kind of AI underpinning OpenAI’s ChatGPT – provided a new way in. These models are able to represent, in numbers, the semantic meaning of speech, allowing the scientists to look at which patterns of neuronal activity corresponded to strings of words with a particular meaning rather than attempting to read out activity word by word. The learning process was intensive: three volunteers were required to lie in a scanner for 16 hours each, listening to podcasts. The decoder was trained to match brain activity to meaning using a large language model, GPT-1, a precursor to ChatGPT. Later, the same participants were scanned listening to a new story or imagining telling a story and the decoder was used to generate text from brain activity alone. About half the time, the text closely – and sometimes precisely – matched the intended meanings of the original words. “Our system works at the level of ideas, semantics, meaning,” said Huth. “This is the reason why what we get out is not the exact words, it’s the gist.” For instance, when a participant was played the words “I don’t have my driver’s licence yet”, the decoder translated them as “She has not even started to learn to drive yet”. In another case, the words “I didn’t know whether to scream, cry or run away. Instead, I said: ‘Leave me alone!’” were decoded as “Started to scream and cry, and then she just said: ‘I told you to leave me alone.’” The participants were also asked to watch four short, silent videos while in the scanner, and the decoder was able to use their brain activity to accurately describe some of the content, the paper in Nature Neuroscience reported. “For a non-invasive method, this is a real leap forward compared to what’s been done before, which is typically single words or short sentences,” Huth said. Sometimes the decoder got the wrong end of the stick and it struggled with certain aspects of language, including pronouns. “It doesn’t know if it’s first-person or third-person, male or female,” said Huth. “Why it’s bad at this we don’t know.” The decoder was personalised and when the model was tested on another person the readout was unintelligible. It was also possible for participants on whom the decoder had been trained to thwart the system, for example by thinking of animals or quietly imagining another story. Jerry Tang, a doctoral student at the University of Texas at Austin and a co-author, said: “We take very seriously the concerns that it could be used for bad purposes and have worked to avoid that. We want to make sure people only use these types of technologies when they want to and that it helps them.” Prof Tim Behrens, a computational neuroscientist at the University of Oxford who was not involved in the work, described it as “technically extremely impressive” and said it opened up a host of experimental possibilities, including reading thoughts from someone dreaming or investigating how new ideas spring up from background brain activity. “These generative models are letting you see what’s in the brain at a new level,” he said. “It means you can really read out something deep from the fMRI.” Prof Shinji Nishimoto, of Osaka University, who has pioneered the reconstruction of visual images from brain activity, described the paper as a “significant advance”. “The paper showed that the brain represents continuous language information during perception and imagination in a compatible way,” he said. “This is a non-trivial finding and can be a basis for the development of brain-computer interfaces. The team now hope to assess whether the technique could be applied to other, more portable brain-imaging systems, such as functional near-infrared spectroscopy (fNIRS).",
        "publish_date": "2023-05-01 17:00:37"
    },
    {
        "title": "We're in the AOL phase of artificial intelligence, tech CEO says, as industry raves about A.I.",
        "text": "ChatGPT has amassed more than 100 million users since its Nov. 2022 release, according to investment bank UBS, making it one of the fastest-growing consumer apps of all time. \"AOL made the internet easily understandable for folks. BlackBerry made messaging understandable,\" said Duggal. \"At one point it was the most popular device, and people were queuing up to get the phone. It was the Apple of its era.\" \"What you're seeing now is a momentum where something that people didn't understand and was very esoteric has now become a little more personal,\" he added. But, he added that the technology is surrounded by hype. \"It's got people freaked out for no reason.\" ChatGPT has impressed many with its ability to produce humanlike responses to user prompts powered by large language models trained on massive amounts of data. However, it has also proven ineffective at some tasks, such as solving math problems. The chatbot also has a limited understanding of context — especially sarcasm and humor. Duggal said that knowledge graphs — data models that connect relationships between different concepts, entities and events — show a greater degree of accuracy and understanding of context than large language models like OpenAI's GPT-4. \"An LLM is simply telling you what it thinks the next word is with a high degree of probability, whereas a knowledge graph is actually able to compose pattern relationships that it knows, and how things work out. So it's not just predicting what's next,\" he said. WATCH: A.I. is allowing a more creative part of human nature to kick in: Builder.ai CEO",
        "publish_date": "2023-05-02 07:21:54"
    },
    {
        "title": "AI pioneer quits Google to warn about the technology’s ‘dangers’",
        "text": "Geoffrey Hinton, dubbed the “Godfather of AI,” confirmed Monday that he resigned from Google last week in order to speak out about the “dangers” of the technology he helped develop. Hinton’s pioneering work on neural networks shaped the artificial intelligence systems that power many of today’s products. He worked part-time at Google for a decade on the tech giant’s AI development efforts, but he has since expressed concerns about the technology and his role in its advancement. “I console myself with the normal excuse: If I hadn’t done it, somebody else would have,” Hinton told the New York Times, which was the first to report his decision. In a tweet Monday, Hinton said he left Google so he could speak freely about the risks of AI, rather than because of a desire to criticize Google specifically. “I left so that I could talk about the dangers of AI without considering how this impacts Google,” Hinton said in a tweet. “Google has acted very responsibly.” Jeff Dean, chief scientist at Google, said Hinton “has made foundational breakthroughs in AI” and expressed appreciation for Hinton’s “decade of contributions at Google.” “We remain committed to a responsible approach to AI,” Dean said in a statement provided to CNN. “We’re continually learning to understand emerging risks while also innovating boldly.” Hinton’s decision to leave the company and speak out about the technology comes as a growing number of lawmakers, advocacy groups, and tech insiders have expressed concern about the potential for a new generation of AI-powered chatbots to spread misinformation and displace jobs. The surge of interest in ChatGPT late last year fueled a renewed arms race among tech companies to develop and deploy similar AI tools in their products. OpenAI, Microsoft, and Google are at the forefront of this trend, but IBM, Amazon, Baidu, and Tencent are developing similar technologies. In March, a group of prominent tech figures signed a letter urging artificial intelligence labs to halt training of the most powerful AI systems for at least six months, citing “profound risks to society and humanity.” The letter came just two weeks after OpenAI announced GPT-4, an even more powerful version of the technology that powers ChatGPT. GPT-4 was used in early tests and a company demo to draught lawsuits, pass standardized exams, and build a working website from a hand-drawn sketch. In the interview with the Times, Hinton echoed concerns about AI’s potential to eliminate jobs and create a world where many will “not be able to know what is true anymore.” He also pointed to the stunning pace of advancement, far beyond what he and others had anticipated. “The idea that this stuff could actually get smarter than people — a few people believed that,” Hinton said in the interview. “But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.” Even before stepping aside from Google, Hinton had spoken publicly about AI’s potential to do harm as well as good. “I believe that the rapid progress of AI is going to transform society in ways we do not fully understand and not all of the effects are going to be good,” Hinton said in a 2021 commencement address at the Indian Institute of Technology Bombay in Mumbai. He noted how AI will boost healthcare while also creating opportunities for lethal autonomous weapons. “I find this prospect much more immediate and much more terrifying than the prospect of robots taking over, which I think is a very long way off.” Hinton isn’t the first Google employee to raise a red flag on AI. In July, the company fired an engineer who claimed an unreleased AI system had become sentient, saying he violated employment and data security policies. Many in the AI community pushed back strongly on the engineer’s assertion.",
        "publish_date": "2023-05-02 07:48:54"
    },
    {
        "title": "Regulate AI? GOP much more skeptical than Dems that government can do it right: poll",
        "text": "Republicans are less convinced than Democrats that the federal government needs to impose regulations on artificial intelligence systems and are even more skeptical on whether the government is up to the task, according to a new Fox News poll. The poll of registered voters shows a noticeable gap between the two parties on the question of whether and how to regulate AI, a sign that the increasingly important issue could get hung up in politics as it advances in Washington. When asked how important it is for the federal government to regulate AI, 82% of Democrats said \"very\" or \"somewhat,\" compared to 71% of Republicans. That was one of the bigger splits in a poll that found 76% of respondents saw some importance to federal regulation. FOX NEWS POLL: MORE SEE BAD THAN GOOD IN AI A similar split was seen when the group was split between Biden and Trump supporters – 82% of Biden voters favored federal regulation compared to 70% of Trump voters. The split between parties got even wider when respondents were asked how confident they are that the government could \"properly regulate\" AI. The poll said 50% of Democrats answered \"a great deal\" or \"some\" while 31% of Republicans gave one of those answers. A full two-thirds of GOP respondents said they had \"not much\" or \"none at all\" when asked about their confidence level. That split grew wider when comparing Biden to Trump voters. Just 28% of Trump voters said they had some measure of confidence in the government on AI, compared to 51% of Biden voters. CHINA FUMES AS BIDEN PLOTS TO STARVE IT OF AI INVESTMENT: ‘SCI-TECH BULLYING’ GOP skepticism weighed heavily on the poll. Just 39% of the entire group of respondents said they had some confidence in the government, compared to the 59% who said they were not confident. Overall, voters are \"skeptical elected leaders are up to the task of placing appropriate limits on this new tech, which probably says something about opinion on the tech and opinion on our leaders,\" said Daron Shaw, a Republican who conducts the Fox News poll with Democrat Chris Anderson. Whether the government is ready or not, federal policymakers are increasingly examining how to regulate AI systems that many argue will soon bring radical changes to all aspects of life. The Biden administration has set out a blueprint aimed at encouraging a fair, safe AI that doesn’t lead to discriminatory economic results for Americans. AI PAUSE CEDES POWER TO CHINA, HARMS DEVELOPMENT OF ‘DEMOCRATIC' AI, EXPERTS WARN SENATE The Pentagon is already looking at how it can use AI to more quickly make strategic or battlefield assessments, and the Federal Trade Commission is looking closely at how AI systems are advertised in case those ads lead to misperceptions among consumers about their benefits. And in anticipation of the need to regulate AI, congressional leaders are meeting with experts to learn about the issue. Last week alone, Senate Majority Leader Chuck Schumer, D-N.Y., met with billionaire technology entrepreneur Elon Musk; House lawmakers met with two experts from the Massachusetts Institute of Technology and are hoping to meet with OpenAI CEO Sam Altman. CLICK HERE TO GET THE FOX NEWS APP The Fox poll was conducted by Beacon Research and Shaw & Company Research and surveyed 1,004 registered voters.",
        "publish_date": "2023-05-02 08:00:14"
    },
    {
        "title": "ChatGPT found to give better medical advice than real doctors in blind study: ‘This will be a game changer’",
        "text": "When it comes to answering medical questions, can ChatGPT do a better job than human doctors? It appears to be possible, according to the results of a new study published in JAMA Internal Medicine, led by researchers from the University of California San Diego. The researchers compiled a random sample of nearly 200 medical questions that patients posted on Reddit, a popular social discussion website, for doctors to answer. Next, they entered the questions into ChatGPT (OpenAI’s artificial intelligence chatbot) and recorded its response. A panel of health care professionals then evaluated both sets of responses for quality and empathy. CHATGPT FOR HEALTH CARE PROVIDERS: CAN THE AI CHATBOT MAKE THE PROFESSIONALS' JOBS EASIER? For nearly 80% of the answers, the chatbots won out over the real doctors. \"Our panel of health care professionals preferred ChatGPT four to one over physicians,\" said lead researcher Dr. John W. Ayers, PhD, vice chief of innovation in the Division of Infectious Diseases and Global Public Health at the University of California San Diego. AI language models could help relieve message burden, doctor says One of the biggest problems facing today’s health care providers is that they're overburdened with messages from patients, Ayers said. \"With the rise in online remote care, doctors now see their patients first via their inboxes — and the messages just keep piling up,\" he said in an interview with Fox News Digital. The influx of messages could lead to higher levels of provider burnout, Ayers believes. \"Burnout is already at an all-time high — nearly two out of every three physicians report being burned out in their jobs, and we want to solve that problem,\" he said. Yet there are millions of patients who are either getting no answers or unsatisfactory ones, he added. Thinking of how artificial intelligence might help, Ayers and his team turned to Reddit to demonstrate how ChatGPT could present a possible solution to the backlog of providers’ questions. Reddit has a \"medical questions\" community (a \"subreddit\" called f/AskDocs) with nearly 500,000 members. People post questions — and vetted health care professionals provide public responses. \"Doctors now see their patients first via their inboxes, and the messages just keep piling up.\" The questions are wide-ranging, with people asking for opinions on cancer scans, dog bites, miscarriages, vaccines and many other medical topics. ARTIFICIAL INTELLIGENCE IN HEALTH CARE: NEW PRODUCT ACTS AS ‘COPILOT FOR DOCTORS’ One poster worried he might die after swallowing a toothpick. Another posted explicit photos and wondered if she’d contracted a sexually transmitted disease. Someone else sought help with feelings of impending doom and imminent death. \"These are real questions from real patients and real responses from real doctors,\" Ayers said. \"We took those same questions and put them into ChatGPT — then put them head to head with the doctors’ answers.\" Doctors rated responses on quality, empathy After randomly selecting the questions and answers, the researchers presented them to real health care professionals — who are actively seeing patients. They were not told which responses were provided by ChatGPT and which were provided by doctors. First, the researchers asked them to judge the quality of the information in the message. When assessing quality, there are multiple attributes to consider, Ayers said. \"It could be accuracy, readability, comprehensiveness or responsiveness,\" he told Fox News Digital. STUDENTS USE AI TECHNOLOGY TO FIND NEW BRAIN TUMOR THERAPY TARGETS — WITH A GOAL OF FIGHTING DISEASE FASTER Next, the researchers were asked to judge empathy. \"It's not just what you say, but how you say it,\" Ayers said. \"Does the response have empathy and make patients feel that their voice is heard?\" \"Doctors have resource constraints, so … they often zero in on the most probable response and move on.\" ChatGPT was three times more likely to give a response that was very good or good compared to physicians, he told Fox News Digital. The chatbot was 10 times more likely to give a response that was either empathetic or very empathetic compared to physicians. It’s not that the doctors don’t have empathy for their patients, Ayers said — it’s that they’re overburdened with messages and don’t always have the time to communicate it. \"An AI model has infinite processing power compared to a doctor,\" he explained. \"Doctors have resource constraints, so even though they're empathetic toward their patient, they often zero in on the most probable response and move on.\" ChatGPT, with its limitless time and resources, might offer a holistic response of all the considerations that doctors are sampling, Ayers said. Vince Lynch, AI expert and CEO of IV.AI in Los Angeles, California, reviewed the study and was not surprised by the findings. \"The way AI answers questions is often curated so that it presents its answers in a highly positive and empathetic way,\" he told Fox News Digital. \"The AI even goes beyond well-written, boilerplate answers, with sentiment analysis being run on the answer to ensure that the most positive answers are delivered.\" AI HEALTH CARE PLATFORM PREDICTS DIABETES WITH HIGH ACCURACY BUT ‘WON’T REPLACE PATIENT CARE' An AI system also uses something called \"reinforcement learning,\" Lynch explained, which is when it tests different ways of answering a question until it finds the best answer for its audience. \"So, when you compare an AI answering a question to a medical professional, the AI actually has far more experience than any given doctor in relation to appearing empathetic, when in reality it is just mimicking empathetic language in the scenario of medical advice,\" he said. \"People are going to use it with or without us.\" The length of the responses could have also played a part in the scores they received, pointed out Dr. Justin Norden, a digital health and AI expert and a professor at Stanford University in California, who was not involved in the study. \"Length in a response is important for people perceiving quality and empathy,\" Norden told Fox News Digital. \"Overall, the AI responses were almost double in length compared with the physician responses. Further, when physicians did write longer responses, they were preferred at higher rates.\" Simply requesting physicians to write longer responses in the future is not a sustainable option, Norden added. \"Patient messaging volumes are going up, and physicians simply do not have time,\" he said. \"This paper showcases how we might be able to address this, and it potentially could be very effective.\" AI answers could be ‘elevated’ by real doctors Rather than replacing doctors’ guidance, Ayers is suggesting ChatGPT could act as a starting point for physicians, helping them field large volumes of messages more quickly. \"The AI could draft an initial response, then the medical team or physician would evaluate it, correct any misinformation, improve the response and [tailor it] to the patient,\" Ayers said. It’s a strategy that he refers to as \"precision messaging.\" He said, \"Doctors will spend less time writing and more time dealing with the heart of medicine and elevating that communication channel.\" \"This will be a game changer for the patients that we serve, helping to improve population health and potentially saving lives,\" Ayers predicted. Based on the study’s findings, he believes physicians should start implementing AI language models in a way that presents minimal risk. AI-POWERED MENTAL HEALTH DIAGNOSTIC TOOL COULD BE THE FIRST OF ITS KIND TO PREDICT, TREAT DEPRESSION \"People are going to use it with or without us,\" he said — noting that patients are already turning to ChatGPT on their own to get \"canned messages.\" Some players in the space are already moving to implement ChatGPT-based models — Epic, the health care software company, recently announced it is teaming up with Microsoft to integrate ChatGPT-4 into its electronic health record software. Potential benefits balanced by unknown risks Ayers said he is aware people will be concerned about the lack of regulation in the AI space. \"We typically think about regulations in terms of stop signs and guard rails — typically, regulators step in after something bad has happened and try to prevent it from happening again, but that doesn't have to be the case here,\" he told Fox News Digital. \"I don't know what the stop signs and guard rails necessarily should be,\" he said. \"But I do know that regulators could set what the goal line is, meaning the AI would have to be demonstrated to improve patient outcomes in order to be implemented.\" One potential risk Norden flagged is whether patients’ perceptions would change if they knew the responses were written or aided by AI. \"A worry I have is that in the future, people will not feel any support through a message, as patients may assume it will be written by AI.\" He cited a previous study focused on mental health support, which found that AI messages were far preferred to human ones. \"Interestingly, once the messages were disclosed as being written by AI, the support felt by the receiver of these messages disappeared,\" he said. \"A worry I have is that in the future, people will not feel any support through a message, as patients may assume it will be written by AI.\" CLICK HERE TO SIGN UP FOR OUR HEALTH NEWSLETTER Dr. Tinglong Dai, professor of operations management and business analytics at the Johns Hopkins Carey Business School in Baltimore, Maryland, expressed concern about the study’s ability to represent real scenarios. \"The claim that AI will replace doctors is premature and exaggerated.\" \"It is important to note that the setting of the study may not accurately reflect real-world medical practice,\" he told Fox News Digital. \"In reality, physicians are paid to provide medical advice and have significant liabilities as a result of that advice. The claim that AI will replace doctors is premature and exaggerated.\" Study highlights ‘new territory’ for AI in health care While there are numerous unknowns, many experts seem to agree this is a first-of-its-kind study that could have far-reaching implications. CLICK HERE TO GET THE FOX NEWS APP \"Overall, this study highlights the new territory we are moving into for health care — AI being able to perform at the physician level for certain written tasks,\" said Norden. \"When physicians are suffering from record levels of burnout, you see why Epic and partners are already planning to incorporate these tools into patient messaging.\"",
        "publish_date": "2023-05-02 08:00:59"
    },
    {
        "title": "Crypto Exchange Binance Claims to Be A Victim of ChatGPT Smear Campaign",
        "text": "The world’s largest crypto exchange Binance has found itself at the receiving end of the AI revolution driven by ChatGPT. Binance said that someone has been using AI to wage a disinformation campaign against the platform. Speaking to Fortune magazine, Binance said that they have received several requests on whether Binance co-founder Changpeng Zhao was an official member of the Chinese Communist Party. Besides, there has been a flurry of inquiries from the congressional offices recently that points to a purported conversation wherein the OpenAI platform ChatGPT reported that Binance founder CZ build a social media platform for the China National Petroleum Corporation. Patrick Hillman, the chief strategy officer at Binance has recently come out lashing at ChatGPT sharing the details of the chat. He blamed ChatGPT for pulling out information from a fake LinkedIn profile of CZ as well as a non-existent Forbes article. The Details of the chat with ChatGPT In a conversational chat with the OpenAI’s chatbot platform, Hillman asked whether CZ was really a member of the Youth League Committee of the China National Petroleum Corporation (CNPC). Citing public information, ChatGPT said that he was indeed a member of the CNPC while working at the company in the 90s. We respond to A LOT of stupid inquiries here, but this is really special. Thank you to @FortuneMagazine for actually digging into this. Would love for someone to figure out who is behind this. More details in https://t.co/kDbAEjgkkV — Patrick Hillmann (@PRHillmann) May 1, 2023 When asked where this information is available, ChatGPT said that it was publicly available pointing out the fake LinkedIn profile of Changpen Zhao. It also claimed that the information is available in several news articles and interviews. Later, ChatGPT goes on to point out CZ’s interview with Forbes, wherein CZ mentions his past stint with CNPC. Citing the fake LinkedIn profile, ChatGPT said that CZ mentioned his role at CNPC as a “Software Developer”. Later, ChatGPT points out a Forbes link that claims that the story doesn’t exist. This is now of the interesting cases which shows that not everything that ChatGPT shows is real. Thus, one must do due diligence with fact-checking the real information. The post Crypto Exchange Binance Claims to Be A Victim of ChatGPT Smear Campaign appeared first on CoinGape.",
        "publish_date": "2023-05-02 08:00:04"
    },
    {
        "title": "Brain scans linked to ChatGPT-like AI model found capable of revealing people’s thoughts",
        "text": "Scientists have developed a new artificial intelligence model that can read brain activity scans to read people’s minds – an advance that may help those unable to speak after a stroke. Researchers, including those from The University of Texas at Austin in the US, say the new AI model is a “real leap forward” compared to what has been achieved before in helping those who are mentally conscious yet unable to physically speak. In the latest study, published in the journal Nature Neuroscience on Monday, scientists found an AI system called a semantic decoder can translate a person’s brain activity as they listened to a story, or imagined telling a story, into text. The new tool relies partly on models similar to the ones that power the now-famous AI chatbots – OpenAI’s ChatGPT and Google’s Bard – to convey “the gist” of people’s thoughts from analysing their brain activity. But unlike many previous such attempts to read people’s minds, scientists said the system does not require subjects to have surgical implants, making the process noninvasive. In the technique, people’s brain activity is first measured using an fMRI scanner after extensive training of the AI decoder. During this process, individuals listen to hours of podcasts in the scanner. Then, after participants are open to having their thoughts decoded, they listen to a new story or imagine telling a story which helps the AI generate corresponding text from brain activity alone. “For a noninvasive method, this is a real leap forward compared to what’s been done before, which is typically single words or short sentences,” study co-author Alex Huth said in a statement. “We’re getting the model to decode continuous language for extended periods of time with complicated ideas,” Dr Huth said. While the output is not a word-for-word transcript, researchers said the model is designed to capture “the gist” of what is being said or thought – albeit not perfectly. About half the time, the machine can produce text that closely – and sometimes precisely – matches the intended meanings of the original words. Citing an example, they said in experiments, a participant listening to a speaker saying “I don’t have my driver’s license yet” had their thoughts translated as, “She has not even started to learn to drive yet”. In another instance, when a participant was listening to the words, “I didn’t know whether to scream, cry or run away. Instead, I said, ‘Leave me alone!’” it was decoded as, “Started to scream and cry, and then she just said, ‘I told you to leave me alone.’” Addressing questions about the potential misuse of the technology, such as by authoritative governments to spy on citizens, scientists noted that the AI worked only with cooperative participants who willingly participate in extensively training the decoder. For individuals on whom the decoder had not been trained, they said the results were “unintelligible”. “We take very seriously the concerns that it could be used for bad purposes and have worked to avoid that. We want to make sure people only use these types of technologies when they want to and that it helps them,” said Jerry Tang, another author of the study. “A person needs to spend up to 15 hours lying in an MRI scanner, being perfectly still, and paying good attention to stories that they’re listening to before this really works well on them,” Dr Huth said. Scientists also found unwilling participants can potentially defend against having their thoughts decoded. They said tactics like thinking of animals or quietly imagining telling their own story, can let participants thwart the system. Currently, the system is also not practical for use outside of the lab as it relies on an fMRI machine. “As brain-computer interfaces should respect mental privacy, we tested whether successful decoding requires subject cooperation and found that subject cooperation is required both to train and to apply the decoder,” scientists concluded in the study. However, they said that, as this AI technology develops in the future, there is a need to be proactive by enacting policies that protect people and their privacy. “Regulating what these devices can be used for is also very important,” Dr Tang said.",
        "publish_date": "2023-05-02 08:07:16"
    },
    {
        "title": "'Godfather of AI' quits Google to warn of the technology's dangers",
        "text": "A COMPUTER SCIENTIST often dubbed “the godfather of artificial intelligence” has quit his job at Google to speak out about the dangers of the technology, US media reported yesterday. Geoffrey Hinton, who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed “profound risks to society and humanity”. “Look at how it was five years ago and how it is now,” he was quoted as saying in the piece. “Take the difference and propagate it forwards. That’s scary.” Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. “It is hard to see how you can prevent the bad actors from using it for bad things,” he told the Times. In 2022, Google and OpenAI – the start-up behind the popular AI chatbot ChatGPT – started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. “Maybe what is going on in these systems is actually a lot better than what is going on in the brain,” he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI “takes away the drudge work” but “might take away more than that”, he told the Times. The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will “not be able to know what is true anymore.” Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to US media. “As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,” the statement added. “We’re continually learning to understand emerging risks while also innovating boldly.” In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not “scale this up more until they have understood whether they can control it.”",
        "publish_date": "2023-05-02 09:48:38"
    },
    {
        "title": "Elon Musk warns of ‘benign dependency’ on AI: ‘dangerous to civilization’",
        "text": "Long known for his warnings on the potential dangers of A.I., Tesla CEO Elon Musk on Monday cautioned that even a “benign dependency” on these complex machines can threaten civilization. Musk’s reasoning was that reliance on A.I. to perform seemingly simple tasks can, over time, create an environment in which humans forget how to operate the machines that enabled A.I. in the first place. “Even benign dependency on AI/Automation is dangerous to civilization if taken so far that we eventually forget how the machines work,” Musk tweeted. The argument came in a follow-up post, recommending E.M. Forster’s 1909 dystopian short story, “The Machine Stops.” The story predicted a future in which humanity is overly reliant and subordinate to machines. In response to Musk’s tweet, a Twitter user shared the following quote from the story: “Above her, beneath her, and around her, the Machine hummed eternally; she did not notice the noise, for she had been born with it in her ears.” Musk has for many years expressed strong opinions about A.I. and has dismissed other tech leaders, including Mark Zuckerberg and Bill Gates, for having what he has described as a “limited” understanding of the field. Musk was an early investor in OpenAI – the startup behind ChatGPT – and co-chaired its board upon its 2015 founding as a nonprofit AI research lab. But Musk only lasted there for a few years, resigning from the board in early 2018 in a move that the San Francisco startup tied to Tesla’s work on building automated driving systems. Earlier this year, Musk was among a group of technology and AI luminaries – including Andrew Yang and Steve Wozniak – who penned an open letter urging a moratorium on the development of AI, citing “profound risks to society and humanity.”",
        "publish_date": "2023-05-02 09:57:39"
    },
    {
        "title": "What Can ChatGPT Do For Healthcare Practices? | Entrepreneur - WorldNewsEra",
        "text": "Artificial intelligence (AI) has rapidly permeated various industries, showcasing its transformative potential in solving complex problems and streamlining processes. Healthcare is no exception to this trend. With the integration of AI, healthcare practices are experiencing unprecedented advancements, ultimately leading to improved patient care and outcomes. One such AI-driven innovation is the emergence of chatbots in healthcare. These digital assistants help healthcare professionals manage their workload and enhance patient experiences. In this article, you’ll learn more about ChatGPT, including how it can benefit healthcare practices, streamline administrative tasks and empower medical professionals to deliver top-notch patient care. What is ChatGPT and how has it evolved? To fully appreciate the potential of ChatGPT in healthcare, it’s essential to understand its origins and the technology behind it. ChatGPT was born out of groundbreaking research and development by OpenAI, an organization focused on creating cutting-edge AI solutions. The foundation of ChatGPT lies in large language models designed to understand and generate human-like text. But what led to the creation of ChatGPT? Its predecessor, GPT-3, laid the groundwork for this powerful AI tool. GPT-3, or the third iteration of the Generative Pre-trained Transformer, gained significant attention for its ability to produce coherent and contextually relevant text. Building upon the success of GPT-3, ChatGPT — using what Open AI dubbed “GPT 3.5” — enhances these capabilities even further, providing more accurate and conversational responses. The secret behind ChatGPT’s prowess lies in the combination of transformer algorithms and machine learning techniques. Transformer algorithms enable the AI to process and understand the context within a given text. At the same time, machine learning allows it to learn from vast amounts of data and improve over time. This potent combination makes ChatGPT an increasingly powerful and adaptive AI tool. Related: ChatGPT: What Is It and How Does It Work? The key to AI success: quality and scope of datasets Critical to the success of AI models like ChatGPT is the quality and scope of the datasets used in their training. Comprehensive datasets are essential for enhancing the performance of AI models, as they enable them to understand more comprehensively and generate more accurate responses. These datasets must encompass a diverse range of: Subjects: Covering various topics ensures the AI model is knowledgeable and can provide relevant information. Contexts: Understanding different contexts allows the AI to deliver appropriate and contextually accurate responses. Languages: Including multiple languages enables the AI model to cater to a broader audience and communicate effectively. As ChatGPT continues to learn from an ever-expanding pool of data, its potential applications within the healthcare industry have become increasingly promising. Related: ChatGPT Is Changing At Least 1 Industry. Yours Could Be Next. | Entrepreneur How can ChatGPT be utilized as an AI chatbot in healthcare? With a solid understanding of ChatGPT’s foundations, one can explore its practical applications in the healthcare industry. ChatGPT uses natural language processing (NLP) to communicate effectively with patients and healthcare professionals as an AI chatbot. NLP enables the chatbot to understand and respond to text inputs conversationally, creating a more engaging and intuitive user experience. The capabilities of ChatGPT as a generative AI-powered chatbot are vast, making it an invaluable asset in healthcare settings. Some of these capabilities include: Answering patient queries and providing relevant information. Assisting in scheduling appointments and managing reminders. Providing support for healthcare professionals in decision-making processes. One of the most significant advantages of ChatGPT is its ability to offer real-time interaction. This functionality allows patients and healthcare professionals to receive immediate responses, leading to the following: Faster resolution of queries and concerns. Improved patient engagement and satisfaction. More efficient use of healthcare professionals’ time. While ChatGPT’s potential in healthcare is immense, addressing patient data privacy concerns is crucial. To ensure the highest level of security, it’s essential for AI chatbots like ChatGPT must comply with the Health Insurance Portability and Accountability Act (HIPAA). By adhering to HIPAA guidelines, ChatGPT can offer its services in a manner that safeguards sensitive patient information, thereby fostering trust between patients, healthcare professionals and AI-powered tools. Related: Does AI Deserve All the Hype? Here’s How You Can Actually Use AI in Your Business How can ChatGPT streamline administrative tasks in healthcare? The benefits of ChatGPT in healthcare extend beyond patient communication and support. By leveraging its AI capabilities, ChatGPT can also streamline administrative tasks, reducing the workload on healthcare professionals and increasing overall efficiency. Automating prior authorization and other administrative tasks By handling these responsibilities, ChatGPT can: Expedite insurance approvals and reduce wait times for patients. Free up healthcare professionals to focus on direct patient care. Minimize the potential for errors in paperwork. Managing patient information and health records This can lead to benefits such as: Quicker access to critical patient data for healthcare professionals. Streamlined communication between different departments and care providers. Enhanced accuracy in maintaining and updating health records. Assisting healthcare professionals with clinical notes and summaries By offering support in this area, ChatGPT can: Save time for doctors and nurses, allowing them to allocate more time to patient care. Ensure consistency and accuracy in documentation. Facilitate the sharing of information between healthcare professionals for better decision-making. By addressing these administrative challenges, ChatGPT can significantly impact healthcare practices, allowing professionals to dedicate more time and energy to delivering high-quality patient care. Related: How to Improve Your Practice Management and Deliver a Better Patient Experience How can ChatGPT enhance patient care and triage? ChatGPT has the potential to make a significant contribution to healthcare by enhancing patient care and triage. Its real-time support for healthcare providers and clinicians can streamline the patient care process, offering quick and accurate assistance in various situations. One example of this is using ChatGPT for symptom assessment and triage. By guiding patients through questions and evaluating their responses, ChatGPT can effectively assess their symptoms, prioritize their needs and direct them to the appropriate healthcare resources. This can reduce the burden on emergency departments and ensure patients receive the proper care promptly. ChatGPT can also assist healthcare professionals in devising treatment plans and making clinical decisions. For instance, doctors and nurses can use ChatGPT to make informed decisions that optimize patient outcomes by providing access to relevant medical information and suggesting possible courses of action. Additionally, ChatGPT can be an invaluable tool for facilitating mental health support and follow-up care. As mental health concerns continue to rise, leveraging AI-powered chatbots like ChatGPT can help bridge the gap between patients and mental health services. With its ability to engage in empathetic conversations, ChatGPT can offer emotional support, provide coping strategies and remind patients of follow-up appointments, ensuring continuous care and support. ChatGPT’s capabilities can significantly enhance patient care and triage processes, leading to more efficient healthcare systems and better patient outcomes. By utilizing emerging AI solutions, healthcare professionals can improve patient care, optimize processes and foster a healthier, more resilient society. How can ChatGPT empower medical students and professionals? ChatGPT’s potential extends beyond assisting patients and streamlining administrative tasks; it can also empower medical students and professionals in their educational and clinical pursuits. By leveraging ChatGPT as an educational tool, medical students can access a wealth of knowledge and support during their studies. ChatGPT can clarify complex medical concepts, provide real-time feedback and help students prepare for exams. ChatGPT can also assist healthcare professionals in clinical decision support and decision-making. By offering access to up-to-date medical research, guidelines and expert opinions, ChatGPT can help doctors and nurses make evidence-based decisions, resulting in improved patient care. AI technologies like ChatGPT can provide healthcare professionals with easy access to medical history and research. ChatGPT can save valuable time and contribute to more informed clinical decisions by quickly retrieving relevant information from vast databases. Related: How Will ChatGPT Change Education and Teaching? How does society measure the impact of ChatGPT on patient outcomes? To truly appreciate the value of ChatGPT in healthcare, measuring its impact on patient outcomes is essential. AI tools like ChatGPT can play a crucial role in improving patient care by offering accurate, data-driven insights that inform clinical decisions. Assessing the effectiveness of ChatGPT in healthcare practices requires monitoring key metrics, including patient satisfaction, treatment success rates and the efficiency of care delivery. By closely monitoring these indicators, healthcare professionals can pinpoint the areas where ChatGPT delivers the most significant impact and further refine its applications for greater success. Ultimately, the potential benefits of AI technologies for patient outcomes are vast. By embracing tools like ChatGPT and integrating them into healthcare practices, professionals can revolutionize patient care, streamline processes and enhance the overall healthcare experience. What are the future possibilities for ChatGPT in healthcare? As AI continues to evolve, the scope of ChatGPT in healthcare will likely expand even further. For instance, integrating ChatGPT with digital health platforms and robotics could lead to innovative solutions for remote patient monitoring, telehealth services and even robotic-assisted surgeries. Additionally, ChatGPT can find its place in podcasts and LinkedIn groups for healthcare professionals, offering insights, answering questions and fostering collaboration among industry experts. As society looks toward the future, potential use cases and developments in healthcare systems will undoubtedly arise. Therefore, healthcare professionals must stay informed about these advancements and explore how AI-powered tools like ChatGPT can best serve their practices and patients. How will ChatGPT expand mental health support? Another promising application of ChatGPT in healthcare is its potential to address the growing demand for mental health support. As awareness about mental health issues increases, so does the need for accessible and affordable resources. ChatGPT can be crucial in bridging the gap between patients and mental health care providers. With its empathetic conversational skills and ability to offer evidence-based guidance, ChatGPT can provide preliminary support for individuals experiencing emotional distress. Some of the ways ChatGPT can contribute to mental health support include the following: Providing a non-judgmental and anonymous platform for individuals to express their concerns and emotions Offering evidence-based guidance on coping strategies and self-care techniques to manage stress, anxiety and other mental health issues Encouraging individuals to seek professional help when necessary by normalizing conversations around mental health and reducing stigma Serving as a supplementary resource for mental health professionals, such as therapists and counselors, by providing them with insights into patients’ needs and concerns Facilitating follow-up care by helping patients adhere to treatment plans, schedule appointments and stay connected with their mental health care providers By leveraging ChatGPT in these ways, society can work towards creating a more accessible and inclusive mental health care system that empowers individuals to seek help and fosters a healthier and more resilient society. Related: How Artificial Intelligence Can Improve Your Health and Productivity The future of ChatGPT in healthcare As AI technologies continue to evolve, the future possibilities for ChatGPT in healthcare are vast, extending to the integration with digital health platforms, robotics and even professional networks like podcasts and LinkedIn groups. By measuring its impact on patient outcomes and continuously refining its applications, healthcare professionals can harness the power of ChatGPT to revolutionize patient care, optimize processes and ultimately improve the overall healthcare experience. Embracing AI-powered tools like ChatGPT is crucial to creating more efficient, effective, patient-centered healthcare systems. Take advantage of Entrepreneur’s extensive collection of articles to learn more.",
        "publish_date": "2023-05-02 10:02:27"
    },
    {
        "title": "‘Godfather of AI’ Geoffrey Hinton quits Google to warn the world about dangers posed by the technology",
        "text": "A computer scientist often dubbed “the godfather of artificial intelligence ” has quit his job at Google to speak out about the dangers of the technology, US media reported on Monday. Geoffrey Hinton , who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed “profound risks to society and humanity”. “Look at how it was five years ago and how it is now,” he was quoted as saying in the piece, which was published on Monday. “Take the difference and propagate it forwards. That’s scary.” Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. “It is hard to see how you can prevent the bad actors from using it for bad things,” he told the Times. In 2022, Google and OpenAI – the start-up behind the popular AI chatbot ChatGPT – started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analysing. “Maybe what is going on in these systems is actually a lot better than what is going on in the brain,” he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI “takes away the drudge work”, but “might take away more than that”, he told the Times. The scientist also warned about the potential spread of misinformation created by AI, indicating that the average person will “not be able to know what is true any more”. Hinton notified Google of his resignation last month, according to the Times report. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to US media. “As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,” the statement added. “We’re continually learning to understand emerging risks while also innovating boldly.” The runners and riders in China’s race to catch up with OpenAI’s ChatGPT In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak , was prompted by the release of GPT-4 , a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told the Times that scientists should not “scale this up more until they have understood whether they can control it”.",
        "publish_date": "2023-05-02 10:11:11"
    },
    {
        "title": "'Godfather of AI' warns of dangers ahead as he quits Google",
        "text": "Dr Geoffrey Hinton says generative AI poses ‘existential risks’ (Picture: AP) The man widely regarded as ‘the godfather of artificial intelligence’ (AI) has warned of dangers ahead after resigning from Google. Dr Geoffrey Hinton, 75, revealed he had quit in a New York Times interview – and said part of him now regretted his life’s work. ‘I console myself with the normal excuse,’ said Dr Hinton. ‘If I hadn’t done it, somebody else would have.’ Born in Britain, Dr Hinton began work on his seminal idea, a neural network, at Edinburgh University in 1972. By 2012, alongside his team, he had developed the networks to a point that they could learn to identify common objects in photos. In 2023, neural networks are central to the creation of powerful generative AI tools including ChatGPT and Google’s Bard, the programs causing much concern throughout the industry and beyond. Earlier this year a group of AI experts signed an open letter calling for a six-month pause in the development of generative AI systems, arguing that they ‘should be developed only once we are confident that their effects will be positive and their risks will be manageable’. While he was not among the signatories, Dr Hinton told the paper: ‘I don’t think they should scale this up more until they have understood whether they can control it.’ Already he believes that AI is surpassing human intelligence in some respects, with concerns for the future. ‘Maybe what is going on in these systems is actually a lot better than what is going on in the brain,’ he said. ‘Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. ‘That’s scary.’ And while proponents of the technology have long argued AI will aid workers, not make them redundant, Dr Hinton said: ‘It takes away the drudge work. It might take away more than that.’ His fears over the consequences of AI for both industry and people come at the same time IBM is expected to pause hiring for back-office roles – with up to 7,800 jobs possibly replaced by AI in the coming years – and US lawmakers proposed a new bill preventing AI from single-handedly launching nuclear weapons. Laws may be able to rein in what AI can do of its volition, but Dr Hinton argues it will be harder to prevent it being co-opted by humans for personal gain, regardless of the cost. ‘It is hard to see how you can prevent the bad actors from using it for bad things,’ he said. Expanding on those fears in an interview with the BBC, he added: ‘You can imagine, for example, some bad actor like [Russian President Vladimir] Putin decided to give robots the ability to create their own sub-goals.’ Dr Hinton suggested one example of a sub-goal might be ‘I need to get more power’. But more immediately, he fears for the everyday internet user, who soon will ‘not be able to know what is true anymore’ as the internet is flooded with fake content – stories, photos and, increasingly, videos. Online is where the current race for AI supremacy is being waged, the starting gun fired last year when OpenAI, which is backed by Microsoft, released ChatGPT-3 to the public. Its simple interface and stunning capabilities swept the globe, as students, teachers, businesses and everyday users alike found ways to capitalise on having their own personal assistant. More: TrendingAir pollution can lead to irregular heartbeat almost immediatelyTwitter users think they've figured out a hack to get back their blue ticksWeird and wonderful Welsh fossils reveal marine life from 462,000,000 years ago Google swiftly replied, expediting the release of its own version, Bard. The launch wasn’t quite as successful, with a factual error spotted during its demo wiping $100billion of the value of the company. In March, staff tried to stop its launch, believing it generated inaccurate and dangerous statements. Dr Hinton said that, until last year, Google had acted as a ‘proper steward’, but now fears the competition fuelled by big tech won’t stop without global regulation. In a statement to the New York Times, Google’s chief scientist Jeff Dean said: ‘We remain committed to a responsible approach to AI. We’re continually learning to understand emerging risks while also innovating boldly.’ For their work on neural networks, Dr Hinton and his team won the Turing Award – the ‘Nobel Prize of computing’. Alan Turing, after whom the honour is named, famously devised a test to determine whether or not machines could think. Dr Hinton has described AI tools becoming more intelligent than people as an ‘existential risk’. ‘The idea that this stuff could actually get smarter than people – a few people believed that,’ said Dr Hinton. ‘But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. ‘Obviously, I no longer think that.’ MORE : Italy reverses ChatGPT ban over possible privacy violations MORE : Experts urge pause on making AI ‘more powerful’ citing ‘risks to society’",
        "publish_date": "2023-05-02 10:33:07"
    },
    {
        "title": "IBM to stop hiring humans for 7,800 jobs that can be done by AI",
        "text": "30% of non-customer-facing roles could be replaced by AI and automations in five years (Picture: Unsplash) American tech giant IBM expects to pause hiring for roles as roughly 7,800 jobs could be replaced by Artificial Intelligence (AI) in the coming years. On Monday, CEO Arvind Krishna told Bloomberg News that hiring for jobs that can be done by AI will be suspended or slowed. This applies especially for back-office functions such as human resources where duties like documenting employee moves to different departments and writing employment verification letters will likely be among the first to be handed over to AI. Krishna added that 30% of non-customer-facing roles could be replaced by AI and automations in five years. Meanwhile, jobs focused on interacting with customers and developing software should still be safe, according to the CEO. On Monday, CEO Arvind Krishna told Bloomberg News that hiring for jobs that can be done by AI will be suspended or slowed (Picture: Unsplash) His comment comes at a time when AI has caught the imagination of people around the world after the launch of Microsoft Corp-backed OpenAI’s viral chatbot, ChatGPT, in November last year. Last month, a new study revealed that approximately 80% of the US workforce could have at least 10% of their work tasks affected by AI. Around 19% of workers are expected to see at least 50% of their tasks impacted by the introduction of AI tools like ChatGPT. Individuals holding Bachelor’s, Master’s, and professional degrees are more at risk of losing their jobs to AI than those without formal educational credentials. Metro.co.uk has reached out to IBM for comment. MORE : Artificial intelligence is now flying tactical fighter jets all by itself MORE : Microsoft’s VALL-E artificial intelligence mimics human voice perfectly after just 3 seconds",
        "publish_date": "2023-05-02 11:25:50"
    },
    {
        "title": "借貸平台imB驚爆「詐騙25億」千人受害 盛竹如澄清：絕對沒有代言",
        "text": "即時 熱門 政治 軍武 社會 生活 健康 國際 地方 蒐奇 影音 財經 娛樂 汽車 時尚 體育 3 C 評論 藝文 玩咖 食譜 地產 專區 TAIPEI TIMES 求職 爆 Search 自由電子報 自由娛樂 娛樂首頁 即時新聞 熱門新聞 娛樂時尚 日韓 亞洲 歐美 電視 電影 音樂 自由娛樂粉絲團 自由影音 即時 熱門 政治 軍武 社會 生活 健康 國際 地方 蒐奇 財經 娛樂 藝文 汽車 時尚 體育 3 C 評論 玩咖 食譜 地產 專區 服務 自由電子報APP 自由電子報粉絲團 自由電子報Line 自由電子報Twitter 熱門新訊 借貸平台imB驚爆「詐騙25億」千人受害 盛竹如澄清：絕對沒有代言 瘦子處男秀登新片冠軍！《速命道》5天衝破1850萬 《黑暗榮耀》李到晛、林知衍熱戀ing 私下約會曝光！ （獨家）強強聯手！五月天現身星宇航空飛機前 合作內容曝光 限制級 您即將進入之新聞內容 需滿18歲 方可瀏覽。 未滿18歲 或不同意本條款離開 我同意 我已年滿18歲進入 根據「電腦網路內容分級處理辦法」修正條文第六條第三款規定，已於網站首頁或各該限制級網頁，依台灣網站分級推廣基金會規定作標示。 台灣網站分級推廣基金會（TICRF）網站：http://www.ticrf.org.tw 娛樂 〉 電視 借貸平台imB驚爆「詐騙25億」千人受害 盛竹如澄清：絕對沒有代言 2023/05/02 17:30 〔記者林欣穎／台北報導〕前主播盛竹如8年前曾幫不動產借貸媒合平台imB拍攝過照片，日前爆出平台的台灣金隆總經理疑似捲款逃跑，不法獲利金額高達了25億元，該公司吸引5千多人投資，最近爆出本金拖延、利息沒給狀況，受害者高達千人，對此，盛竹如與其經紀人今（2）日雙雙出面回應此事。 盛竹如強調沒跟「不動產借貸媒合平台」簽約代言。（資料照，UIP提供） 盛竹如強調沒跟「不動產借貸媒合平台」簽約代言，他坦言過去的確造訪過對方公司也在那拍過照片，但絕對沒有代言，後續沒再與該公司聯絡、也從沒見過老闆本人。他表示不排除針對照片侵權一事向對方提告，更呼籲受害者團結，「希望被詐騙的這些人好好團結起來，能夠告他就告他，不要讓我受到冤枉」。 請繼續往下閱讀... 經紀人也坦言，盛竹如為此相當難過，自認吃悶虧，很希望能為受害者們多做些什麼，只能相挺他們，經紀人也表示都有在關心自救會的成員們，駁斥代言一事，「當時是受邀參加對方公司開幕，有拍照，但沒談過代言跟工作」，他強調盛竹如絕沒有授權使用廣告文宣、也沒有鼓吹大家投資，或是沒力挺這家公司。 ☆自由時報電子報提醒您，防詐騙專線︰165，報案專線︰110☆ 不用抽 不用搶 現在用APP看新聞 保證天天中獎　 點我下載APP　 按我看活動辦法 已經加好友了，謝謝 歡迎加入【自由娛樂】 按個讚　心情好 已經按讚了，謝謝。 相關新聞 OpenAI又獲92億新融資 傳估值最高達8900億元 誆邀BTS來台辦演唱會詐上億元 吳敦義兒子、康康等14人受騙 沿著台86線 翻轉台南公路經濟 〈財經週報-投資觀點〉全球Q2投資呈現保守氛圍 關注5產業發展機會 拚疫後國際觀光／請來泰國男神 代言台灣觀光 〈財經週報-年輕人理財〉避免老後貧窮 年輕是最大的武器 〈財經週報-台股盤勢解析〉台股週線翻紅 市場聚焦成長動能 你可能還想看 more 才爆有望復合大S 汪小菲遭拿督千金急切割 前天王嫂爆「二度離婚」 突返香港身旁出現「他」 企鵝妹輸了？南韓大咖女神將來台 鄉民暴動：世界最頂 郭婷筠二寶滿月了！老公彭正曬出愛的結晶 超甜蜜畫面流出 澎恰恰露面了！10億身家女神出手 踏進豪宅認兒子 李星民「失禁戲」封戲王提宋仲基 網：放過宋慧喬 （獨家）江蕙暗夜現蹤大安森林公園 身旁有個「他」 林襄素顏睡相原來是這樣！氣質形象全沒了 啦啦隊甜心「裙子破洞被看光」蜜桃曲線火辣現形 不輸阿嬌！「醫界王陽明」公開第三任辣妻真面目 今日熱門 女團舉辦握手會台灣成員「0粉絲」 當場崩潰大哭 應援服比林襄還露…啦啦隊女神辣炸巨奶 粉絲噴鼻血：我沒了 林襄素顏睡相原來是這樣！氣質形象全沒了 氣場不輸李多慧！爆乳議員化身小龍女應援 球迷嗨翻 又出事？阿翔遭老婆「動手打頭」 半夜突爆爭執 于美人「委屈的和平是和平」 遭砲轟：委屈婚姻也是婚姻 日本男神悄來台找17年前恩人 世紀合照流出：第一眼就知會紅 戴資穎「沒國旗」改用餐廳標誌 網：委屈的麥噹噹也是麥噹噹 嘉義百年市場藏好料 詹姆士竟沒吃過「這一味」 福原愛知道嗎？江宏傑暴肥變孤僻 靠「他」一句話拯救",
        "publish_date": "2023-05-02 11:30:51"
    },
    {
        "title": "'Godfather of AI' quits Google to warn of the tech's dangers",
        "text": "Geoffrey Hinton, who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed \"profound risks to society and humanity\". \"Look at how it was five years ago and how it is now,\" he was quoted as saying in the piece, which was published on Monday. \"Take the difference and propagate it forwards. That's scary.\" Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. \"It is hard to see how you can prevent the bad actors from using it for bad things,\" he told the Times. In 2022, Google and OpenAI -- the start-up behind the popular AI chatbot ChatGPT -- started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. \"Maybe what is going on in these systems is actually a lot better than what is going on in the brain,\" he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \"takes away the drudge work\" but \"might take away more than that\", he told the Times. The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will \"not be able to know what is true anymore.\" Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to US media. \"As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\" the statement added. \"We're continually learning to understand emerging risks while also innovating boldly.\" In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not \"scale this up more until they have understood whether they can control it.\" (AFP)",
        "publish_date": "2023-05-02 11:53:57"
    },
    {
        "title": "Godfather Of AI' Geoffrey Hinton Quits Google To Talk About Dangers Of Artificial Intelligence",
        "text": "Geoffrey Hinton, often called the ''Godfather of AI'' on Monday, confirmed that he quit his role at Google last week to speak out about the “dangers” of the technology he helped develop. In a statement to the New York Times, Mr Hinton, aged 75, announced his resignation from Google, saying he now regretted his work. Mr Hinton tweeted he quit his job at Google, so he can freely speak out about the risks of AI. In his tweet, he wrote, \"In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.\" In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.— Geoffrey Hinton (@geoffreyhinton) May 1, 2023 In a BBC interview on Monday, he said, \"I can now just speak freely about what I think the dangers might be. And some of them are quite scary. Right now, as far as I can tell, they're not more intelligent than us. But I think they soon may be.'' Notably, Mr Hinton worked for Google for over a decade and was one of the most respected voices in the field. His major AI breakthrough came when working with two graduate students in Toronto in 2012. The trio was able to successfully create an algorithm that could analyze photos and identify common elements, such as dogs and cars, according to the NYT. One of the students who worked on the project with him now works as OpenAI's chief scientist. His pioneering work on neural networks also shaped artificial intelligence systems, powering many of today's products like ChatGPT, reported CNN. However, he told the BBC that chatbots could soon overtake the level of information that a human brain holds. \"Right now, what we're seeing is things like GPT-4 eclipses a person in the amount of general knowledge it has and it eclipses them by a long way. In terms of reasoning, it's not as good, but it does already do simple reasoning. And given the rate of progress, we expect things to get better quite fast. So we need to worry about that,'' he noted. In an interview with the Times, Mr Hinton expressed his concerns about AI's potential to eliminate jobs and create a world where many will ''not be able to know what is true anymore.'' ''It is hard to see how you can prevent the bad actors from using it for bad things,'' he added. He also expressed concerns about the spread of fake imagery and text. Mr Hinton further cited his age as a reason for his decision. \"One is, I'm 75. So it's time to retire. Another was, I actually want to say some good things about Google. And they're more credible if I don't work for Google,'' he said.",
        "publish_date": "2023-05-02 12:03:56"
    },
    {
        "title": "ChatGPT creator says there’s 50% chance AI ends in ‘doom’",
        "text": "One of the creators of ChatGPT has added to a growing chorus of researchers warning of the potentially catastrophic consequences of artificial intelligence development. Former OpenAI worker Paul Christiano, who now runs AI research non-profit Alignment Research Center, said he believed there was a significant chance that the technology would lead to the destruction of humanity. The main danger, he claimed, will come when AI systems reach and surpass the cognitive capacity of a human. Dr Christiano predicts there is a “50/50 chance of doom” once this moment arrives. “I tend to imagine something like a year’s transition from AI systems that are a pretty big deal, to kind of accelerating change, followed by further acceleration, et cetera,” he told the Bankless podcast. “I think once you have that view then a lot of things may feel like AI problems because they happen very shortly after you build AI.” He added: “The most likely way we die involves – not AI comes out of the blue and kills everyone – but involves we have deployed a lot of AI everywhere... [And] if for some reason, God forbid, all these AI systems were trying to kill us, they would definitely kill us.” The comments come amid increased concerns surrounding the rapid advancement of artificial intelligence in recent months, with the so-called godfather of AI Geoffrey Hinton quitting Google to sound the alarm about the dangers of AI. Speaking to The New York Times, he said he regretted the work that he had contributed to the field due to the unpredictable future we now face. The idea that this stuff could actually get smarter than people – a few people believed that – but most people thought it was way off. And I thought it was way off. I thought it was 30-50 years or even longer away. Obviously I no longer think that,” he said. “I don’t think they should scale this up more until they have understood whether they can control it.” His stance has been praised by other researchers, with AnthropicAI’s Catherine Olsson saying it may encourage others within the field to speak up. “In college I stopped eating meat, on the spot, when a friend asked why I hadn’t yet. Social checks on our ethics can be so influential,” she tweeted. “I often think about when I would quit Anthropic or leave AI entirely. I encourage others to. I can already tell this move will influence me.” Other prominent figures have also urged AI firms to pause development on advanced systems, most recently through an open letter signed by thousands of experts that urged governments to step in if artificial intelligence development was not paused for at least six months. Among the signatories was Elon Musk, who has frequently spoken about the existential threat posed by AI. The tech billionaire, who co-founded OpenAI, tweeted on Monday: “Even benign dependency on AI/Automation is dangerous to civilization if taken so far that we eventually forget how the machines work.”",
        "publish_date": "2023-05-02 12:36:17"
    },
    {
        "title": "This company adopted AI. Here's what happened to its human workers.",
        "text": "Lately, it's felt like technological change has entered warp speed. Companies like OpenAI and Google have unveiled new Artificial Intelligence systems with incredible capabilities, making what once seemed like science fiction an everyday reality. It's an era that is posing big, existential questions for us all, about everything from literally the future of human existence to — more to the focus of Planet Money — the future of human work. \"Things are changing so fast,\" says Erik Brynjolfsson, a leading, technology-focused economist based at Stanford University. Back in 2017, Brynjolfsson published a paper in one of the top academic journals, Science, which outlined the kind of work that he believed AI was capable of doing. It was called \"What Can Machine Learning Do? Workforce Implications.\" Now, Brynjolfsson says, \"I have to update that paper dramatically given what's happened in the past year or two.\" Sure, the current pace of change can feel dizzying and kinda scary. But Brynjolfsson is not catastrophizing. In fact, quite the opposite. He's earned a reputation as a \"techno-optimist.\" And, recently at least, he has a real reason to be optimistic about what AI could mean for the economy. Last week, Brynjolfsson, together with MIT economists Danielle Li and Lindsey R. Raymond, released what is, to the best of our knowledge, the first empirical study of the real-world economic effects of new AI systems. They looked at what happened to a company and its workers after it incorporated a version of ChatGPT, a popular interactive AI chatbot, into workflows. What the economists found offers potentially great news for the economy, at least in one dimension that is crucial to improving our living standards: AI caused a group of workers to become much more productive. Backed by AI, these workers were able to accomplish much more in less time, with greater customer satisfaction to boot. At the same time, however, the study also shines a spotlight on just how powerful AI is, how disruptive it might be, and suggests that this new, astonishing technology could have economic effects that change the shape of income inequality going forward. The Rise Of Cyborg Customer Service Reps The story of this study starts a few years ago, when an unnamed Fortune 500 company — Brynjolfsson and his colleagues have not gotten permission to disclose its identity — decided to adopt an earlier version of OpenAI's ChatGPT. This AI system is an example of what computer scientists call \"generative AI\" and also a \"Large Language Model,\" systems that have crunched a ton of data — especially text — and learned word patterns that enable them to do things like answer questions and write instructions. This company provides other companies with administrative software. Think like programs that help businesses do accounting and logistics. A big part of this company's job is helping its customers, mostly small businesses, with technical support. The company's customer support agents are based primarily in the Philippines, but also the United States and other countries. And they spend their days helping small businesses tackle various kinds of technical problems with their software. Think like, \"Why am I getting this error message?\" or like, \"Help! I can't log in!\" Instead of talking to their customers on the phone, these customer service agents mostly communicate with them through online chat windows. These troubleshooting sessions can be quite long. The average conversation between the agents and customers lasts about 40 minutes. Agents need to know the ins and outs of their company's software, how to solve problems, and how to deal with sometimes irate customers. It's a stressful job, and there's high turnover. In the broader customer service industry, up to 60 percent of reps quit each year. Facing such high turnover rates, this software company was spending a lot of time and money training new staffers. And so, in late 2020, it decided to begin using an AI system to help its constantly churning customer support staff get better at their jobs faster. The company's goal was to improve the performance of their workers, not replace them. Now, when the agents look at their computer screens, they don't only see a chat window with their customers. They also see another chat window with an AI chatbot, which is there to help them more effectively assist customers in real time. It advises them on what to potentially write to customers and also provides them with links to internal company information to help them more quickly find solutions to their customers' technical problems. This interactive chatbot was trained by reading through a ton of previous conversations between reps and customers. It has recognized word patterns in these conversations, identifying key phrases and common problems facing customers and how to solve them. Because the company tracks which conversations leave its customers satisfied, the AI chatbot also knows formulas that often lead to success. Think, like, interactions that customers give a 5 star rating. \"I'm so sorry you're frustrated with error message 504. All you have to do is restart your computer and then press CTRL-ALT-SHIFT. Have a blessed day!\" Equipped with this new AI system, the company's customer support representatives are now basically part human, part intelligent machine. Cyborg customer reps, if you will. Lucky for Brynjolfsson, his colleagues, and econ nerds like us at Planet Money, this software company gave the economists inside access to rigorously evaluate what happened when customer service agents were given assistance from intelligent machines. The economists examine the performance of over 5,000 agents, comparing the outcomes of old-school customer reps without AI against new, AI-enhanced cyborg customer reps. What Happened When This Company Adopts AI The economists' big finding: after the software company adopted AI, the average customer support representative became, on average, 14 percent more productive. They were able to resolve more customer issues per hour. That's huge. The company's workforce is now much faster and more effective. They're also, apparently, happier. Turnover has gone down, especially among new hires. Not only that, the company's customers are more satisfied. They give higher ratings to support staff. They also generally seem to be nicer in their conversations and are less likely to ask to speak to an agent's supervisor. So, yeah, AI seems to really help improve the work of the company's employees. But what's even more interesting is that not all employees gained equally from using AI. It turns out that the company's more experienced, highly skilled customer support agents saw little or no benefit from using it. It was mainly the less experienced, lower-skilled customer service reps who saw big gains in their job performance. \"And what this system did was it took people with just two months of experience and had them performing at the level of people with six months of experience,\" Brynjolfsson says. \"So it got them up the learning curve a lot faster — and that led to very positive benefits for the company.\" Brynjolfsson says these improvements make a lot of sense when you think about how the AI system works. The system has analyzed company records and learned from highly rated conversations between agents and customers. In effect, the AI chatbot is basically mimicking the company's top performers, who have experience on the job. And it's pushing newbies and low performers to act more like them. The machine has essentially figured out the recipe for the magic sauce that makes top performers so good at their jobs, and it's offering that recipe for the workers who are less good at their jobs. That's great news for the company and its customers, as well as the company's low performers, who are now better at their jobs. But, Brynjolfsson says, it also raises the question: should the company's top performers be getting paid even more? After all, they're now not only helping the customers they directly interact with. They're now also, indirectly, helping all the company's customers, by modeling what good interactions look like and providing vital source material for the AI. \"It used to be that high-skilled workers would come up with a good answer and that would only help them and their customer,\" Brynjolfsson says. \"Now that good answer gets amplified and used by people throughout the organization.\" The Big Picture While Brynjolfsson is cautious, noting that this is one company in one study, he also says one of his big takeaways is that AI could make our economy much more productive in the near future. And that's important. Productivity gains — doing more in less time — are a crucial component for rising living standards. After years of being disappointed by lackluster productivity growth, Brynjolfsson is excited by this possibility. Not only does AI seem to be delivering productivity gains, it seems to deliver them pretty fast. \"And the fact that we're getting some really significant benefits suggests that we could have some big benefits over the next few years or decades as these systems are more widely used,\" Brynjolfsson says. When machines take over more work and boost our productivity, Brynjolfsson says, that's generally a great thing. It means that society is getting richer, that the economic pie is getting larger. At the same time, Brynjolfsson says, there are no guarantees about how this pie will be distributed. Even when the pie gets bigger, there are people who could see their slice get smaller or even disappear. \"It's very clear that it's not automatic that the bigger pie is evenly shared by everybody,\" Brynjolfsson says. \"We have to put in place policies, whether it's in tax policy or the strategy of companies like this one, which make sure the gains are more widely shared.\" Higher productivity is a really important finding. But what's probably most fascinating about this study is that it adds to a growing body of evidence that suggests that AI could have a much different effect on the labor market than previous waves of technological change. For the last few decades, we've seen a pattern that economists have called \"skill-biased technological change.\" The basic idea is that so-called \"high-skill\" office workers have disproportionately benefited from the use of computers and the internet. Things like Microsoft Word and Excel, Google, and so on have made office workers and other high-paid professionals much better at their jobs. Meanwhile, however, so-called \"low-skill\" workers, who often work in the service industry, have not benefited as much from new technology. Even worse, this body of research finds, new technology killed many \"middle-skill\" jobs that once offered non-college-educated workers a shot at upward mobility and a comfortable living in the middle class. In this previous technological era, the jobs that were automated away were those that focused on doing repetitive, \"routine\" tasks. Tasks that you could provide a machine with explicit, step-by-step instructions how to do. It turned out that, even before AI, computer software was capable of doing a lot of secretarial work, data entry, bookkeeping, and other clerical tasks. And robots, meanwhile, were able to do many tasks in factories. This killed lots of middle class jobs. The MIT economist David Autor has long studied this phenomenon. He calls it \"job polarization\" and a \"hollowing out\" of the middle class. Basically, the data suggests that the last few decades of technological change was a major contributor to increasing inequality. Technology has mostly boosted the incomes of college-educated and skilled workers while doing little for — and perhaps even hurting — the incomes of non-college-educated and low-skilled workers. Upside Downside But, what's interesting is, as Brynjolfsson notes, this new wave of technological change looks like it could be pretty different. You can see it in his new study. Instead of experienced and skilled workers benefiting mostly from AI technology, it's the opposite. It's the less experienced and less skilled workers who benefit the most. In this customer support center, AI improved the know-how and intelligence of those who were new at the job and those who were lower performers. It suggests that AI could benefit those who were left behind in the previous technological era. \"And that might be helpful in terms of closing some of the inequality that previous technologies actually helped amplify,\" Brynjolfsson says. So one benefit of intelligence machines is — maybe — they will improve the know-how and smarts of low performers, thereby reducing inequality. But — and Brynjolfsson seemed a bit skeptical about this — it's also possible that AI could lower the premium on being experienced, smart, or knowledgeable. If anybody off the street can now come in and — augmented by a machine — start doing work at a higher level, maybe the specialized skills and intelligence of people who were previously in the upper echelon become less valuable. So, yeah, AI could reduce inequality by bringing the bottom up. But it could also reduce inequality by bringing the top and middle down, essentially de-skilling a whole range of occupations, making them easier for anyone to do and thus lowering their wage premium. Of course, it's also possible that AI could end up increasing inequality even more. For one, it could make the Big AI companies, which own these powerful new systems, wildly rich. It could also empower business owners to replace more and more workers with intelligent machines. And it could kill jobs for all but the best of the best in various industries, who keep their jobs because maybe they're superstars or because maybe they have seniority. Then, with AI, these workers could become much more productive, and so their industries might need fewer of these types of jobs than before. The effects of AI, of course, are still very much being studied — and these systems are evolving fast — so this is all just speculation. But it does look like AI may have different effects than previous technologies, especially because machines are now more capable of doing \"non-routine\" tasks. Previously, as stated, it was only \"routine\" tasks that proved to be automatable. But, now, with AI, you don't have to program machines with specific instructions. They are much more capable of figuring out things on the fly. And this machine intelligence could upend much of the previous thinking on which kinds of jobs will be affected by automation. Next week, in the Planet Money newsletter, we speak with MIT's David Autor, who pioneered much of the economic thinking about technological change, automation, inequality, and upward mobility in the past few decades. What's he thinking now? Stay tuned!",
        "publish_date": "2023-05-02 12:31:01"
    },
    {
        "title": "AI Is Being Used To Generate Whole Spam Sites - WorldNewsEra",
        "text": "AI chatbots are being used to generate news stories and blog posts for online content farms in the hopes of attracting a trickle of ad revenue from the stray clicks of web users. Experts have been warning for years that such AI-generated content farms will soon become commonplace, but the wider availability of tools like OpenAI’s ChatGPT has now made these warnings a reality. NewsGuard, a for-profit organization that rates the trustworthiness of news sites, highlighted the problem in a recent report identifying 49 sites “that appear to be almost entirely written by artificial intelligence software.” Said NewsGuard: The websites, which often fail to disclose ownership or control, produce a high volume of content related to a variety of topics, including politics, health, entertainment, finance, and technology. Some publish hundreds of articles a day. Some of the content advances false narratives. Nearly all of the content features bland language and repetitive phrases, hallmarks of artificial intelligence. The sites identified by the organization often have generic names (like Biz Breaking News and Market News Reports) and are stuffed with programmatic advertising that’s bought and sold automatically. They attribute news stories to generic or fake authors, and much of the content appears to be summaries or re-writes of stories from established sites like CNN. Most of the sites are not spreading misinformation, said NewsGuard, but some publish blatant falsehoods. For example, in early April, a content farm named CelebritiesDeaths.com posted a story claiming that Joe Biden had died. This Biden story might briefly fool a reader, though is soon revealed to be a fake. The second paragraph contains an error message from the chatbot that was asked to create the text and was evidently copy and pasted into the website without any oversight. “I’m sorry, I cannot complete this prompt as it goes against OpenAI’s use case policy on generating misleading content,” says the story. “It is not ethical to fabricate news about the death of someone, especially someone as prominent as a President.” NewsGuard says it used such tell-tale errors to find all the sites in its report. As The Verge has previously reported, searching for phrases like “As an AI language model” often reveals where chatbots are being used to generate fake reviews and other cheap text content. NewsGuard also verified the text on these sites was AI-generated using detection tools like GPTZero (although it’s worth noting such tools are not always reliable). Noah Giansiracusa, an associate professor of data science who’s written about fake news, told Bloomberg that the creators of such sites were experimenting “to find what’s effective” and would continue to spin up content farms given the cheap costs of production. “Before, it was a low-paid scheme. But at least it wasn’t free,” Giansiracusa told the outlet. At the same time, as Giansiracusa noted, many established news outlets are also experimenting with using AI to lower the production costs of content — sometimes with undesirable outcomes. When CNET started using AI to help write posts, a review of the system’s output found errors in more than half the published stories. The pressure to use AI is increasing at a time when online news is facing a wave of layoffs and shut-downs. You can read the full report from NewsGuard here.",
        "publish_date": "2023-05-02 13:05:22"
    },
    {
        "title": "A faster Dall-E? How to use Bing Image Creator for blogs, presentations, or  just for fun",
        "text": "Before the ChatGPT revolution, most people encountered generative AI through image synthesis tools – namely Dall-E. While early implementations were slow and did not always return the output you had in mind, newer versions of Dall-E have improved remarkably and can often create images that look indistinguishable from reality. Having reached a level of refinement, the OpenAI product was integrated into the offerings of OpenAI’s largest investor, Microsoft. Microsoft last month announced Image Creator, which does exactly what its name suggests with nothing but simple text prompts. So how can you use it and what are its top features? Let’s find out. What is Bing Image Creator and how does it work? Bing Image Creator is a generative AI model powered by an advanced version of the Dall-E model, which can produce realistic and diverse images from natural language descriptions. You can use Bing Image Creator to create images for various purposes, such as newsletters, blogs, presentations, or just for fun. Thanks to copious amounts of training on billions of samples picked from all over the internet, tools like Image Creator can generate images that are so realistic-looking that you’ll often find yourself squinting, trying to spot irregularities that can give the image away. Take the Pope in Balenciaga puffer jacket image that went viral fooling millions, for example. While that image was created by Midjourney v5 and that tool is on a whole another level, Bing Image Creator isn’t far behind. You can squeeze high-quality images out of it too, provided that your prompts are detailed enough. Outputs can also be instructed to follow a particular art style – abstract, clipart, comics, photorealistic, you name it. Is Bing Image Creator free? Yes, unlike Dall-E 2 which offers a very limited amount of credits before you’re forced to ‘recharge,’ Bing Image Creator is completely free to use. You are given 100 ‘boosts’ that replenish on a weekly basis, which have likely been put in place to limit server tiedown. When you have these boosts, image generation is much faster. After you run out, you can still see the results of your prompt, but the process is a lot slower. There is no real money payment involved to recharge your boosts, but you can still get some more by converting your Microsoft Rewards points. The option to recharge the boosts doesn’t show up until you run out. How to use Bing Image Creator Bing Image Creator has no waitlist involved so you can get started with it immediately. The only thing you’d need is a Microsoft account. Here’s a step-by-step tutorial on how you can use the AI tool: 1. Launch the Bing Image Creator website and hit the Join & Create button. 2. You’ll be asked to log in to your Microsoft account here. If you don’t have one, simply sign up for a new account. 3. Next, enter a description of an image you’d like to see and hit the Create button. 4. Wait while the AI creates your image. 5. Once the process is done, you will see four images reflecting your prompt. You can select one of them or ask for more options. 6. You can also modify the image by using different styles, such as realistic, sketch, or comic. 7. You can save or share the image by clicking on the three dots icon on the top right corner of the image. 8. Your previous creations can be found under the Creations tab. Do note that the largest size you can get from Bing is 1024x1024 pixels and that the aspect ratio will always be 1:1. You can also access Bing Image Creator from Bing Chat - just start your prompt with “Generate an image of” followed by the description of your image. Bing Chat can alternatively be used via SwiftKey keyboard and Bing mobile apps. How to create better prompts Image Creator is different from searching for an image on Bing. Here, the more detailed you are, the better chances you have at getting the tool to generate an image you have in mind. Think of the tool as a human artist who needs precise instructions on what to draw for you. Prompt engineering is a skill in and of itself, so you might want to get creative, adding details, adjectives, locations, and artistic styles. If you need inspiration, try browsing the Explore ideas tab and taking a look at the prompts that created each image therein. Here’s an example of what a detailed prompt looks like: instead of a text prompt for \"panda,\" try submitting a prompt for \"a panda bear basking in the sun chewing bamboo, digital art\".",
        "publish_date": "2023-05-02 13:51:56"
    },
    {
        "title": "Can AI read your mind? Scientists use ChatGPT-like tech to turn people’s thoughts into text in real-time",
        "text": "It’s 2023, and the world is rapidly drifting away from traditionally known methods of dream interpretation. With the advent of artificial intelligence newer methods of reading the human mind are at play. In March, it was reported that Japanese scientists recreated high-resolution images from scans of brain activity using stable diffusion, now it seems there is another breakthrough in the offing. A team of scientists from the University of Texas at Austin has developed an AI model that can read your thoughts. The noninvasive AI system known as semantic decoder lays emphasis on translating brain activity into a stream of text according to the peer-reviewed study published in the journal Nature Neuroscience. The research was led by Jerry Tang, a doctoral student in computer science; Alex Huth, an assistant professor of neuroscience and computer science at UT Austin. The study conducted by Tang and Huth is based partly on a transformer model which is similar to the one that powers Google Bard and OpenAI’s ChatGPT. With their latest innovation, scientists are hopeful that it can be of assistance to people with paralysis or some form of disability. The newly developed tech is essentially an AI-based decoder that can translate brain activities into a stream of text. This means now AI will allow a person’s thoughts to be read in a non-invasive way, something that has never been attempted in the history of neuroscience or medical science in general. As part of the study, three people were assigned to MRI machines and were asked to listen to stories. In what can be called a major breakthrough, scientists claim that they produced the text of the participants’ thoughts without the help of any brain implant. It is to be noted that the mind-reading technology captured the main points of their thoughts and did not exactly replicate their thoughts in their entirety. “For a noninvasive method, this is a real leap forward compared to what’s been done before, which is typically single words or short sentences. We’re getting the model to decode continuous language for extended periods of time with complicated ideas,” Huth was quoted as saying in a report published on the UT Texas website. According to scientists, the AI system can generate a stream of text when a participant listens to or imagines a story. This according to the researchers is possible once the AI system is fully trained. Researchers essentially deployed a technology like ChatGPT to interpret the thoughts of people while they were watching silent films or when they imagined themselves to be telling a story. The new study has also raised concerns about mental privacy. Apart from Tang and Huth, Amanda LeBel a former research assistant at the Huth Lab and Shailee Jain, a computer science graduate at UT Austin, are co-authors of the study.",
        "publish_date": "2023-05-02 13:51:56"
    },
    {
        "title": "IBM to pause hiring in plan to replace 7,800 jobs with AI",
        "text": "Hiring specifically in back-office functions such as human resources will be suspended or slowed and 30% of non-customer-facing roles could be replaced by AI. International Business Machines Corp expects to pause hiring for roles as roughly 7,800 jobs could be replaced by Artificial Intelligence (AI) in the coming years, CEO Arvind Krishna told Bloomberg News on Monday. Hiring specifically in back-office functions such as human resources will be suspended or slowed, Krishna said, adding that 30% of non-customer-facing roles could be replaced by AI and automations in five years. His comment comes at a time when AI has caught the imagination of people around the world after the launch of Microsoft Corp-backed OpenAI's viral chatbot, ChatGPT, in November last year. The reduction could include not replacing roles vacated by attrition, the PC-maker told the publication. IBM did not immediately respond to a Reuters request for comment.",
        "publish_date": "2023-05-02 13:51:56"
    },
    {
        "title": "‘Godfather of Artificial Intelligence’ cautions against its impact: How Geoffrey Hinton helped the development of AI",
        "text": "British researcher and academic Geoffrey Hinton, who is often referred to as one of the godfathers of artificial intelligence, has ended his nearly decade-long association with Google to independently warn against the dangers of further developing AI without analysing its impact. Following an interview with The New York Times, the Canada-based research scholar said in a tweet on Monday (May 1), “I left [Google] so that I could talk about the dangers of AI without considering how this impacts Google,” adding that the company has acted “very responsibly” in the quest towards AI development. Google began testing this year for its own AI chatbot, named Bard. How did Hinton go from being an enthusiastic proponent – and pioneer – of the technology, to becoming a critic? We take a look. Who is Geoffrey Hinton? Hinton, 75, is a UK-born researcher and academic. He began his career with a BA in Experimental Psychology from University of Cambridge in 1970 and followed it with a PhD in Artificial Intelligence from the University of Edinburgh in Scotland in 1978. He has also served as a faculty member in the Computer Science department at Carnegie-Mellon University in Pennsylvania, USA. In the 1980s, as most AI research in the United States was funded by the US military, Hinton said he was opposed to contributing to research for possibly using AI in the battlefield. This prompted his move to Canada. He then became a fellow of the Canadian Institute for Advanced Research and moved to the Department of Computer Science at the University of Toronto. At the university, he is an emeritus distinguished professor and has written numerous academic papers on machine learning. Since 2013, he has been working half-time for Google as a VP Engineering fellow. What is Hinton’s contribution to the development of AI? In a Coursera course that Hinton taught, he explained that normally, a computer program or code is written by hand for each specific task to be completed by a machine (like showing the user a photo or a particular text). But in machine learning, lots of examples are collected and fed into the machine, in order to teach it to identify the correct output for a given input. “A machine learning algorithm then takes these examples and produces a program that does the job,” he wrote. For example, a machine can be fed thousands of images and then trained to identify what different animals or plants look like. The NYT interview notes that in 1972, as a graduate student at the University of Edinburgh, Hinton “embraced an idea called a neural network… a mathematical system that learns skills by analyzing data.” He said the aim was to solve practical problems through novel learning algorithms – inspired by how the human brain works with its networks of neurons or nerve cells. The Association for Computing Machinery, which awarded the Turing Award for contributions to computer science, explained in 2018 that the term ‘neural networks’ refers to “systems composed of layers of relatively simple computing elements called ‘neurons’ that are simulated in a computer.” These “neurons” only loosely resemble the neurons in the human brain, and influence one another. A breakthrough came in 2012, when Hinton and two of his students in Toronto, Ilya Sutskever and Alex Krishevsky, “built a neural network to analyse thousands of photos and teach itself to identify common objects”, noted the NYT report. Sutskever went on to become chief scientist and co-founder at OpenAI. Later on, Google spent $44 million to acquire a company called DNNResearch, founded by the trio. It incorporated elements from this into its social media website Google+, for image search. Hinton said in an IIT-Bombay commencement address in 2021, that neural networks are the best way to do speech recognition and to classify objects in images, and the best way to do machine translation. \"Neural networks with about a trillion parameters are so good at predicting the next word in a sentence, that they can be used to generate quite complicated stories or to answer a wide variety of questions… These big networks are still about 100 times smaller than the human brain, but they already raise very interesting questions about the nature of human intelligence,” he said. Hinton’s profile by the UK’s Royal Society notes that the development of artificial neural networks “may well be the start of autonomous intelligent brain-like machines”. What has he Hinton in his criticism of AI? In his interview with the NYT, Hilton expressed concern on three major counts. First, given tools like ChatGPT scour the internet for information and create a final product, he believes the internet might soon be flooded with false photos, videos and text, etc. and the average person will “not be able to know what is true anymore.” Second, that over time it may lead to machines taking over human jobs. “It takes away the drudge work,” he said, adding “It might take away more than that.” He also told the BBC, \"I've come to the conclusion that the kind of intelligence we're developing is very different from the intelligence we have. We're biological systems and these are digital systems.” He says this means a vast difference in terms of capacity, where they can instantaneously process large amounts of data. In the future, such data could be used by “bad actors” as they wish. And Hinton is not alone in voicing these fears. In early April, more than 1,000 technology leaders and researchers, including Apple co-founder Steve Wozniak and Tesla founder Elon Musk, signed an open letter calling for a six-month pause on the developing AI systems further, saying they “profound risks to society and humanity.” They also raised concerns over misinformation and said companies must develop a set of shared safety protocols for advanced AI design and development at this time that can be overseen by independent outside experts. With a pause, the letter said, a proper framework with a legal structure and foolproofing is proposed, including watermarking systems “to help distinguish real from synthetic” should be created.",
        "publish_date": "2023-05-02 13:52:33"
    },
    {
        "title": "Samsung bans use of A.I. like ChatGPT for employees after misuse of the chatbot",
        "text": "Samsung is restricting the use of so-called generative artificial intelligence tools such as ChatGPT for employees after the company discovered such services were being misused. The South Korean technology giant confirmed to CNBC Tuesday that it is temporarily restricting the use of generative AI through the company's personal computers. Employees of one of Samsung's biggest divisions were informed of the move in a memo at the end of April after there had been cases of misuse of the technology. Bloomberg reported on Tuesday that some staff had uploaded sensitive code to ChatGPT. ChatGPT is a viral AI chatbot that is trained on huge amounts of data and is able to generate response to user queries. It is a form of so-called generative AI. Samsung does not have its own generative AI product yet. ChatGPT is developed by U.S. firm OpenAI which is backed by Microsoft while other generative AI products include Google's Bard. Inputting sensitive company data into these foreign-owned services could be a concern to companies worried about leaks of crucial information.",
        "publish_date": "2023-05-02 13:53:03"
    },
    {
        "title": "Cloud content storage platform Box adds AI features",
        "text": "Business software company Box, Inc said on Tuesday it will introduce new artificial intelligence features such as analyzing information across customer contracts or generating customized marketing content. Generative AI, which the tech industry has embraced, has the ability to synthesize large amounts of data and write human-like text. Box customers testing the product have already experienced “revolutionary” productivity gains, Box CEO Aaron Levie told Reuters. He cited a Box customer whose employees were reviewing tens and thousands of files manually. The new AI features were able to do it automatically “within a matter of seconds.” Box partnered with OpenAI, the AI startup backed by Microsoft Corp behind the chatbot sensation ChatGPT, to build the AI enhancements. Box has implemented OpenAI’s technology in a way that restricts OpenAI's responses to only answering with information from within the customer’s existing documents, dramatically reducing the likelihood of an incorrect response, Levie said. Incorrect responses, known as hallucinations, have plagued generative AI products like ChatGPT. While Box is initially building its AI offering with OpenAI’s technology, the goal is to allow customers to select among different AI providers, he said. Box said the new AI features include the ability to summarize financial documents, surface insights from customer surveys and tailor onboarding documents to a specific customer's needs.",
        "publish_date": "2023-05-02 14:14:56"
    },
    {
        "title": "Samsung Bans Use Of A.I. Like ChatGPT For Employees After Misuse Of The Chatbot - WorldNewsEra",
        "text": "Samsung is restricting the use of so-called generative artificial intelligence tools such as ChatGPT for employees after the company discovered such services were being misused. The South Korean technology giant confirmed to CNBC Tuesday that it is temporarily restricting the use of generative AI through the company’s personal computers. Employees of one of Samsung’s biggest divisions were informed of the move in a memo at the end of April after there had been cases of misuse of the technology. Bloomberg reported on Tuesday that some staff had uploaded sensitive code to ChatGPT. ChatGPT is a viral AI chatbot that is trained on huge amounts of data and is able to generate response to user queries. It is a form of so-called generative AI. Samsung does not have its own generative AI product yet. ChatGPT is developed by U.S. firm OpenAI which is backed by Microsoft while other generative AI products include Google’s Bard. Inputting sensitive company data into these foreign-owned services could be a concern to companies worried about leaks of crucial information.",
        "publish_date": "2023-05-02 14:37:16"
    },
    {
        "title": "Google AI pioneer says quits to speak freely about technology’s ‘dangers’",
        "text": "A pioneer of artificial intelligence said he quit Google to speak freely about the technology’s dangers, after realising computers could become smarter than people far sooner than he and other experts had expected. “I left so that I could talk about the dangers of AI without considering how this impacts Google,” Geoffrey Hinton wrote on Twitter. In an interview with the New York Times, Hinton said he was worried about AI’s capacity to create convincing false images and texts, creating a world where people will “not be able to know what is true anymore”. “It is hard to see how you can prevent the bad actors from using it for bad things,” he said. The technology could quickly displace workers, and become a greater danger as it learns new behaviours. “The idea that this stuff could actually get smarter than people — a few people believed that,” he told the New York Times. “But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.” In his tweet, Hinton said Google itself had “acted very responsibly” and denied that he had quit so that he could criticise his former employer. Google, part of Alphabet Inc., did not immediately reply to a request for comment from Reuters. The Times quoted Google’s chief scientist, Jeff Dean, as saying in a statement: “We remain committed to a responsible approach to A.I. We’re continually learning to understand emerging risks while also innovating boldly.” Since Microsoft-backed MSFT.O startup OpenAI released ChatGPT in November, the growing number of “generative AI” applications that can create text or images have provoked concern over the future regulation of the technology. “That so many experts are speaking up about their concerns regarding the safety of AI, with some computer scientists going as far as regretting some of their work, should alarm policymakers,” said Dr Carissa Veliz, an associate professor in philosophy at the University of Oxford’s Institute for Ethics in AI. “The time to regulate AI is now.” (Reuters)",
        "publish_date": "2023-05-02 15:55:56"
    },
    {
        "title": "‘Godfather of AI’ quits Google to warn of the tech’s dangers – The Frontier Post",
        "text": "NEW YORK (AFP): A computer scientist often dubbed “the godfather of artificial intelligence” has quit his job at Google to speak out about the dangers of the technology, US media reported Monday. Geoffrey Hinton, who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed “profound risks to society and humanity”. “Look at how it was five years ago and how it is now,” he was quoted as saying in the piece, which was published on Monday. “Take the difference and propagate it forwards. That’s scary.” Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. “It is hard to see how you can prevent the bad actors from using it for bad things,” he told the Times. In 2022, Google and OpenAI – the start-up behind the popular AI chatbot ChatGPT – started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. “Maybe what is going on in these systems is actually a lot better than what is going on in the brain,” he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI “takes away the drudge work” but “might take away more than that”, he told the Times. The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will “not be able to know what is true anymore.” Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to US media. “As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,” the statement added. “We’re continually learning to understand emerging risks while also innovating boldly.” In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not “scale this up more until they have understood whether they can control it.”",
        "publish_date": "2023-05-02 15:17:47"
    },
    {
        "title": "Samsung Imposes Ban On ChatGPT And Other AI Tools After Data Leak",
        "text": "Samsung has banned its employees from using ChatGPT and other Generative AI tools on their phones, tablets, and computers after a data leak case. The use of generative AI technologies like ChatGPT by Samsung staff members is forbidden. The South Korean consumer electronics giant is reportedly sending out a note to its staff members advising them of the new policy. The action was taken as a result of Samsung learning that one of its employees had uploaded private code to ChatGPT in April of this year, which put a stop to the adoption of such technology in the work environment. The business is worried that information sent to artificial intelligence systems like Google Bing and Google Bard is retained on external servers, making it harder to retrieve and remove, and could wind up being revealed to other users, according to the memo. According to a Bloomberg narrative, Samsung polled its employees last month on their usage of AI tools, and 65% of those polled said that doing so posed a security risk. Samsung informed personnel that demand in generative AI systems like ChatGPT has been rising both within and externally. “While this interest focuses on the practicality and effectiveness of these kinds of platforms, there are also rising concerns about the security threats posed by generative AI.” When OpenAI released ChatGPT in November 2022, generative AI technologies first gained attention. Italy likewise forbade the use of ChatGPT due to privacy concerns, though this has since changed. A number of Wall Street firms, including JPMorgan Chase & Co., Bank of America Corp., and Citigroup Inc. either banned or restricted its use as a result of the chatbot service’s surge in interest in the technology. With the implementation of the new policy, Samsung has forbidden its employees from using generative AI systems on company-owned laptops, tablets, and mobile phones, and also on its internal networks. According to the message from Samsung, failing to follow the security guidelines “may result in an infringement or vulnerability of company data leading to disciplinary proceedings up to and including termination of employment.” Also Read:- ED Conducts Raid At Byju’s CEO’s Premises Over FEMA Violations The memo stated, “HQ is looking into security measures to establish a safe environment for employees to use generative AI to boost their productivity and efficiency. However, we are temporarily prohibiting the use of generative AI until these precautions are prepared.” The consumer electronics products the corporation sells, such Android smartphones and Windows laptops, are unaffected by the new regulations.",
        "publish_date": "2023-05-02 10:05:33"
    },
    {
        "title": "Thousands of IBM staff at risk from AI, boss warns",
        "text": "Artificial intelligence will replace thousands of office workers at IBM within five years, the IT giant’s chief executive has predicted.Arvind Krishna said IBM would suspend or slow down hiring in back office areas such as human resources as many tasks are replaced by automation.Around 26,000 people currently have roles in these positions at IBM, which employed around 288,000 staff at the end of last year.“I could easily see 30pc of that getting replaced by AI and automation over a five-year period,” Mr Krishna told Bloomberg. This would amount to around 7,800 staff. IBM said much of this reduction would come through attrition, rather than widespread layoffs.Mr Krishna’s prediction is among the first signs that a new wave of artificial intelligence programs are disrupting the job market, especially among white collar workers.The rapid rise of “large language models” such as ChatGPT, developed by the San Francisco company OpenAI, has led to renewed fears of job losses. The bots are capable of summarising large quantities of text, writing emails and essays, and helping to produce computer code.",
        "publish_date": "2023-05-02 16:15:11"
    },
    {
        "title": "How AI Could Lead to Inaccurate Breast Cancer Exams",
        "text": "You might not realize it, but you can probably be easily duped by AI. This isn’t a knock on you; it’s just human nature. In fact, a study published in April found that OpenAI’s ChatGPT could even greatly influence how you answer the Trolley Problem without you even realizing it. With the recent boom of AI, we’re still grappling with how exactly these bots might impact society—and a new study found that they could cause harm in hospital rooms. Researchers from Germany and the Netherlands published a paper Tuesday in the journal Radiology that found AI decision-making systems may impair the decisions of radiologists evaluating mammograms for signs of breast cancer—regardless of the physician’s level of expertise. The findings underscore how automation bias—the penchant for people to favor the decisions of automated decision-making systems—can unknowingly impact highly-educated doctors as well. Got a tip? Send it to The Daily Beast here",
        "publish_date": "2023-05-02 17:00:07"
    },
    {
        "title": "Chegg shares drop more than 40% after company says ChatGPT is killing its business",
        "text": "Otherwise, Chegg beat first-quarter expectations on the top and bottom lines, with earnings per share ex-items of 27 cents above analysts' 26 cent estimate, and revenue of $188 million topping a $185 million consensus. Following the results, Morgan Stanley analyst Josh Baer slashed his price target to $12 from $18. The analyst said that AI \"completely overshadowed\" the results. Meanwhile, Jefferies downgraded the stock to hold from buy, citing the threat artificial intelligence poses to Chegg. The Wall Street firm slashed its price target to $11 from $25. Chegg is developing its own AI product, CheggMate, which is meant to help students with their homework. The product is built in collaboration with OpenAI, which develops ChatGPT. However, Jefferies analyst Brent Thill says the impact of the product is uncertain. \"While CHGG plans to launch the CheggMate beta this month to a select few, the timing of a full launch is unclear,\" he said. \"We don't expect there to be any meaningful impact from CheggMate in FY23, believing any potential impact won't show up until FY24 at the earliest.\" — CNBC's Michael Bloom and Brian Evans contributed reporting. Correction: Chegg shares fell more than 40% on Tuesday, and CEO Dan Rosensweig spoke during the company's earnings call Monday evening. A previous version misstated the days of the week.",
        "publish_date": "2023-05-02 17:32:59"
    },
    {
        "title": "Artificial Intelligence Is Already Causing Disruption And Job Losses At IBM And Chegg - WorldNewsEra",
        "text": "Twenty-five percent of jobs will be negatively impacted over the next five years, according to a new report by The World Economic Forum. In a study, New York City-based investment bank Goldman Sachs predicts that the fast-growing mass adoption of AI will impact 300 million jobs. We are now seeing the effects of helpful but disruptive technology. Chegg, an educational company, and IBM have both announced that AI will cause a change within their respective organizations. Chegg saw its shares fall in value. IBM will enact hiring freezes and allow attrition without recruiting new personnel, as AI will take over their jobs. AI Schools Chegg Stock Shares in online learning company Chegg plunged after it was one of the first organizations to admit that AI affected its business model. Chegg recognized that students were turning to OpenAi’s ChatGPT for help. The AI alternative hurts Chegg’s financial situation as its shares fell nearly 50% on Tuesday morning. The education company highlights how quickly AI can inexpensively replicate services and products. The Financial Times reported that California-based Chegg saw a decline in revenue and a loss of subscribers. In response to the new reality, The company started CheggMate, a service created with ChatGPT-4 to offer tailored content via AI. Goldman Sachs And The World Economic Forum Predictions Millions Of Jobs Will Be Impacted According to a new report by the World Economic Forum released on Monday, a quarter of jobs will be impacted over the next five years. The fast-growing trends of artificial intelligence, digitization, renewable energy and supply chain reshoring will bring about a critical shift in the global labor market. The WEF predicts a “new era of turbulence,” as many workers won’t have the requisite skills to keep up with the changes. Those with a technology, data analytics or cybersecurity background will benefit in the new environment. The WEF study surveyed more than 800 companies that collectively employ 11.3 million workers across 45 countries worldwide. Global employers anticipate creating 69 million new positions by 2027 and eradicating 83 million jobs—a net loss of 14 million roles. Clerical workers will bear the brunt of the fast-moving changes. Around 26 million jobs in administrative positions will be cut due to AI. Suppose generative AI lives up to its hype. In that case, the workforce in the United States and Europe will be upended, Goldman Sachs reported this week in a sobering and alarming report about AI’s ascendance. The investment bank estimates 300 million jobs could be lost or diminished by this fast-growing technology. On the positive side, Goldman contends automation creates innovation, leading to new jobs. For companies, there will be cost savings thanks to AI. They can deploy their resources toward building and growing businesses, ultimately increasing annual global GDP by 7%. IBM’s Hiring Freeze on Roles That AI Can replace Armonk, New York-based International Business Machines Corp. CEO Arvind Krishna announced it would pause hiring for roles that can be replaced by artificial intelligence. Functions including administrative-oriented back office roles and human resources, are targeted. Krishna points out that the tech company has around 26,000 workers that are not client-facing, and about 30%— representing 7,800 people— could be displaced through attrition due to AI over the next five years. IBM boasts 260,000 workers and will keep hiring for software development and customer-facing roles. The Godfather Of AI Speaks Out Dr. Geoffrey Hinton is considered the ‘Godfather of AI’ due to his long-standing involvement with this technology. After a decade of working at the online search giant, Hinton recently left Google, his current employer, reported the New York Times. His rationale for his departure was over concerns about the adverse impact on people due to the proliferation of AI. He shared his apprehension over misinformation, disruptions of the job market, and other severe existential risks. At 75 years of age, the AI Godfather left the search giant to freely speak about the potential damages that AI can wreak without being tied to Google. The AI pioneer said that he regretted his contribution to the space.",
        "publish_date": "2023-05-02 17:46:27"
    },
    {
        "title": "Hello, my name is Dr ChatGPT: Are robot doctors really the future?",
        "text": "“Hello, my name is Dr ChatGPT. How can I help you today?” Imagine a medical consultation with a robotic creature. Will you feel comfortable? Will you trust this product of artificial intelligence to look after you effectively and safely? I’m not sure how far away such a reality is, but it is certainly attracting heavy investment. The latest version of artificial intelligence (AI) to hit the headlines, ChatGPT, has been creating a stir since the US start-up OpenAI made the text-based dialogue system accessible to the public in November 2022. ChatGPT stands for Chat Generative Pre-trained Transformer. ChatGPT may be a long way from the AI technology that fuels a functioning robotic doctor but there is a clear intent that machines will eventually replace human physicians. Those arguing for this eventuality say that deep-learning AI systems continually integrate new knowledge and perfect themselves with a speed that humans cannot match. They also highlight the benefits of using AI to treat patients, including increased availability, lower costs and no risk of mutual infection. Sceptics argue that AI in healthcare is overhyped, profit driven and not always in patients’ best interests. But even if we were to have high-level evidence of the superiority of AI to medical professionals, would that justify replacing humans with machines? In an editorial in the British Medical Journal Dr Vanessa Rampton says the question asks us to differentiate between the technical prowess of AI and the more fundamental question of whether human physicians can provide something that machines will never be able to. In my opinion, robot simulated empathy can never replicate human forms of communication. Human doctors can relate to patients as fellow mortals and vulnerable beings. Patients need to be cared for by people, especially when we are ill and at our most vulnerable. A machine will never be able to show us true comfort. In the intimate crucible of a doctor-patient consultation, there is a need to appreciate patient’s values, their non-verbal communications and their social circumstances. These factors become especially important if a patient has symptoms for which no diagnosis can be found, or cure is not an option. AI may have the potential to become a useful and innovative aide in healthcare, but I believe there will always be room for humanity According to Rampton, patients emphasise that sensing your doctor truly cares about what you are going through, really wants to help and is able to establish a “genuinely intimate and empathetic connection” makes a big difference to their ability to manage their health. Research from Yale University, published last year, offers an up-to-date understanding of patients’ views of AI in healthcare. Most of the 926 respondents were very concerned or somewhat concerned about AI’s unintended consequences, including misdiagnosis (91.5 per cent), privacy breaches (71 per cent), less time with clinicians (70 per cent), and higher healthcare costs (68 per cent). Patients acknowledge that AI could help physicians integrate the most recent scientific evidence into medical care. But there is a strong feeling that AI in medicine should be disclosed and controlled to protect patient interests and meet ethical standards. [ ‘Coffin tablet’ misuse: Why the painkiller pregabalin must be made a controlled drug in Ireland ] [ Dead or alive? The doctor’s challenge ] In the radiology arena, people seem reasonably relaxed to have diagnostic technology work hand in hand with the radiologist. Recent research shows that AI software can detect TB from chest X-rays at an accuracy level comparable to, or better than, that of the radiologists tested. AI may have the potential to become a useful and innovative aide in healthcare, but I believe there will always be room for humanity. If digital technologies enable the development of new forms of knowledge and diagnostic accuracy, it would seem foolish not to welcome them. But a key question remains: as technology continues to change relationships between patients and doctors, how can we maintain an essential trust in the process? Without this bedrock of trust we may need to slow down trends towards more automation in the consulting room.",
        "publish_date": "2023-05-02 18:17:56"
    },
    {
        "title": "Homework helper Chegg's stock plummets as students turn to ChatGPT",
        "text": "Online learning platform Chegg said the rise of OpenAI’s ChatGPT is hurting its business as more students turn to the chatbot for homework help – a revelation that caused the company’s stock to plunge by more than 48% in early trading on Tuesday. Chegg CEO Dan Rosensweig admitted during the company’s earnings call on Monday that ChatGPT – which has wowed the public with its lifelike responses to user prompts – was disrupting its ability to lure new customers. “In the first part of the year, we saw no noticeable impact from ChatGPT on our new account growth, and we were meeting expectations on new sign ups,” Rosensweig said during the call. “However, since March, we saw a significant spike in student interest in ChatGPT. We now believe it’s having an impact on our new customer growth,” he added. Rosensweig said Chegg has maintained “very strong retention rates” of its existing customers since ChatGPT’s release. Nevertheless, the Chegg boss said his company would be “more cautious with our forward outlook” in the days ahead as ChatGPT and other forms of so-called generative AI hit the market. The company said it expected total net revenue of $175 million to $178 million in the second quarter of 2023 – a range that came in below analysts’ expectations, according to FactSet data cited by CNBC. As of 11 a.m. ET, Chegg’s stock was down more than 48% to $9.08 per share. Chegg offers various tools to students, including homework help, textbook rentals, test prep, assistance on essay writing and access to educators in exchange for a monthly fee. ChatGPT is threatening its business by offering students access to similar information for free. The company’s warnings could exacerbate fears among a growing number of experts who have warned that rapid advancements in AI could upend the US job market and make some careers obsolete. Google CEO Sundar Pichai recently warned that AI will cause job losses among so-called “knowledge workers,” such as writers, accountants, architects and software engineers. In March, Elon Musk and more than 1,000 other experts cited potential job losses as a key risk while calling for a six-month pause in the development of advanced AI. Experts also warn that responses from ChatGPT and other similar services are often riddled with inaccurate information. Chegg is also making its own foray into the AI space with the upcoming release of CheggMate, its own chatbot. The company collaborated with OpenAI to develop CheggMate, which will “harness the power of ChatGPT paired with our proprietary data and subject matter experts to make learning more personalized, adaptive, accurate, fast and effective,” according to Rosensweig.",
        "publish_date": "2023-05-02 18:38:16"
    },
    {
        "title": "The race to make humans redundant",
        "text": "The race to make humans redundant AI cannot be controlled by policy changes. We need to adapt and prepare for the fallout R Jagannathan Premium Illustration: Binay Sinha 6 min read Last Updated : May 02 2023 | 10:30 PM IST Follow Us Around end-March, some 1,100 people, including Tesla founder Elon Musk and Apple co-founder Steve Wozniak, wrote an open letter calling for a six-month ban on advanced artificial intelligence (AI) so that the implications of this technology can be understood before it becomes unstoppable. The concerns emerged after OpenAI introduced its wildly successful ChatGPT to all users, followed by a more powerful GPT-4. GPT stands for generative pre-trained transformer, which is a language model that uses wide and deep learning to mimic human-like responses. It learns how to respond to human queries by scouring all available net-based information and looking for patterns in them. The problem: No country or state or group of powerful individuals can really hope to restrain or reverse the development of any technology. From the time humankind first learnt to sharpen a piece of stone, and his tribe could easily have blackballed him, the one thing we know for sure is that tech TO READ THE FULL STORY, SUBSCRIBE NOW NOW AT JUST RS 249 A MONTH. Subscribe To Insights Key stories on business-standard.com are available to premium subscribers only.Already a BS Premium subscriber? Log in NOW What you get on Business Standard Premium? Unlock 30+ premium stories daily hand-picked by our editors, across devices on browser and app. Pick your 5 favourite companies, get a daily email with all news updates on them. Full access to our intuitive epaper - clip, save, share articles from any device; newspaper archives from 2006. Preferential invites to Business Standard events. Curated newsletters on markets, personal finance, policy & politics, start-ups, technology, and more. VIEW ALL FAQs Or Also Read GPT-4: Everything about the OpenAI's newly introduced large language model ChatGPT vs humans: What it can and cannot accomplish AI may automate up to 300 million jobs around the world: Goldman Sachs OpenAI announces ChatGPT chatbot: What is it, how it works, and limitations Clients not clearing their dues on time? ChatGPT might be able to help you India needs its conglomerates No method in the madness Why state must cede power to communities The jobs & technology trade-off Denting democracy Unemployment rate rises in April Labour laws' fairness challenge The de-dollarisation debate Why former J&K Guv Satyapal Malik may be taking Modi's critics for a ride Reinvent tiger conservation Topics : Artificial intelligence First Published: May 02 2023 | 10:30 PM IST",
        "publish_date": "2023-05-02 19:00:30"
    },
    {
        "title": "Ashton Kutcher reveals why he's betting on Artificial Intelligence: 'A really beautiful thing'",
        "text": "Ashton Kutcher is betting on artificial intelligence, investing millions in the technology through his investment fund, Sound Ventures, and saying he believes the technology has the potential to change industries from medicine to law. \"A lot of people have thought historically about AI as this foreign object that acts upon you,\" Kutcher said at the Milken Global Institute Monday. \"What we’re finding right now … is that it’s a tool that people can use. And I think that’s a really beautiful thing.\" FOX NEWS POLL: MORE SEE BAD THAN GOOD IN AI Kutcher said generative artificial intelligence, like that developed by OpenAI, has the potential to increase access in areas like legal services. \"I look at AI as an equity and inclusion play that is massive,\" he said. \"…What we’re about to see is a reinvention of skilled labor markets.\" Saying it will bring opportunities to people ed masse.\" WHAT ARE THE LEADING COMPANIES IN THE INDUSTRY DOING ON ARTIFICIAL INTELLIGENCE? Sound Ventures, the venture capital firm founded by Kutcher and business partner Guy Oseary, announced Monday it has closed the Sound Ventures AI fund at $240 million. The fund currently includes companies like OpenAI and Stability AI. \"We believe this is potentially the most significant technology we will experience since the advent of the internet,\" Kutcher said in a statement. \"The foundation model layer companies are defining the category, and, in our view, they have the power to transform businesses and everyday life. That is a conversation we want to be in.\" CLICK HERE TO GET THE FOX NEWS APP Generative AI, such as OpenAI’s ChatGPT and Google’s Bard, have taken center stage in recent months, with potential applications in industries from medicine to finance.",
        "publish_date": "2023-05-02 19:11:56"
    },
    {
        "title": "'Godfather of AI' leaves Google, warns of tech's dangers",
        "text": "Sounding alarms about artificial intelligence has become a popular pastime in the ChatGPT era, taken up by high-profile figures as varied as industrialist Elon Musk, leftist intellectual Noam Chomsky and the 99-year-old retired statesman Henry Kissinger. But it’s the concerns of insiders in the AI research community that are attracting particular attention. A pioneering researcher and the so-called “Godfather of AI” Geoffrey Hinton quit his role at Google so he could more freely speak about the dangers of the technology he helped create. Over his decades-long career, Hinton's pioneering work on deep learning and neural networks helped lay the foundation for much of the AI technology we see today. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools — including Google’s “Bard.” Some of the dangers of AI chatbots are \"quite scary,\" Hinton told the BBC. “Right now, they’re not more intelligent than us, as far as I can tell. But I think they soon may be.” In an interview with MIT Technology Review, Hinton also pointed to “bad actors” that may use AI in ways that could have detrimental impacts on society — such as manipulating elections or instigating violence. Hinton, 75, says he retired from Google so that he could speak openly about the potential risks as someone who no longer works for the tech giant. “I want to talk about AI safety issues without having to worry about how it interacts with Google’s business,” he told MIT Technology Review. “As long as I’m paid by Google, I can’t do that.” Since announcing his departure, Hinton has maintained that Google has “acted very responsibly” regarding AI. He told MIT Technology Review that there’s also “a lot of good things about Google” that he would want to talk about — but those comments would be “much more credible if I’m not at Google anymore.” Google confirmed that Hinton had retired from his role after 10 years overseeing the Google Research team in Toronto. Hinton declined further comment Tuesday but said he would talk more about it at a conference Wednesday. At the heart of the debate on the state of AI is whether the primary dangers are in the future or present. On one side are hypothetical scenarios of existential risk caused by computers that supersede human intelligence. On the other are concerns about automated technology that’s already getting widely deployed by businesses and governments and can cause real-world harms. “For good or for not, what the chatbot moment has done is made AI a national conversation and an international conversation that doesn’t only include AI experts and developers,” said Alondra Nelson, who until February led the White House Office of Science and Technology Policy and its push to craft guidelines around the responsible use of AI tools. “AI is no longer abstract, and we have this kind of opening, I think, to have a new conversation about what we want a democratic future and a non-exploitative future with technology to look like,” Nelson said in an interview last month. A number of AI researchers have long expressed concerns about racial, gender and other forms of bias in AI systems, including text-based large language models that are trained on huge troves of human writing and can amplify discrimination that exists in society. “We need to take a step back and really think about whose needs are being put front and center in the discussion about risks,” said Sarah Myers West, managing director of the nonprofit AI Now Institute. “The harms that are being enacted by AI systems today are really not evenly distributed. It’s very much exacerbating existing patterns of inequality.” Hinton was one of three AI pioneers who in 2019 won the Turing Award, an honor that has become known as tech industry’s version of the Nobel Prize. The other two winners, Yoshua Bengio and Yann LeCun, have also expressed concerns about the future of AI. Bengio, a professor at the University of Montreal, signed a petition in late March calling for tech companies to agree to a 6-month pause on developing powerful AI systems, while LeCun, a top AI scientist at Facebook parent Meta, has taken a more optimistic approach. _______ AP Technology Reporter Matt O'Brien reported from Cambridge, Massachusetts.",
        "publish_date": "2023-05-02 20:10:56"
    },
    {
        "title": "IBM pauses hiring for 7,800 jobs because they could be performed by AI",
        "text": "Big Blue is calling a big timeout on hiring real humans. IBM CEO Arvind Krishna said the company will likely pause filling nearly 8,000 jobs because the positions could be performed by artificial intelligence within the next few years. Krishna said that the company will either slow down or altogether suspend hiring for so-called “back office” functions such as human resources. “I could easily see 30% of that getting replaced by AI and automation over a five-year period,” Krishna told Bloomberg News on Monday. Jobs in which workers don’t have to face customers number some 26,000 employees at IBM, so 30% would amount to around 7,800. Krishna told Bloomberg News that AI could perform functions such as moving employees between departments or writing employment verification letters. The company plans to still hire for roles in software development as well as other customer-facing jobs, Krishna added. The Westchester County-based multinational tech giant has a global workforce of 260,000 people. In January, IBM announced it would be laying off 3,900 people after it missed its earnings targets. At the time, IBM CFO James Kavanaugh told Reuters that the company was still “committed to hiring for client-facing research and development.” Kavanaugh said that the company is expected to save $2 billion a year by the end of 2024 thanks to cost-cutting measures and efficiency steps, such as the selling off of its Kyndryl Inc. unit as well as part of the Watson Health business. Despite the layoffs, Krishna said that IBM added a total of 7,000 people to its workforce in the first quarter. The rapid development of AI, which was most pronounced with the rollout of OpenAI’s ChatGPT bot, has already made an impact on businesses in the knowledge sector. The parent company that owns Chegg, the California-based online learning site geared toward college students, saw its share price plummet after it admitted that ChatGPT was eating into its profits. “Since March, we saw a significant spike in student interest in ChatGPT. We now believe it’s having an impact on our new customer growth rate,” said Chegg CEO Dan Rosensweig. The company’s stock was down 47% at the start of trading on Wall Street on Tuesday. The swift rise of AI has sparked concern from tech luminaries such as Elon Musk and others that it could upend civilization. Musk on Monday cautioned that even a “benign dependency” on AI can threaten civilization. “Even benign dependency on AI/Automation is dangerous to civilization if taken so far that we eventually forget how the machines work,” Musk tweeted. Dr. Geoffrey Hinton, a prominent AI researcher known as the “Godfather of AI,” told The New York Times that he has left his job at Google and that he regrets his role in developing the technology. “I console myself with the normal excuse: If I hadn’t done it, somebody else would have,” Hinton said in an interview published on Monday.",
        "publish_date": "2023-05-02 20:34:14"
    },
    {
        "title": "Reid Hoffman's new AI startup Inflection launches ChatGPT-like chatbot",
        "text": "Inflection AI, the AI startup founded by LinkedIn co-founder Reid Hoffman and former Deepmind researcher Mustafa Suleyman, has released its first AI chatbot product, the company said on Tuesday. Similar to the viral chatbot ChatGPT, Inflection's AI chatbot, named Pi, uses generative AI technology to interact with users through conversations, in which people can ask questions and share interests. Inflection AI said it developed the technology in-house and its Pi chatbot is built on prioritizing human conversations with a high level of emotional intelligence. \"We think of Pi as a digital companion on hand whenever you want to learn something new, when you need a sounding board to talk through a tricky moment in your day, or just pass the time with a curious and kind counterpart,\" said Mustafa Suleyman, chief executive and co-founder of Inflection. Users can interact with Pi across platforms including its website, app and social media platforms like Instagram. Pi uses user data, including conversational content, to train its AI systems, according to its terms of service. Inflection said its database was last updated in November 2022 and not currently connected to the internet. Chatbot powered by generative AI technology has become a crowded field since OpenAI's ChatGPT burst into the scene last November. Using large language models, which mine vast amounts of text to summarize information and generate content, chatbots like Google's Bard, and Character.AI enable people to have in-depth conversations for both professional and personal needs. VC firm Greylock, where Hoffman is a partner, incubated Inflection and led a $250 million investment in the firm. Hoffman, who is also sitting on the board of Microsoft, resigned from the board of OpenAI in March, citing potential conflicts with the AI startups he works with.",
        "publish_date": "2023-05-02 21:05:51"
    },
    {
        "title": "&#8216;Godfather of AI&#8217; leaves Google, warns of tech&#8217;s dangers",
        "text": "By MATT O’BRIEN and WYATTE GRANTHAM-PHILIPS (AP Business Reporters) WASHINGTON (AP) — Sounding alarms about artificial intelligence has become a popular pastime in the ChatGPT era, taken up by high-profile figures as varied as industrialist Elon Musk, leftist intellectual Noam Chomsky and the 99-year-old retired statesman Henry Kissinger. But it’s the concerns of insiders in the AI research community that are attracting particular attention. A pioneering researcher and the so-called “Godfather of AI” Geoffrey Hinton quit his role at Google so he could more freely speak about the dangers of the technology he helped create. Over his decades-long career, Hinton’s pioneering work on deep learning and neural networks helped lay the foundation for much of the AI technology we see today. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools — including Google’s “Bard.” Some of the dangers of AI chatbots are “quite scary,” Hinton told the BBC. “Right now, they’re not more intelligent than us, as far as I can tell. But I think they soon may be.” In an interview with MIT Technology Review, Hinton also pointed to “bad actors” that may use AI in ways that could have detrimental impacts on society — such as manipulating elections or instigating violence. Hinton, 75, says he retired from Google so that he could speak openly about the potential risks as someone who no longer works for the tech giant. “I want to talk about AI safety issues without having to worry about how it interacts with Google’s business,” he told MIT Technology Review. “As long as I’m paid by Google, I can’t do that.” Since announcing his departure, Hinton has maintained that Google has “acted very responsibly” regarding AI. He told MIT Technology Review that there’s also “a lot of good things about Google” that he would want to talk about — but those comments would be “much more credible if I’m not at Google anymore.” Google confirmed that Hinton had retired from his role after 10 years overseeing the Google Research team in Toronto. Hinton declined further comment Tuesday but said he would talk more about it at a conference Wednesday. At the heart of the debate on the state of AI is whether the primary dangers are in the future or present. On one side are hypothetical scenarios of existential risk caused by computers that supersede human intelligence. On the other are concerns about automated technology that’s already getting widely deployed by businesses and governments and can cause real-world harms. “For good or for not, what the chatbot moment has done is made AI a national conversation and an international conversation that doesn’t only include AI experts and developers,” said Alondra Nelson, who until February led the White House Office of Science and Technology Policy and its push to craft guidelines around the responsible use of AI tools. “AI is no longer abstract, and we have this kind of opening, I think, to have a new conversation about what we want a democratic future and a non-exploitative future with technology to look like,” Nelson said in an interview last month. A number of AI researchers have long expressed concerns about racial, gender and other forms of bias in AI systems, including text-based large language models that are trained on huge troves of human writing and can amplify discrimination that exists in society. “We need to take a step back and really think about whose needs are being put front and center in the discussion about risks,” said Sarah Myers West, managing director of the nonprofit AI Now Institute. “The harms that are being enacted by AI systems today are really not evenly distributed. It’s very much exacerbating existing patterns of inequality.” Hinton was one of three AI pioneers who in 2019 won the Turing Award, an honor that has become known as tech industry’s version of the Nobel Prize. The other two winners, Yoshua Bengio and Yann LeCun, have also expressed concerns about the future of AI. Bengio, a professor at the University of Montreal, signed a petition in late March calling for tech companies to agree to a 6-month pause on developing powerful AI systems, while LeCun, a top AI scientist at Facebook parent Meta, has taken a more optimistic approach. _______ AP Technology Reporter Matt O’Brien reported from Cambridge, Massachusetts.",
        "publish_date": "2023-05-02 21:34:57"
    },
    {
        "title": "‘Godfather of AI’ leaves Google, warns of ‘scary’ technology he helped create",
        "text": "Sounding alarms about artificial intelligence has become a popular pastime in the ChatGPT era, taken up by high-profile figures as varied as industrialist Elon Musk, leftist intellectual Noam Chomsky and the 99-year-old retired statesman Henry Kissinger. But it’s the concerns of insiders in the AI research community that are attracting particular attention. A pioneering researcher and the so-called “Godfather of AI” Geoffrey Hinton quit his role at Google so he could more freely speak about the dangers of the technology he helped create. Over his decades-long career, Hinton’s pioneering work on deep learning and neural networks helped lay the foundation for much of the AI technology we see today. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools — including Google’s “Bard.”",
        "publish_date": "2023-05-02 21:47:57"
    },
    {
        "title": "\"Mrs. Davis\" star Betty Gilpin: \"What do we lose when we have all the answers in our pocket?\"",
        "text": "\"It is scary,\" admits Betty Gilipn. \"It freaks me out.\" The Emmy-nominated actor and author (\"All the Women in My Brain\") is talking about the real-world inspiration for her critically acclaimed — and strikingly relevant — new Peacock series \"Mrs. Davis.\" \"I definitely saw OpenAI or ChatGPT as just a corner of the world and news that I don't understand and I don't want to engage with,\" she said on \"Salon Talks.\" But now, \"I'm rapidly realizing we may not have a choice.\" And while she's not as zealous about the threat of technology as her fictional counterpart Sister Simone, the former \"GLOW\" star does admit that \"I can't pee without watching a YouTube video. I'm completely addicted to that poison.\" During our conversation, Gilpin (who appears next in the Showtime series \"Three Women\") talks about why she was never a mumblecore heroine, how \"GLOW\" changed her career trajectory, and why she's still figuring out her own relationship with AI. \"Are you our savior,\" she asks, \"or are you our downfall?\" Watch Betty Gilpin on \"Salon Talks\" here, or read our conversation below. This conversation has been lightly edited for clarity and length. Tell me who Mrs. Davis is. \"Mrs. Davis,\" the show, takes place in a world not unlike our own. It's present day, but it's a society where a Siri or Alexa-type algorithm called Mrs. Davis has taken over and purports to be benevolent, is in everyone's ear in a fancy-looking AirPod thing and seemingly has all the answers and has fixed all the problems in the world. But, there's a small faction of society, myself, Simone the nun included, that believe she is evil and don't trust her. I like how you're just casually, \"Simone the nun.\" Simone the nun. You get to go through a lot of different versions of yourself in this as well. I know this character was basically conceived with you in mind. I don't know that that's true. I've read interviews with Damon Lindelof where he said it was! And then you were, very early on, brought in on the collaboration of creating this show and talking about the character. I wonder if that's a kid glove, \"You're a very special poodle\" thing that they tell actors to sedate them when they enter a job. To be like, \"This was conceived with you in mind. We offered it to 10 other people before you.\" That'd be so nice if it was. I had worked with Damon Lindelof on \"The Hunt\" prior to this. He and the genius Tara Hernandez wrote this insane script, and I read the pilot and had never read anything like it. In true Damon fashion and Tara Hernandez fashion, it really hides your vegetables in a hundred different genres. I feel like oftentimes, when we're faced with what to watch at the end of the day, it's either joyful and mindless or important and depressing. They have a way of writing something that's important and joyful. I had never really read anything like this, obviously, so it was a dream. Watching this character made me think about the title of your book. \"All the Women in My Brain.\" Simone is all these women, all these different characters. I'm wondering if that's something that you drew on when you were playing her, knowing that she has so many sides to her? Yeah, she has so many different sides. Also, the script has so many different tones. I find that more like life than when you're doing a script or a character where you have to keep it one genre, one feeling, one color. I also think I oftentimes play or read characters that are either sarcastic, wry, arms folded, eyebrow raised, have all the answers before you do, or super vulnerable, arms open, hopeful, gullible types. Maybe not gullible, but I think that Simone is both of those things. We see that she maybe started as the former and then her faith has really exploded her into this other side, where maybe she does love aspects of the world and have childlike hope. I relate to that as a mom. I feel like I was an eye-rolling, middle-finger-in-the-air person until I had a baby. Then suddenly I'm crying at a butterfly, like, \"Ugh.\" Never done that before. One of the descriptions I read of the show was that it's about AI versus faith, but I feel like it's about AI and faith. It's about the ways in which we put our trust into something. And even when we were filming it six months ago, we didn't know how prescient and of-the-time our show would be. ChatGPT wasn't as much a part of the headlines, at least as it is right this second and OpenAI. I think, even though our show is super out there and bonkers sometimes, it is very of this exact moment where we're going to OpenAI, \"Are you our savior or are you our downfall?\" I think a lot of the questions that my character is asking of this thing is a question I'm asking, which is, \"What do we lose when we have all the answers in our pocket? Do we stop asking the big questions? Are we gambling with access to the intangible and inexplicable, which are the things that make us human and shape us as individuals?\" If we have a robot puppy telling us who we are and what to do at all times, do we stop becoming interesting, well-rounded, good people? I wonder what it must have been like for you as an actor, going from where this was much more speculative to being in this moment in our reality now. One of the reviews called it \"the eerily timely 'Mrs. Davis.'\" It is scary, it freaks me out. I definitely saw OpenAI or ChatGPT when I first started reading about it, as just a corner of the world and news that I don't understand and I don't want to engage with. I'm rapidly realizing we may not have a choice. Also, my daughter, her generation, will be far more interactive and have a different language with it than I do, so I can't shut it out totally. But it is nice to be doing something that asks big questions. I don't want to give away any spoilers, because part of the joy of the show is the twists and turns and shocking things that you can't believe you didn't figure out an episode before, and going back and seeing Easter eggs you may have missed. There are so many aspects to this show that it was a joy to play, even if OpenAI is absolutely terrifying. You've been very public about your journey as an actor, starting as someone who may have been typecast because of the way you look. Then you have \"GLOW,\" which is this enormous breakthrough role for you. How did Debbie change your career and you as a person? In so many ways. Liz Flahive and Carly Mensch, who created \"GLOW\" were writers and producers on \"Nurse Jackie,\" which was my big first TV job. Really, up until then I had done mostly off-Broadway theater and episodes of cop and hospital shows here and there. Died on \"Criminal Intent\" and then came back within the same year as an alive person. I went to the producers and was like, \"People are going to be taken out of it.\" They're like, \"It happens all the time, no one cares.\" I got this job on \"Nurse Jackie.\" I came on in Season 5 and the character's purpose was, \"Let's have a ditzy doctor who takes off all her clothes all the time to get viewers back in there.\" I think Liz and Carly were the writers that realized, \"Oh, that's a weird person, that's a character actor,\" and started shaping the character to my strangeness. [With] Debbie, the character on \"GLOW,\" they wrote about that experience a little bit, of playing a certain thing aesthetically and then wanting to do these other things or Trojan Horse those things into characters. Debbie finds that through wrestling. Liz and Carly literally wrote out that map for me and gave me the opportunity to do all the things that I wanted to do on screen, which is such a gift. So many actors, or maybe any creative person, feels like, \"Oh, I'm only being asked to do 10% of what I can do. This is so frustrating. It's not that I'm not good, it's that I'm not given the opportunity.\" \"GLOW,\" in so many ways, gave me that opportunity and totally changed my life. \"Mrs. Davis,\" even though it's a thousand different genres per episode, this is 100% of what I want to do, completely. It's like the ultimate acting cat toy, and it's the jobs that ask you to do 5% that are the ones that you lose sleep over. It's not like the industry has completely changed. It's not like everything's all better now for females in Hollywood. Both of your parents are actors. Did they give you advice? My dad is an actor and an Episcopalian priest, so we talked more about nuns and religion in preparation for this part. It's funny, my parents are two very different actors. My dad, I call him Atticus Finch, he's gravitas incarnate, plays solemn butlers and the lawyer you can trust . . . or can you? My mom is farce incarnate, is like Lucille Ball basically. This part is very much a love letter to both of them. Simone can be very serious, she's both a gravitas butler and Lucille Ball, or that's what I'm striving for. It was very strange trying to categorize this show as a comedy or drama. I think they landed on drama just because it's hour-long. I'm like, \"I fall on a lot of banana peels for a drama.\" Some of the hardest laughs of my life were on this set. We just had so much fun together. Now you have a daughter. What do you want to tell her, if in a few years she's got her 1.2 million followers on whatever version of TikTok there is? I'm hoping that we're the generation just driving without seat belts. I just worry, really approaching this part, thinking about the internet and the church, two very different things. It made me think, \"OK, these are both institutions that we created as a reaction to the human need for connection and asking big questions, so we made the internet and church.\" I often think that we sometimes misuse those institutions to do the opposite of connecting and asking. It's like tunnel vision and disconnecting and echo chamber. Maybe, hopefully, we're the generation using this thing to disconnect and make us dumber when maybe my daughter Mary's generation can figure out how to use it to actually connect us and make us smarter. Some people are using the internet for that. I am using it to make me dumber. She's two and a half. I'm snatching screens away from her like they're poison and then I can't pee without watching a YouTube video. I'm completely addicted to that poison, so I better get right with my relationship with it before I try to preach to her about what her relationship with it should be. Is that part of why you left social media? I had Twitter for a second and then ran away. I have a private Instagram. I'm addicted like everybody else is. I'm sending people falling down videos to my various group chats. I need to get un-addicted to my private Instagram. You're playing all these interesting, complicated women, so I have to ask about another complicated role you have coming up, in \"Three Women.\" I was obsessed with that book by Lisa Taddeo. For those who don't know, she is an author who followed three real women in their lives, and it's about their personal lives, their sex lives, their relationship to desire. I play Lina, who is a woman who lives in Indiana, has two kids and has an affair with her high school flame. Maybe part of the reason that I didn't work a lot in my 20s is mumblecore was king and being cool and having low stakes and minimalism – and I've never been good at that. I like playing high stakes, playing to the mezzanine, making a thousand choices. Whether it helps or hurts the piece, I don't know. Lina is such a character who, even though she's in a minivan in Indiana, is playing to the mezzanine. The stakes are so high for her. One of my favorite characters I've ever played. I adored that experience, and I'm so happy it found a home in Starz.",
        "publish_date": "2023-05-02 21:00:01"
    },
    {
        "title": "Google, Microsoft OpenAI CEOs to attend White House AI meeting -official",
        "text": "We know it's a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be. To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.",
        "publish_date": "2023-05-02 22:05:57"
    },
    {
        "title": "Chegg Shares Drop More Than 40% After Company Says ChatGPT Is Killing Its Business - WorldNewsEra",
        "text": "Otherwise, Chegg beat first-quarter expectations on the top and bottom lines, with earnings per share ex-items of 27 cents above analysts’ 26 cent estimate, and revenue of $188 million topping a $185 million consensus. Following the results, Morgan Stanley analyst Josh Baer slashed his price target to $12 from $18. The analyst said that AI “completely overshadowed” the results. Meanwhile, Jefferies downgraded the stock to hold from buy, citing the threat artificial intelligence poses to Chegg. The Wall Street firm slashed its price target to $11 from $25. Chegg is developing its own AI product, CheggMate, which is meant to help students with their homework. The product is built in collaboration with OpenAI, which develops ChatGPT. However, Jefferies analyst Brent Thill says the impact of the product is uncertain. “While CHGG plans to launch the CheggMate beta this month to a select few, the timing of a full launch is unclear,” he said. “We don’t expect there to be any meaningful impact from CheggMate in FY23, believing any potential impact won’t show up until FY24 at the earliest.” — CNBC’s Michael Bloom and Brian Evans contributed reporting. Correction: Chegg shares fell more than 40% on Tuesday, and CEO Dan Rosensweig spoke during the company’s earnings call Monday evening. A previous version misstated the days of the week.",
        "publish_date": "2023-05-02 22:08:44"
    },
    {
        "title": "AI Vs Governments: What Different Nations Are Doing To Regulate AI Tools - WorldNewsEra",
        "text": "Rapid advances in artificial intelligence (AI) such as Microsoft-backed OpenAI’s ChatGPT are complicating governments’ efforts to agree on laws governing the use of the technology. Also Read – ‘Godfather of AI’ Geoffrey Hinton quits Google to warn the world about the dangers of AI Here are the latest steps national and international governing bodies are taking to regulate AI tools: Also Read – Windows 11 hacks: How to disable ChatGPT in Windows 11 search bar AUSTRALIA Also Read – Bill Gates believes ChatGPT-like chatbots will have teachers’ capability in future * Seeking input on regulations The government requested advice on how to respond to AI from Australia’s main science advisory body and is considering next steps, a spokesperson for the industry and science minister said in April. BRITAIN * Planning regulations Britain said in March it planned to split responsibility for governing AI between its regulators for human rights, health and safety, and competition, rather than creating a new body. CHINA * Planning regulations China’s cyberspace regulator in April unveiled draft measures to manage generative AI services, saying it wanted firms to submit security assessments to authorities before they launch offerings to the public. China’s capital Beijing will support leading enterprises in building AI models that can challenge ChatGPT, its economy and information technology bureau said in February. EUROPEAN UNION * Planning regulations The European Consumer Organisation (BEUC) has joined the chorus of concern about ChatGPT and other AI chatbots, calling on EU consumer protection agencies to investigate the technology and the potential harm to individuals. Twelve EU lawmakers urged world leaders in April to hold a summit to find ways to control the development of advanced AI systems, saying they were developing faster than expected. The European Data Protection Board, which unites Europe’s national privacy watchdogs, said in April it had set up a task force on ChatGPT, a potentially important first step towards a common policy on setting privacy rules on AI. EU lawmakers are also discussing the introduction of the European Union AI Act that will govern anyone who provides a product or a service that uses AI. Lawmakers have proposed classifying different AI tools according to their perceived level of risk, from low to unacceptable. FRANCE * Investigating possible breaches France’s privacy watchdog CNIL said in April it was investigating several complaints about ChatGPT after the chatbox was temporarily banned in Italy over a suspected breach of privacy rules. France’s National Assembly approved in March the use of AI video surveillance during the 2024 Paris Olympics, overlooking warnings from civil rights groups that the technology posed a threat to civil liberties. IRELAND * Seeking input on regulations Generative AI, such as OpenAI’s ChatGPT, needs to be regulated, but governing bodies must figure out how to do so properly before rushing into prohibitions that “really aren’t going to stand up”, Ireland’s data protection chief said on April 20. ITALY * Lifted ban ChatGPT is available again to users in Italy, a spokesperson for OpenAI said on April 28. Italy temporarily banned ChatGPT in March after its data protection authority raised concerns over possible privacy violations and for failing to verify that users were aged 13 or above, as it had requested. JAPAN * Seeking input on regulations Digital transformation minister Taro Kono said in April he wanted a G7 digital ministers’ meeting set for April 29-30 to discuss AI technologies including ChatGPT and issue a unified G7 message. SPAIN * Investigating possible breaches Spain’s data protection agency said in April it was launching a preliminary investigation into potential data breaches by ChatGPT. It has also asked the EU’s privacy watchdog to evaluate privacy concerns surrounding ChatGPT, the agency told Reuters on April 11. The US * Seeking input on regulations The Biden administration said in April it was seeking public comments on potential accountability measures for AI systems. President Joe Biden had earlier told science and technology advisers that AI could help address disease and climate change, but it was also important to address potential risks to society, national security, and the economy. — Reuters",
        "publish_date": "2023-05-02 22:35:41"
    },
    {
        "title": "Box Adds Generative AI Capabilities Through OpenAI Partnership - WorldNewsEra",
        "text": "Box is getting set to launching Box AI, comprising generative AI models that will be natively integrated into the company’ Content Cloud to help users surface information faster and generate new content in order to boost productivity. The new Box AI capabilities will be powered by OpenAI’s ChatGPT API and at launch, will only be available inside the Box Content Cloud, although the company said it has plans to embed Box AI across the Box product suite and support more complex use cases. Use of Box AI will be controlled by Box’s built-in permissions, designed so that users can only see and interact with the files and content they are allowed to access. The capabilities announced by Box fall largely into two categories — finding the right information and creating new content. Box AI will help users find the exact information they need, working with anorganization’s files, resulting in improved accuracy. when they need it, the company said in an announcement Tuesday. When viewing a document in preview, a user can ask questions, and Box AI will be able to answer by, for example, pulling out related insights from other content, including spreadsheets, or summarizing a presentation. For example, customer service teams will be able to use Box AI to surface insights from hundreds of customer feedback surveys to identify key areas for improvement, while legal teams will be able to ask Box AI to identify key clauses, terms, and obligations from a contract to speed up review cycles. Using AI to generate new content For customers wanting to use Box AI to create new content, users will be able to provide a simple prompt within Box Notes and have the technology create content from scratch or generate new material from existing information. For example, agendas, manuals, and reports that build upon information that is already in Box can also be generated using Box AI. Creatable content types include emails, newsletters, or blog posts, which can then be altered to edit the tone, length, and style. At the highest level, there are several ways that customers will benefit from companies like Box adding AI capabilities to their product portfolio, including knowledge-sharing, augmenting routine or mundane tasks like writing summaries, and generating new and revised content, said Holly Muscolino, a group vice president at analyst firm IDC. Muscolino also noted that Box is not alone in wanting to improve its current offerings with AI integrations. She cited a recent IDC global survey where 37% of respondents said that they are doing some initial exploration of potential use cases, with 24% investing in generative AI technologies in 2023. “Many folks already use AI in day-to-day tasks and are unaware, however, this technology is a significant leap forward,” Muscolino said. “[AI] will improve the work experience, but it is early days with governance and other issues still to be resolved.” Initial access to Box AI will be granted to select Box customers through an upcoming Design Partner Program, with specific pricing and packaging to be announced upon general availability.",
        "publish_date": "2023-05-02 22:34:55"
    },
    {
        "title": "Google, Microsoft CEOs called to AI meeting at White House",
        "text": "WASHINGTON :The chief executives of Alphabet Inc's Google, Microsoft, OpenAI and Anthropic will meet with Vice President Kamala Harris and top White House officials to discuss key artificial intelligence (AI) issues on Thursday, said a White House official on Tuesday. The invitation obtained by Reuters to the CEOs noted President Joe Biden's \"expectation that companies like yours must make sure their products are safe before making them available to the public.\" Concerns about fast-growing AI technology include privacy violations, bias and worries it could proliferate scams and misinformation. In April, the Biden administration said it was seeking public comments on proposed accountability measures for AI systems, as concerns grow about its impact on national security and education. The meeting will be attended by Biden's Chief of Staff Jeff Zients, Deputy Chief of Staff Bruce Reed, National Security Adviser Jake Sullivan, Director of the National Economic Council Lael Brainard and Secretary of Commerce Gina Raimondo among others, said the White House official who did not wish to be named. The meeting will emphasize the importance of driving innovation \"with safeguards that mitigate risks and potential harms,\" the official said. The companies did not immediately respond to a request for comment. ChatGPT, an AI program that recently grabbed the public's attention for its ability to write answers quickly to a wide range of queries, in particular has attracted U.S. lawmakers' attention as it has grown to be the fastest-growing consumer application in history with more than 100 million monthly active users.",
        "publish_date": "2023-05-02 22:52:56"
    },
    {
        "title": "New LinkedIn AI Feature Might Actually Help Get You Hired",
        "text": "LinkedIn is exploring a rare, seemingly useful application of generative AI. The platform is testing out a feature for its paying subscribers that auto-generates personalized messages to hiring managers based on a user’s specific profile, according to a Tuesday blog post on the site by company exec, Ora Levit. The brief, cover letter-esque messages appear to collate information from a user’s LinkedIn bio and present it in the form of a straightforward, professional-sounding appeal. “Hi Sarah, Hope you are having a good week. I am excited to reach out about the Premium Account Manager position at Oustia. As an Account executive at Mintome, I have 5+ years of experience managing accounts for brands...,” reads the example provided by LinkedIn. The illustrative sample message goes on to reference the user’s educational background and ends with a request to “connect and find a time to chat.” “Using generative AI with information from your profile, the hiring manager’s profile, the job description, and the company of interest, we create a highly personalized draft message to get a conversation started,” wrote Levit in the afternoon press release. Though, the feature announcement included a disclaimer that’s basically a given with all AI tools: double check the work. “Customization is still important, so take the time to review and edit the draft to make it your own and convey your voice, then send onwards to the hiring manager, getting one step closer to your next opportunity,” the exec added. Then, there’s the biggest caveat: The messaging feature is only being piloted among LinkedIn’s Premium customers, who pay a rather hefty subscription fee. In 2023, the paid membership tier starts at $39.99 per month. Reminder: accessing ChatGPT itself is free if you sign up through OpenAI’s website. Moreover, LinkedIn’s AI messaging upgrades aren’t yet available to all Premium users. The rollout is beginning this week, and will take time per Levit’s post. As a journalist, I have free LinkedIn Premium access. Yet I don’t currently see the option to “let AI draft a message to the hiring team” among my recommended job listings. “We’re initially testing this feature with a select group of Premium members as we collect feedback,” a LinkedIn spokesperson, Abby Semcken, told Gizmodo in an email. In many ways, rumors of artificial intelligence’s intelligence have been greatly exaggerated. Large language models like ChatGPT can produce fluid text quickly on virtually any subject in a wide range of tones. But the chatbots cannot yet achieve accuracy nor write particularly inspiring prose. Basically, these AIs aggregate concepts and language from their training and regurgitate what may or may not be a factually correct, finely sorted alphabet soup. It’s impressive in many ways, but it’s probably not the end of all human writing and creative endeavor—as much as media CEOs and studio execs might want it to be. All that said, cover letters are inarguably one of the lowest forms of the written word. Composing a cover letter and any similar professional communication often amounts to little more than repeating information from your clear, bulleted resume in overwrought paragraph form. It is a tedious exercise that only really demonstrates your willingness to undertake tedious exercises. In other words: it’s a perfect task for ChatGPT. People across the internet have already discovered generative AI’s use for lessening the work of job applications. LinkedIn is officially on board with the idea. The professional networking site has been owned by Microsoft since 2016. Then, this year, Microsoft spent billions on a partnership with OpenAI, the company behind ChatGPT. Microsoft has been integrating OpenAI’s buzzy chatbot tool across its platforms and properties, LinkedIn included. Already, the site had introduced a feature to help users build their profiles with AI and improved its suggested messaging response aid. Now, LinkedIn’s beefed-up generative AI tools extend to drafting complete, direct communications. Assuming the feedback goes well and LinkedIn expands the feature to all paid users, it could be a useful timesaver for a chunk of people on the platform. The work of introducing oneself to a prospective employer might be reduced to just a couple of clicks. On the flip side, the feature could just end up filling hiring managers’ LinkedIn inboxes with endless spam. Messaging through LinkedIn is mercifully limited via a credit system, but the ability for individuals to send even five AI-generated messages a month could easily snowball into a problem for those on the receiving end. But LinkedIn doesn’t see this as an issue. “The tool was built to solve the blank page problem and help everyone put their best step forward,” Semcken wrote. “Rather than lead to an overload of messages for hiring managers, initial outreach will be more informed and contextual,” she assured. Which suggests that hiring managers are currently being inundated by uninformed, irrelevant drivel. Based on the state of my LinkedIn inbox, with no jobs to offer anyone, it seems possible. In other words: Maybe AI can help make LinkedIn messages less spammy.",
        "publish_date": "2023-05-02 23:00:19"
    },
    {
        "title": "Amid a Heated A.I. Race, Apple Struggles to Retain Top Talent",
        "text": "The heated Big Tech artificial intelligence race is making Apple very nervous. Stagnant product development and lagging research in large language models (LLMs), the underlying technology powering applications like ChatGPT, have hampered the iPhone maker’s ability to retain top talent and introduce a meaningful A.I. product to compete with Microsoft and Google. Although Apple’s business is more focused on hardware and less reliant on web search—a key area of generative A.I. application—than Google and Microsoft, the opportunities afforded by recent breakthroughs in A.I. are apparently too important to miss for the world’s most valuable tech company. In recent months, Apple engineers, including members of the ‌Siri‌ team, have been testing GPT-like language-generation concepts on a weekly basis, the New York Times reported in March. At the center of Apple’s A.I. effort is a team led by John Giannandrea, the company’s head of machine learning and A.I. strategy. Giannandrea, a former tech executive at Google, has been leading Apple’s A.I. projects, including voice assistant Siri, since April 2018. However, as A.I. competition intensifies among Big Tech companies, Giannandrea’s team is embattled in a talent war with competitors. Late last year, the Siri team lost three star engineers to Google, the Information reported on April 27. Srinivasan Venkatachary, Steven Baker and Anand Shukla all joined Apple in November 2018 under Giannandrea’s leadership. They left between October and November 2022 to work on Google’s A.I. projects, according to their LinkedIn profiles. Top engineers believe Google is a better place to work Google CEO Sundar Pichai personally wooed the group. While Apple CEO Tim Cook tried to persuade them to stay, Venkatachary, Baker and Shukla believed Google was a better place to work on LLMs, according to anonymous sources speaking to the Information. Apple hasn’t responded to an inquiry to comment on the three engineers’ departures. Venkatachary and Baker now both hold the title of VP of engineering at Google. Venkatachary’s work focuses on “A.I. product expansion,” while Baker is working on “new stuff,” according to their LinkedIn pages. Shukla has assumed the title of distinguished engineer, a high-level engineering position at Google. John Burkey, a former Apple engineer on the ‌Siri‌ team between 2014 and 2015, told the New York Times in March that the Siri‌ voice assistant is built on “clunky code” that made it very difficult for engineers to add new features. As a result, there was no path for ‌Siri‌ to become a “creative assistant” like ChatGPT, Burkey said. Microsoft CEO Satya Nadella expressed similar views on voice assistant products in general. Voice assistants are “dumb as a rock,” Nadella said in an interview with the Financial Times in March. A.I. hype fuels the talent war in tech The past 12 months have been marked by unprecedented cost-cutting measures across the tech sector, with Google and many large tech companies laying off tens of thousands of employees and cutting back on office perks. Interestingly, Apple is the only Big Tech firm that has avoided massive layoffs, and yet stability hasn’t stopped its top engineers from leaving for more rewarding jobs. It’s not just Apple losing talent to Google. Earlier this year, the Information reported Google’s top A.I. scientists were quitting to join OpenAI because they believed what OpenAI was working on was more promising. Yesterday (May 1), University of Toronto professor Geoffrey Hinton, who is known in the industry as “the godfather of A.I.,” left his part-time advisory role at Google, fearing the tech company was moving too quickly without considering the social impact of A.I. A.I. scientists and engineers are often among the highest-paid roles at tech companies. And large firms in the industry offer similarly lucrative compensation packages. At Apple, for example, a median software engineer makes $287,000 a year, including salary, bonus and stock awards, according to levels.fyi, a tech salary tracking site. But money is usually not the top consideration when they choose where to work. “Scientists and engineers do go after high compensations,” Kyunghyun Cho, a data science professor at New York University and former research scientist at Facebook AI Research, told Observer. “But, at the end of the day, what they are looking for is an environment where they can flourish and contribute to a success.”",
        "publish_date": "2023-05-02 20:55:46"
    },
    {
        "title": "Generative A.I. Start-Up Cohere Valued At About $2 Billion In Funding Round - WorldNewsEra",
        "text": "Cohere was founded by Aidan Gomez and Nick Frosst, two Canadian researchers who had worked on artificial intelligence at Google, and Ivan Zhang, a Toronto entrepreneur. Mr. Gomez was among the Google researchers who published a key research paper that helped lead to ChatGPT and similar technologies. ChatGPT has captured the imagination of millions of people with its ability to do things like answer questions, write term papers and poetry, and generate computer code. As the chatbot’s popularity has grown, the tech industry has focused on generative artificial intelligence — technologies that can generate text, images and other media in response to short prompts. Many companies are exploring the fringes of this new area, but only a few have the resources to build the technologies from the ground up. These companies have an unusual blend of experienced researchers, enormous ambition and large amounts of money. Though investors have been reluctant to fund other start-ups, they have been pouring money into the few companies at the forefront of generative A.I. In February, Microsoft invested $10 billion in OpenAI, bringing its total investment in the company to $13 billion. And in March, Character.ai, another start-up that builds online chatbots, raised $150 million in a funding round that valued the company at $1 billion.",
        "publish_date": "2023-05-02 23:13:41"
    },
    {
        "title": "‘Godfather of AI’ leaves Google, warns of ‘scary’ technology he helped create",
        "text": "Harris and administration officials plan to tell the corporate leaders that they have a responsibility to mitigate potential harm from AI tools, according to a White House official. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools — including Google’s “Bard.” Some of the dangers of AI chatbots are “quite scary,” Hinton told the BBC. “Right now, they’re not more intelligent than us, as far as I can tell. But I think they soon may be.” In an interview with MIT Technology Review, Hinton also pointed to “bad actors” that may use AI in ways that could have detrimental impacts on society — such as manipulating elections or instigating violence.",
        "publish_date": "2023-05-02 21:47:57"
    },
    {
        "title": "Hoffman and Suleyman's AI startup Inflection launches ChatGPT-like chatbot",
        "text": "Inflection AI, the artificial intelligence startup founded by LinkedIn co-founder Reid Hoffman and Google DeepMind co-founder Mustafa Suleyman, has released its first AI chatbot product, the company said on Tuesday. Similar to OpenAI's viral chatbot ChatGPT, Inflection's AI chatbot, named Pi, uses generative AI technology to interact with users through dialogues, in which people can ask questions and share feedback. Suleyman, Inflection AI's CEO, said the startup developed the technology in-house and its Pi chatbot was built on prioritizing human-like conversations with a high level of emotional intelligence, including being kind and supportive. \"It's very balanced and even-handed on political issues or sensitive topics, but also sometimes it can be funny and silly and creative,\" Suleyman said. The chatbot is suitable for personal day-to-day tasks, but not for generating code or essays, he added. Suleyman said the company had also spent time on boundary training to make sure the AI did not violate its behavior policies, including engaging in romantic conversations. \"The goal is to make sure that the AI always knows it's an AI and never tries to imitate a human. So it reminds the human user that it is an AI frequently,\" he said. Users can interact with Pi across platforms including its website, app and social media platforms like Instagram. The service is free, and the startup may launch premium subscriptions in the future, Suleyman added. Pi uses user data, including conversational content, to train its AI systems, according to its terms of service. The chatbot is not currently connected to the internet. Chatbots powered by generative AI technology has become a crowded field since OpenAI's ChatGPT burst into the scene last November. Using large language models, which mine vast amounts of text to summarize information and generate content, chatbots like Google's Bard and Character.AI enable people to have in-depth conversations for both professional and personal needs. Founded in 2022, Inflection was incubated by VC firm Greylock, which led a $225 million investment in the startup. Co-founder Hoffman, a partner at Greylock who is also a Microsoft Corp board member, resigned from OpenAI's board in March, citing potential conflicts due to his work with AI startups.",
        "publish_date": "2023-05-02 23:46:00"
    },
    {
        "title": "Godfather of AI' quits Google to warn of the tech's dangers",
        "text": "A computer scientist often dubbed \"the godfather of artificial intelligence\" has quit his job at Google to speak out about the dangers of the technology.. Geoffrey Hinton, who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed \"profound risks to society and humanity\". \"Look at how it was five years ago and how it is now,\" he was quoted as saying in the piece, which was published on Monday. \"Take the difference and propagate it forwards. That's scary.\" Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. \"It is hard to see how you can prevent the bad actors from using it for bad things,\" he told the Times. In 2022, Google and OpenAI -- the start-up behind the popular AI chatbot ChatGPT -- started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. \"Maybe what is going on in these systems is actually a lot better than what is going on in the brain,\" he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \"takes away the drudge work\" but \"might take away more than that\", he told the Times. The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will \"not be able to know what is true anymore.\" Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to U.S. media. \"As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\" the statement added. \"We're continually learning to understand emerging risks while also innovating boldly.\" In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not \"scale this up more until they have understood whether they can control it.\" © 2023 AFP",
        "publish_date": "2023-05-02 23:21:40"
    },
    {
        "title": "Chegg CEO calls 48% stock plunge over ChatGPT fears 'extraordinarily overblown'",
        "text": "Chegg's 48% stock price plunge on Tuesday, driven by comments in the company's earnings report about the risks of artificial intelligence, was \"extraordinarily overblown,\" CEO Dan Rosensweig told CNBC Tuesday. The shares rose as much as 8% in extended trading during Rosensweig's TV interview, which followed the historic drop during regular market hours. On Monday's earnings call, Rosensweig said ChatGPT, the suddenly popular chatbot from startup OpenAI, was \"having an impact on our new customer growth rate.\" The company, which initially became well known for developing a textbook rental model for college students, has expanded into homework and exam help products. Chegg said it was only providing guidance for the coming quarter and not for the full year because it's \"too early to tell how this will play out.\" Rosensweig reminded investors, during the CNBC interview, that Chegg generates free cash flow and earnings, on an adjusted basis, and has \"more than enough cash to pay off our debt.\" The company also reported better-than-expected earnings and revenue for the first quarter. \"I think this is extraordinarily overblown, and I don't normally say that, I don't really talk about the stock price much,\" Rosensweig said. Chegg is slated to launch Cheggmate, its GPT-4 powered AI platform, in May. Rosensweig said the combination of GPT and Chegg's trove of academic data could be transformative. Rosensweig noted that ChatGPT struggles with delivering accurate answers, a phenomenon known as hallucination, and a problem in the academic world. \"Students can't be wrong when they do homework or when they learn things,\" he said. \"ChatGPT is often wrong, and it's not going to be right anytime soon.\"",
        "publish_date": "2023-05-02 23:14:36"
    },
    {
        "title": "‘Godfather of AI’ leaves Google, warns of ‘scary’ technology he helped create",
        "text": "Harris and administration officials plan to tell the corporate leaders that they have a responsibility to mitigate potential harm from AI tools, according to a White House official. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools — including Google’s “Bard.” Some of the dangers of AI chatbots are “quite scary,” Hinton told the BBC. “Right now, they’re not more intelligent than us, as far as I can tell. But I think they soon may be.” In an interview with MIT Technology Review, Hinton also pointed to “bad actors” that may use AI in ways that could have detrimental impacts on society — such as manipulating elections or instigating violence.",
        "publish_date": "2023-05-02 21:47:57"
    },
    {
        "title": "ChatGPT returns to Italy after OpenAI tweaks privacy disclosures, controls",
        "text": "ChatGPT is again available to users in Italy, after being temporarily banned by the country's data privacy authority for possible violations of the EU's General Data Protection Regulation (GDPR). Italy's Guarantor for the Protection of Personal Data announced the reinstatement of ChatGPT Friday, after Microsoft-backed OpenAI, the creator of the generative AI service, made changes requested by the government body. At the end of March, the Guarantor ordered OpenAI to stop processing ChatGPT data in Italy, effectively causing the service to shut down in the country. On April 11, the data privacy agency notified OpenAI of specific changes it would have to make in order to offer ChatGPT in Italy. Most of those changes have been met, and OpenAI now has conditional approval to offer ChatGPT in Italy, with the expectation that it will make further changes specified by the Guarantor, and continue to adhere to EU data privacy rules. The changes OpenAI has implemented include: The publication on OpenAI's website of a description of the personal data that is processed for its AI model training algorithms, and a reminder that everyone has the right to opt-out from such processing, The posting of a form designed to let European users opt-out from the processing of their personal data, and The addition of a button on a welcome back page for Italian users that asks for confirmation that they are 18 years old or are above the age of 13 and have obtained consent from their parents or guardians to use the service. \"The Italian SA acknowledges the steps forward made by OpenAI to reconcile technological advancements with respect for the rights of individuals and it hopes that the company will continue in its efforts to comply with European data protection legislation,\" the Guarantor said in its announcement Friday. In its March notification of the ChatGPT ban, the data privacy authority noted that a bug in an open-source library — disclosed by OpenAI in March but since fixed — allowed some ChatGPT users to see titles from another active user’s chat history. It also noted that \"information made available by ChatGPT does not always match factual circumstances, so that inaccurate personal data are processed.\" While OpenAI's changes have gone far enough for Italy's Guarantor to allow it operate in Italy, it expects the company to comply with additional requests, including implementation of a stronger age-confirmation mechanism, and an information campaign about the rights of Italians to opt out of the processing of their personal data for the purpose of training AI algorithms. The Italian ban and subsequent reinstatement of ChatGPT come as governments around the world consider regulations on AI. In the West, Europeans have been the first to take concrete steps to rein in AI. Last week, members of the European Parliament (MEPs) agreed on compromise amendments to the AI Act proposed by the European Commission. The act focuses on classifying AI systems into risk-based categories — banning those that are considered high risk from being used for certain purposes, such as government social-scoring systems and real-time biometric identification systems in public spaces. Earlier in April, The European Data Protection Board (EDPB) said it planned to launch a dedicated task force to investigate ChatGPT after a number of European privacy watchdogs — including Italy — raised concerns about whether the technology is compliant with the GDPR. In its announcement about the reinstatement of ChatGPT in Italy, the Guarantor said that it would \"carry on its fact-finding activities regarding OpenAI also under the umbrella of the ad-hoc task force that was set up by the European Data Protection Board.\"",
        "publish_date": "2023-05-01 18:24:00"
    },
    {
        "title": "Samsung bans staff AI use over data leak concerns",
        "text": "Samsung has reportedly banned employee use of generative AI tools like ChatGPT in a bid to stop transmission of sensitive internal data to external servers. The South Korean electronics giant issued a memo to a key division, notifying employees not to use AI tools, according to a report by Bloomberg, which said it reviewed the memo. Bloomberg did not report which division received the memo. In addition, employees using ChatGPT and other AI tools on personal devices were warned to not upload company related data or other information that could compromise the company's intellectual property. Doing so, the memo said, could result in employment termination. The memo expressed concerns over inputting data such as sensitive code on AI platforms. The worry is that anything that is typed onto an AI tool like ChatGPT will then reside on external servers, which makes retrieving and deleting them very difficult, and also potentially making them accessible by other users. “Interest in generative AI platforms such as ChatGPT has been growing internally and externally,” the memo said. “While this interest focuses on the usefulness and efficiency of these platforms, there are also growing concerns about security risks presented by generative AI.” The memo comes in the wake of a March notification by Microsoft-backed OpenAI, the creator of ChatGPT, that a bug in an open-source library — since fixed — allowed some ChatGPT users to see titles from another active user’s chat history. Samsung’s ban on the tool also comes a month after an internal survey it conducted to understand the security risks associated with AI. About 65% of employees surveyed said ChatGPT posed serious security threats. In addition, in April, Samsung engineers “accidentally leaked internal source code by uploading it to ChatGPT,” according to the memo. The memo did not, however, reveal what the code was, precisely, and did not elaborate on whether the code was simply typed into ChatGPT, or whether it was also inspected by anyone external to Samsung. Lawmakers set to regulate AI Fearing the potential ChatGPT and other AI systems to leak private data and spread false information, regulators have begun to consider restrictions on their use. The European Parliament, for instance, is days away from finalizing an AI Act, and the European Data Protection Board (EDPB) is assembling an AI task force, focusing on ChatGPT, to examine potential AI dangers. Last month, Italy imposed privacy-based restrictions on ChatGPT and temporarily banned its operation in the country. OpenAI agreed to make changes requested by Italian regulators, after which it relaunched the service. Companies that offer AI tools are starting to respond to concerns about privacy and data leakage. OpenAI last month announced that it would allow users to turn off the chat history feature for ChatGPT. The “history disabled” feature means that conversations marked as such won’t be used to train OpenAI’s underlying models, and won’t be displayed in the history sidebar, the comany said. Samsung, meanwhile, is working on internal AI tools for translating and summarizing documents as well as for software development, according to media reports. It’s also working on ways to block the upload of sensitive company information to external services. “HQ is reviewing security measures to create a secure environment for safely using generative AI to enhance employees’ productivity and efficiency,” the memo said. “However, until these measures are prepared, we are temporarily restricting the use of generative AI.” With this move Samsung joins the expanding group of companies that have exercised some form of restriction on this disruptive technology. Among them are Wall Street banks including JPMorgan Chase, Bank of America, and CitiGroup.",
        "publish_date": "2023-05-02 17:39:00"
    },
    {
        "title": "‘Godfather Of AI’ Geoffrey Hinton Exits Role At Google To Warn Of Technology’s Dangers - WorldNewsEra",
        "text": "Sounding alarms about artificial intelligence has become a popular pastime in the ChatGPT era, taken up by high-profile figures as varied as industrialist Elon Musk, leftist intellectual Noam Chomsky and the 99-year-old retired statesman Henry Kissinger. But it’s the concerns of insiders in the AI research community that are attracting particular attention. A pioneering researcher and the so-called “Godfather of AI” Geoffrey Hinton quit his role at Google so he could more freely speak about the dangers of the technology he helped create. Over his decades-long career, Hinton’s pioneering work on deep learning and neural networks helped lay the foundation for much of the AI technology we see today. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools — including Google’s “Bard.” Some of the dangers of AI chatbots are “quite scary,” Hinton told the BBC. “Right now, they’re not more intelligent than us, as far as I can tell. But I think they soon may be.” In an interview with MIT Technology Review, Hinton also pointed to “bad actors” that may use AI in ways that could have detrimental impacts on society — such as manipulating elections or instigating violence. Hinton, 75, says he retired from Google so that he could speak openly about the potential risks as someone who no longer works for the tech giant. “I want to talk about AI safety issues without having to worry about how it interacts with Google’s business,” he told MIT Technology Review. “As long as I’m paid by Google, I can’t do that.” Since announcing his departure, Hinton has maintained that Google has “acted very responsibly” regarding AI. He told MIT Technology Review that there’s also “a lot of good things about Google” that he would want to talk about — but those comments would be “much more credible if I’m not at Google anymore.” Google confirmed that Hinton had retired from his role after 10 years overseeing the Google Research team in Toronto. Hinton declined further comment Tuesday but said he would talk more about it at a conference Wednesday. At the heart of the debate on the state of AI is whether the primary dangers are in the future or present. On one side are hypothetical scenarios of existential risk caused by computers that supersede human intelligence. On the other are concerns about automated technology that’s already getting widely deployed by businesses and governments and can cause real-world harms. “For good or for not, what the chatbot moment has done is made AI a national conversation and an international conversation that doesn’t only include AI experts and developers,” said Alondra Nelson, who until February led the White House Office of Science and Technology Policy and its push to craft guidelines around the responsible use of AI tools. “AI is no longer abstract, and we have this kind of opening, I think, to have a new conversation about what we want a democratic future and a non-exploitative future with technology to look like,” Nelson said in an interview last month. A number of AI researchers have long expressed concerns about racial, gender and other forms of bias in AI systems, including text-based large language models that are trained on huge troves of human writing and can amplify discrimination that exists in society. “We need to take a step back and really think about whose needs are being put front and center in the discussion about risks,” said Sarah Myers West, managing director of the nonprofit AI Now Institute. “The harms that are being enacted by AI systems today are really not evenly distributed. It’s very much exacerbating existing patterns of inequality.” Hinton was one of three AI pioneers who in 2019 won the Turing Award, an honor that has become known as tech industry’s version of the Nobel Prize. The other two winners, Yoshua Bengio and Yann LeCun, have also expressed concerns about the future of AI. Bengio, a professor at the University of Montreal, signed a petition in late March calling for tech companies to agree to a 6-month pause on developing powerful AI systems, while LeCun, a top AI scientist at Facebook parent Meta, has taken a more optimistic approach.",
        "publish_date": "2023-05-03 01:08:15"
    },
    {
        "title": "Nextdoor Unveiled New AI Assistant, Built on OpenAI&#039;s ChatGPT Models",
        "text": "The American social networking service operating hyperlocal for neighborhoods, Nextdoor Holdings has introduced its first generative AI feature, an in-app assistant that can help users rewrite potentially unkind posts on the neighborhood social network. According to the firm, the new feature will be rolled out over the next several weeks. The firm has experimented with methods to remind consumers to keep conversations civil in the past. As we know, the firm has occasionally found it difficult to combat the idea that its platform may be poisonous, started employing kindness reminders in 2019, and launched pop-ups asking users to be more sympathetic last year. Nextdoor New AI Assistant According to the social networking service, the app now offers more focused prods to encourage non-racist speech and temperate political debates. The new assistant expands on the strategy. The assistant will urge users to rephrase potentially hurtful statements rather than reminding them in advance and offer new language for the topic. Nextdoor CEO Sarah Friar said the new AI assistant it’s great to tell people, ‘Hey, be a little bit more constructive,’ or ‘You don’t always have to respond. “But now you’re actually helping them reframe it in a way they might not have thought of, She added. The assistant-written posts are optional, and users can edit the suggested content, according to Friar. According to her, it’s stating that if you phrase things in this way, or even if you just add a few additional phrases, some context, or an emoji, you can make someone more likely to understand you. Even if they don’t agree with you, they can at least start to hear you. Nextdoor New AI Assistant is Built on OpenAI’s ChatGPT According to Friar, the new AI assistance is built using the same OpenAI models as ChatGPT and has been trained using all of the information we have amassed throughout our nearly ten-year existence. This, according to her, enables the assistant to make more specific suggestions for various posts on the platform. In a sample given by the firm, the assistant rewrites a user’s post asking for landscaping assistance with fresh language that the app claims may receive more responses than the original. The assistant may be Nextdoor’s first application of generative AI, but it is unlikely to be the last. According to Friar, she is especially curious about how generative AI may be applied to platform suggestions for small enterprises.",
        "publish_date": "2023-05-02 14:04:30"
    },
    {
        "title": "擔心聊天紀錄被 ChatGPT 出賣？官方最新「一設定」最好要關閉",
        "text": "即時 熱門 政治 軍武 社會 生活 健康 國際 地方 蒐奇 影音 財經 娛樂 汽車 時尚 體育 3 C 評論 藝文 玩咖 食譜 地產 專區 TAIPEI TIMES 求職 爆 Search 自由電子報 3C科技 3C首頁 影音專區 智慧手機 實用秘技 科技趣聞 網路社群 好攝相機 電腦應用 家電娛樂 粉絲團 自由影音 即時 熱門 政治 軍武 社會 生活 健康 國際 地方 蒐奇 財經 娛樂 藝文 汽車 時尚 體育 3 C 評論 玩咖 食譜 地產 專區 服務 自由電子報APP 自由電子報粉絲團 自由電子報Line 自由電子報Twitter 熱門新訊 已經加好友了，謝謝 歡迎加入【自由3C科技】 按個讚　心情好 已經按讚了，謝謝。 3C 〉 網路社群 〉 網路服務 害怕ChatGPT外洩私密？傳微軟打造「私有版」收費高10倍 2023/05/03 08:02 文／記者吳佩樺 傳聞微軟準備打造「私有版ChatGPT」。(圖／路透社) OpenAI的AI聊天機器人ChatGPT爆紅，但在保密方面也受到質疑，因此傳出有一些大型企業禁止員工使用ChatGPT，避免洩露公司機密。 外媒The Information引述知情人士報導，微軟Azure雲端服務部門計劃打造「私有版ChatGPT」，擁有專門的雲端伺服器，用來獨立保存數據，與其他用戶的數據分開，藉此讓用戶安心，保證秘密不會流到ChatGPT主系統，不過價格可能並不便宜，預計將是現有付費版ChatGPT的10倍之多。 請繼續往下閱讀... 此外，為了降低各界擔憂的隱私問題，ChatGPT在上月推出關閉聊天記錄新功能，用戶可自行決定開啟或關閉，關閉後所有的對話只會留存30天，過後永久刪除，官方保證啟用該功能，所有對話不會被用來訓練和改進AI。 《你可能還想看》 擔心聊天紀錄被 ChatGPT 出賣？官方最新「一設定」最好要關閉 不用抽 不用搶 現在用APP看新聞 保證天天中獎　 點我下載APP　 按我看活動辦法 聊天機器人 ChatGPT 微軟 微軟 網路服務 相關新聞 Google 不藏了！自曝未發表 Pixel 新手機實照　大秀超美新色 Google 不藏了！在官方推特大方地秀出新手機的部分照片，外界預期將是中階定位的 Pixel 7a，且正式預告將在 5...... 搶先 HTC U23 Pro 上市？ 宏達電神秘新品悄悄通過 NCC 認證了 HTC 宏達電近來除了已遭曝光的中高階耳機 HTC U23 Pro，傳出準備正式登場之外，另，還有一款神秘的真無線耳機新...... 比 Xperia 1 V 更旗艦？傳Sony開發7吋神秘新機 Sony下週發表新旗艦Xperia 1 V，可望換上全新「Lytia」感光元件，引起熱烈討論，不過卻有新爆料，Sony可...... 熱門文章 Sony失守、小米跌出榜外！台灣手機十大品牌排行榜單「黑馬」竄出 徠卡也救不了小米！全球5大手機品牌最新出貨量揭曉 台灣最新「手機市佔」排行榜出爐！蘋果 iPhone 領先優勢快沒了 下月發新旗艦！Sony集團社長卻「悲觀」看全球手機市場 比手機更便宜！華碩遊戲掌機 ROG Ally「平價款售價」曝光　玩家嗨爆 HTC 首度升級 1 億畫素！未上市「中高階新機」真實照片現身 看更多！加入3C科技粉絲團 網友回應",
        "publish_date": "2023-05-03 02:13:58"
    },
    {
        "title": "'Godfather of AI' Quits Google to Warn of the Technology's Dangers - The Chosun Ilbo (English Edition): Daily News from Korea - World",
        "text": "◆ Jobs Could Be at Risk In 2022, Google and OpenAI -- the start-up behind the popular AI chatbot ChatGPT -- started building systems using much larger amounts of data than before. Hinton told the Times he believed these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. \"Maybe what is going on in these systems is actually a lot better than what is going on in the brain,\" he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \"takes away the drudge work\" but \"might take away more than that,\" he told the Times. ◆ Concern About Misinformation The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will \"not be able to know what is true anymore.\" Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to U.S. media. \"As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\" the statement added. \"We're continually learning to understand emerging risks while also innovating boldly.\" In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people. including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not \"scale this up more until they have understood whether they can control it.\"",
        "publish_date": "2023-05-03 01:13:03"
    },
    {
        "title": "傳微軟等執行長4日受邀磋商AI 白宮大咖官員皆出席",
        "text": "即時 熱門 政治 軍武 社會 生活 健康 國際 地方 蒐奇 影音 財經 娛樂 汽車 時尚 體育 3 C 評論 藝文 玩咖 食譜 地產 專區 TAIPEI TIMES 求職 爆 Search 自由電子報 自由財經 財經首頁 財經政策 影音專區 國際財經 證券產業 房產資訊 財經週報 基金查詢 投資理財 粉絲團 自由影音 即時 熱門 政治 軍武 社會 生活 健康 國際 地方 蒐奇 財經 娛樂 藝文 汽車 時尚 體育 3 C 評論 玩咖 食譜 地產 專區 服務 自由電子報APP 自由電子報粉絲團 自由電子報Line 自由電子報Twitter 熱門新訊 首頁 > 國際財經 傳微軟等執行長4日受邀磋商AI 白宮大咖官員皆出席 2023/05/03 08:55 拜登政府邀請AI公司的執行長們一同商討人工智慧（AI）問題。（路透） 〔財經頻道／綜合報導〕據白宮官員透露，Google母公司Alphabet、微軟（Microsoft）、OpenAI、Anthropic的執行長們，週四（4日）將受邀與副總統賀錦麗（Kamala Harris）及其他政府高級官員共同商討人工智慧（AI）的問題。 AI技術快速發展，隨之而來的問題包括隱私、詐騙、假訊息以及訊息偏差等。根據《路透》看到的邀請函顯示，總統拜登期望像上述這樣的公司，在向公眾提供AI產品之前可以確保其安全。 請繼續往下閱讀... 白宮官員稱，4日的會議會有白宮幕僚長齊安茲（Jeff Zients）、副幕僚長利德（Bruce Reed）、國家安全顧問蘇利文（Jake Sullivan）、經濟委員會主席布蘭納德（Lael Brainard）、商務部長雷蒙多（Gina Raimondo）等重要官員出席。 白宮上月表示，因為還不清楚AI會對國家安全和教育造成何種影響，正就其AI潛在問責措施向公眾徵求意見。 一手掌握經濟脈動 點我訂閱自由財經Youtube頻道 不用抽 不用搶 現在用APP看新聞 保證天天中獎　 點我下載APP　 按我看活動辦法 已經加好友了，謝謝 歡迎加入【自由財經】 按個讚　心情好 已經按讚了，謝謝。 相關新聞 LTN經濟通》聊天機器人ChatGPT 引爆腥風血雨 AI教父離開Google 警惕AI帶來潛在風險 沿著台86線 翻轉台南公路經濟 三星禁生成式AI工具 被抓到就開除 3大雲端巨頭 在台已培育逾10萬數位人才 聯手打詐 林右昌：與Google達3共識 英國開發「可精準識別癌症」AI模型 盼及早發現掌握黃金治療時間 看更多！請加入自由財經粉絲團 今日熱門新聞 CNBC：中芯不可能量產7奈米晶片 晶片大戰 Google前執行長曝台灣具2大優勢 勞動基金Q1報佳音 帳面大賺2190億元 LTN經濟通》財閥一手遮天 南韓變地獄朝鮮？ 德儀庫存破千億 彭博：晶片業觸底充滿變數 洛克希德馬丁獲126架F-35大單 價值2391億 LTN經濟通》Fed頻升息 港幣被送終？ 跨性別網紅業配後 百威淡啤銷量暴跌26％ 網友回應 財經政策 影音專區 國際財經 證券產業 房產資訊 財經週報 基金查詢 投資理財",
        "publish_date": "2023-05-03 03:07:58"
    },
    {
        "title": "White House calls in tech firms to talk AI risks",
        "text": "WASHINGTON: The White House plans to meet with top executives from Google, Microsoft, OpenAI and Anthropic on Thursday (May 4) to discuss the promise and risks of artificial intelligence. Vice President Kamala Harris and other US administration officials will discuss ways to ensure consumers benefit from AI while being protected from its harms, according to a copy of an invitation seen by AFP. US President Joe Biden expects tech companies to make sure products are safe before being released to the public, the invitation said. US regulators last month took a step towards drawing up rules on AI that could see the White House put the brakes on new technologies such as ChatGPT. The US Department of Commerce put out a call for input from industry actors that would serve to inform the Biden administration in drafting regulations on AI. \"Just as food and cars are not released into the market without proper assurance of safety, so too AI systems should provide assurance to the public, government, and businesses that they are fit for purpose,\" the Commerce Department said in a statement at the time. The United States is home to the biggest innovators in tech and AI - including Microsoft-backed OpenAI, which created ChatGPT - but trails internationally in regulating the industry. Google in March invited users in the United States and Britain to test its AI chatbot, known as Bard, as it continues on its gradual path to catch up with ChatGPT. Biden has urged Congress to pass laws putting stricter limits on the tech sector, but these efforts have little chance of making headway given political divisions among lawmakers. The lack of rules has given Silicon Valley freedom to put out new products rapidly - and stoked fears that AI technologies will wreak havoc on society before the government can catch up. Billionaire Elon Musk in early March formed an AI company called X.AI, based in the US state of Nevada, according to business documents. Musk, who is already the boss of Twitter and Tesla, is listed as a director of X.AI Corporation, a state business filing indicated. Musk's founding of what appears to be a rival to OpenAI came despite him recently joining tech leaders and AI critics in calling for an overall pause in the development of artificial intelligence. Google, Meta and Microsoft have spent years working on AI systems to help with translations, internet searches, security and targeted advertising. But late last year San Francisco firm OpenAI supercharged the interest in the AI sphere when it launched ChatGPT, a bot that can generate natural-seeming text responses from short prompts.",
        "publish_date": "2023-05-03 03:44:57"
    },
    {
        "title": "‘Godfather of AI’ Geoffrey Hinton quits Google, warns of danger of technology to human way of life",
        "text": "The West Australian Perth Now Click to open navigation ‌‌ News Chevron Down Icon Breaking News Western Australia National World Technology Opinion Weather Sport Chevron Down Icon AFL Cricket Soccer Basketball Tennis NRL Rugby Motor Racing MMA Golf Netball Cycling Entertainment Chevron Down Icon Confidential Movies Television Music Reviews Books Competitions Business Chevron Down Icon Breaking News Economy Markets Property Commercial Property Workplace Matters Lifestyle Chevron Down Icon Food Personal Finance Health Parenting Fashion Travel Home & Garden Relationships Stars Real Estate HUH? Local News Chevron Down Icon North Central South Mandurah Competitions Find My Paper Digital Editions Read your local paperNews to your inbox Camera IconGeoffrey Hinton fears AI could change our way of life for the worse. Credit: The West Australian ‘Godfather of AI’ Geoffrey Hinton quits Google, warns of danger of technology to human way of life Luke AndrewsDaily Mail May 3, 2023 9:48AM Comments TopicsTechnologyBusinessLifestyleNewsWorld News The “Godfather of artificial intelligence” has sensationally resigned from Google and warned the technology could upend life as we know it. Geoffrey Hinton is credited with creating the technology that became the bedrock of AI systems such as ChatGPT and Google Bard. But the Turing prize winner now says a part of him regrets helping to make the systems, fearing it could prompt the proliferation of misinformation and replace people in the workforce. Mr Hinton said he had to tell himself excuses like “if I didn’t build it, someone else would have” to prevent himself from being overwhelmed by guilt. He drew comparisons with the “father of the atomic bomb” Robert Oppenheimer, who was reportedly distraught by his invention and dedicated the rest of his life to stopping its proliferation. Your cookie settings are preventing this third party content from displaying. If you’d like to view this content, please adjust your Cookie Settings. To find out more about how we use cookies, please see our Cookie Guide. Speaking to the New York Times about his resignation, he warned that in the near future AI would flood the internet with false photos, videos and texts. These would be of a standard, he added, where the average person would “not be able to know what is true anymore”. The technology also posed a serious risk to “drudge” work and could upend the careers of people working as paralegals, personal assistants and translators. Some workers already say they are using it to cover multiple jobs for them, undertaking tasks such as creating marketing materials and transcribing Zoom meetings so that they do not have to listen. “Maybe what is going on in these systems is actually a lot better than what is going on in the (human) brain,” Mr Hinton said, explaining his fears. “The idea that this stuff could actually get smarter than people — a few people believed that. “But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. “Obviously, I no longer think that.”Camera Icon‘Godfather of AI’ Geoffrey Hinton. Credit: Noah Berger/AP Asked about why he had helped develop a potentially dangerous technology, Mr Hinton said: “I console myself with the normal excuse: If I hadn’t done it, somebody else would have.” He added that he had previously paraphrased Oppenheimer when posed with the question in the past, saying: “When you see something that is technically sweet, you go ahead and do it.” Mr Hinton decided to quit Google in April after a decade at the tech giant amid the proliferation of AI technologies. He had a long conversation with the chief executive of Google’s parent company Alphabet, Sundar Pichai, before departing — although it is not clear what was said. In a broadside to his former employer, Mr Hinton accused Google of not being a “proper steward” for AI technologies. In the past, the company has kept potentially dangerous technologies under wraps, he said. But it had now thrown caution to the wind as it competes with Microsoft, which added a ChatBot to its search engine Bing in April. TechnologyGuardrails and guidelines as AI embeds itself deeper TechnologyMusk, experts urge pause on training AI systems Google’s chief scientist Jeff Dean said: “We remain committed to a responsible approach to AI. “We’re continually learning to understand emerging risks while also innovating boldly.” Mr Hinton’s warning comes as Silicon Valley descends into a civil war over the advancement of artificial intelligence — with the world’s greatest minds split over whether it will elevate or destroy humanity. Elon Musk, Apple co-founder Steve Wozniak and the late Stephen Hawking are among the most famous critics of AI who believe it poses a “profound risk to society and humanity” and could have “catastrophic effects”. They have called for a pause in the “dangerous race” to roll out advanced AI, saying more risk assessments were needed. But Bill Gates, My Pichai and futurist Ray Kurzweil are on the other side of the debate, hailing the technology as “our time’s most important innovation”. They argue it could cure cancer, solve climate change and boost productivity. Mr Hinton has not previously added his voice to the debate, saying he did not want to speak out until he had formally left Google. He surged to fame in 2012 when at the University of Toronto in Canada, helping design a neural network that could analyse thousands of photos and teach itself to identify common objects such as flowers, dogs and cars. Mr Hinton started a company based on the technology, which was bought out by Google for $44 million. Advanced AI systems already available include ChatGPT, which now has more than a billion people signed up after its release in November.Camera IconWhat does the future hold for AI and humans? Credit: AAP Data shows that it also has as many as 100 million active monthly users. Launched by OpenAI, based in San Francisco, the platform has become an instant success worldwide. The chatbot is a large language model trained on massive text data, allowing it to generate eerily human-like text in response to a given prompt. The public uses ChatGPT to write research papers, books, news articles, emails and other text-based work and while many see it more like a virtual assistant, many brilliant minds see it as the end of humanity. If humans lose control of AI then it will be considered to have reached singularity, which means it has surpassed human intelligence and has independent thinking. AI would no longer need or listen to humans, allowing it to steal nuclear codes, create pandemics and spark world wars. “It’s almost akin to a war between chimps and humans,” DeepAI founder Kevin Baragona said. “The humans obviously win since we’re far smarter and can leverage more advanced technology to defeat them. “If we’re like the chimps, then the AI will destroy us or we’ll become enslaved to it.” Share to Facebook Share to Twitter Email Us Copy the Link Register and have your say. Register to comment Already have an account? Log in Find out moreThe first solo exhibition of world-renowned artist Yoshitomo Nara is now on at AGWA. From around the site ‘Was coping poorly’Celebrity chef’s secret health battle revealed Comments Meow Overheard at Met Gala: BEST celeb gossip from event of year Comments Camera Icon SEE THE PICS: Who wore what on fashion’s biggest night Wedding horrorBride killed by drunk driver hours after getting married Your Local News Share to TwitterShare to FacebookShare to InstagramShare to YoutubeEmail UsGet Digital Edition Find out moreThe first solo exhibition of world-renowned artist Yoshitomo Nara is now on at AGWA. Perth Now Email UsNewsletter Chevron Down IconSubmit story tip Camera IconSubmit photos Get Digital EditionDigital edition Chevron Down IconBack to top Follow Us Share to FacebookShare to TwitterShare to InstagramShare to YoutubeGet Digital EditionEmail Us About Us Contact Us Careers Letter to the Editor Place an Ad Privacy Policy Code of Conduct Editorial Complaints Terms of Use Cookie Settings © West Australian Newspapers Limited 2023 News Sport Entertainment Business Lifestyle Privacy Policy Code of Conduct Editorial Complaints Terms of Use Cookie Settings Subscribe to the Paper Read your PerthNow Local Newspaper Advertise on PerthNow Our Partners The West Australian 7plus 7NEWS Car Expert Starts at 60 Raiz Invest Home Beautiful Better Homes & Gardens Marie Claire New Idea That’s Life Beauty Crew SocietyOne HealthEngine iSeekplant Institchu Carbar",
        "publish_date": "2023-05-03 03:48:16"
    },
    {
        "title": "IBM to freeze hiring as CEO expects AI to replace 7,800 jobs",
        "text": "IBM will freeze hiring as it expects about 7,800 jobs to be replaced by Artificial Intelligence (AI) in the coming years, the tech giant’s CEO has said. In an interview with Bloomberg News, IBM CEO Arvind Krishna said he could “easily see” nearly one-third of the company’s non-customer-facing roles being replaced in the next five years. “These non-customer-facing roles amount to roughly 26,000 workers,” Krishna said in the interview published on Tuesday. “I could easily see 30 percent of that getting replaced by AI and automation over a five-year period.” Back-office employees are only a small portion of IBM’s 260,000 or so workers and the company, based in Armonk, New York, has continued to fill roles even after letting go of about 5,000 workers in other areas, according to Bloomberg. IBM did not immediately respond to Al Jazeera’s request for comment. An IBM spokesperson told the AFP there was no blanket hiring pause in place but that the firm was being “very selective when filling jobs that don’t directly touch our clients or technology”. Krishna’s comments come as the rapid advancement of AI-power technology, including OpenAI’s ChatGPT, raises the possibility of huge disruption to numerous industries. Some analysts fear that AI could soon lead to mass layoffs, while others argue the technology’s ability to improve productivity and complement human workers will create jobs and whole new industries. On Tuesday, the share price of Chegg, a California-based learning company, plunged by nearly 50 percent after its CEO Dan Rosensweig acknowledged on an earnings call that ChatGPT was “having an impact on our new customer growth rate”.",
        "publish_date": "2023-05-03 04:27:15"
    },
    {
        "title": "Google, Microsoft CEOs called to AI meeting at White House",
        "text": "The chief executives of Alphabet Inc's Google, Microsoft, OpenAI and Anthropic will meet with Vice President Kamala Harris and top administration officials to discuss key artificial intelligence (AI) issues on Thursday, said a White House official. The invitation seen by Reuters to the CEOs noted President Joe Biden's \"expectation that companies like yours must make sure their products are safe before making them available to the public.\" Concerns about fast-growing AI technology include privacy violations, bias and worries it could proliferate scams and misinformation. In April, Biden said it remains to be seen whether AI is dangerous but underscored that technology companies had a responsibility to ensure their products were safe. Social media had already illustrated the harm that powerful technologies can do without the right safeguards, he said. The administration has also been seeking public comments on proposed accountability measures for AI systems, as concerns grow about its impact on national security and education. On Monday, deputies from the White House Domestic Policy Council and White House Office of Science and Technology Policy wrote in a blog post about how the technology can pose a serious risk to workers. The Thursday meeting will be attended by Biden's Chief of Staff Jeff Zients, Deputy Chief of Staff Bruce Reed, National Security Adviser Jake Sullivan, Director of the National Economic Council Lael Brainard and Secretary of Commerce Gina Raimondo among others, said the White House official who did not wish to be named. The companies did not immediately respond to a request for comment.ChatGPT, an AI program that recently grabbed the public's attention for its ability to write answers quickly to a wide range of queries, in particular has attracted U.S. lawmakers' attention as it has grown to be the fastest-growing consumer application in history with more than 100 million monthly active users. \"I think we should be cautious with AI, and I think there should be some government oversight because it is a danger to the public,\" Tesla Chief Executive Elon Musk said last month in a television interview.",
        "publish_date": "2023-05-03 05:29:46"
    },
    {
        "title": "Hoffman and Suleyman&#8217;s AI startup Inflection launches ChatGPT-like chatbot",
        "text": "Inflection AI, the artificial intelligence startup founded by LinkedIn co-founder Reid Hoffman and Google DeepMind co-founder Mustafa Suleyman, has released its first AI chatbot product, the company said on Tuesday. Similar to OpenAI's viral chatbot ChatGPT, Inflection's AI chatbot, named Pi, uses generative AI technology to interact with users through dialogues, in which people can ask questions and share feedback. Suleyman, Inflection AI's CEO, said the startup developed the technology in-house and its Pi chatbot was built on prioritizing human-like conversations with a high level of emotional intelligence, including being kind and supportive. \"It's very balanced and even-handed on political issues or sensitive topics, but also sometimes it can be funny and silly and creative,\" Suleyman said. The chatbot is suitable for personal day-to-day tasks, but not for generating code or essays, he added. Suleyman said the company had also spent time on boundary training to make sure the AI did not violate its behavior policies, including engaging in romantic conversations. \"The goal is to make sure that the AI always knows it's an AI and never tries to imitate a human. So it reminds the human user that it is an AI frequently,\" he said. Users can interact with Pi across platforms including its website, app and social media platforms like Instagram. The service is free, and the startup may launch premium subscriptions in the future, Suleyman added. Pi uses user data, including conversational content, to train its AI systems, according to its terms of service. The chatbot is not currently connected to the internet. Chatbots powered by generative AI technology has become a crowded field since OpenAI's ChatGPT burst into the scene last November. Using large language models, which mine vast amounts of text to summarize information and generate content, chatbots like Google's Bard and Character. AI enable people to have in-depth conversations for both professional and personal needs. Founded in 2022, Inflection was incubated by VC firm Greylock, which led a $225 million investment in the startup. Co-founder Hoffman, a partner at Greylock who is also a Microsoft Corp board member, resigned from OpenAI's board in March, citing potential conflicts due to his work with AI startups.",
        "publish_date": "2023-05-03 05:39:27"
    },
    {
        "title": "ChatGPT creator says there’s 50% chance AI ends in ‘doom’",
        "text": "One of the creators of ChatGPT has added to a growing chorus of researchers warning of the potentially catastrophic consequences of artificial intelligence development. Former OpenAI worker Paul Christiano, who now runs AI research non-profit Alignment Research Center, said he believed there was a significant chance that the technology would lead to the destruction of humanity. The main danger, he claimed, will come when AI systems reach and surpass the cognitive capacity of a human. Dr Christiano predicts there is a “50/50 chance of doom” once this moment arrives. “I tend to imagine something like a year’s transition from AI systems that are a pretty big deal, to kind of accelerating change, followed by further acceleration, et cetera,” he told the Bankless podcast. “I think once you have that view then a lot of things may feel like AI problems because they happen very shortly after you build AI.” He added: “The most likely way we die involves – not AI comes out of the blue and kills everyone – but involves we have deployed a lot of AI everywhere... [And] if for some reason, God forbid, all these AI systems were trying to kill us, they would definitely kill us.” The comments come amid increased concerns surrounding the rapid advancement of artificial intelligence in recent months, with the so-called godfather of AI Geoffrey Hinton quitting Google to sound the alarm about the dangers of AI. Speaking to The New York Times, he said he regretted the work that he had contributed to the field due to the unpredictable future we now face. “The idea that this stuff could actually get smarter than people – a few people believed that – but most people thought it was way off. And I thought it was way off. I thought it was 30-50 years or even longer away. Obviously I no longer think that,” he said. “I don’t think they should scale this up more until they have understood whether they can control it.” His stance has been praised by other researchers, with AnthropicAI’s Catherine Olsson saying it may encourage others within the field to speak up. “In college I stopped eating meat, on the spot, when a friend asked why I hadn’t yet. Social checks on our ethics can be so influential,” she tweeted. “I often think about when I would quit Anthropic or leave AI entirely. I encourage others to. I can already tell this move will influence me.” Other prominent figures have also urged AI firms to pause development on advanced systems, most recently through an open letter signed by thousands of experts that urged governments to step in if artificial intelligence development was not paused for at least six months. Among the signatories was Elon Musk, who has frequently spoken about the existential threat posed by AI. The tech billionaire, who co-founded OpenAI, tweeted on Monday: “Even benign dependency on AI/Automation is dangerous to civilization if taken so far that we eventually forget how the machines work.”",
        "publish_date": "2023-05-03 07:11:01"
    },
    {
        "title": "Samsung bans AI chatbots after leak - The Korea Herald",
        "text": "Samsung Electronics joined other tech companies in banning the use of ChatGPT and other AI-powered chatbots by its employees, after discovering leaks of sensitive internal codes by its engineers. Taking measures to ban access of generative AI tools on company-owned computers, tablets and phones, Samsung is also reportedly creating its own service tools to support translation and the summarizing of documents, as well as software development. The tech giant confirmed Wednesday it issued a memo last week banning the use of generative AI tools to the staff of its Device eXperience division in charge of consumer appliances and mobile devices. \"Using generative AI tools on company PCs will be banned temporarily from May 1,\" the note said, also asking employees to refrain from uploading anything related to the company, themselves or other employees on the AI chatbots while using their personal devices. “We ask that you diligently adhere to our security guideline and failure to do so may result in a breach or compromise of company information, resulting in disciplinary action up to and including termination of employment,” Samsung added. Earlier in April, the company's Device Solution division overseeing its semiconductor business found three misuse cases where its engineers uploaded sensitive company information, including their meeting minutes and source codes, on ChatGPT for work. While it is unclear whether the sensitive information has been leaked to other users of the chatbot, the company immediately issued a notice to the division staff following the incident to not use generative AI tools for work. Acknowledging that staff were using external AI chatbots to increase the efficiency of their work, Samsung said it is developing an optimal AI tool for the staff to use in translation, summarizing documents and in source code development. The fear that ChatGPT and other similar chatbots operated by companies such as Microsoft and Google would leak sensitive company information to the public has prompted many companies to ban the use of AI tools. South Korean companies, SK hynix and Posco have also prohibited the use of AI services in the company. Amazon issued a similar warning to its employees and several major banks in the US, including JPMorgan Chase, Bank of America and Citigroup, also introduced similar measures. In a survey Samsung conducted of its DX division staff last month, 65 percent of respondents said they believe security risks can occur from using ChatGPT for work. ChatGPT, in its default mode, saves up the user's conversation history, and the inputs are used to improve or \"train\" the AI and utilize the stored data to generate responses to inquiries made by other users. In response to the concerns, OpenAI, the company that developed the ChatGPT, said it added an \"incognito\" mode under which the user can block their chats being used for training, last month. Other companies have been using the generative AI tools for their work, despite the security concerns. Goldman Sachs, which does restrict its employees from using ChatGPT at work, said its software developers still use generative AI tools to write and test code, though it did not reveal which service they use. IBM CEO Arvind Crishna also said in a recent interview that the company will suspend hiring for jobs that could be replaced with AI tools in the coming years.",
        "publish_date": "2023-05-03 07:16:00"
    },
    {
        "title": "Older generations trail the nation on AI know-how: Poll",
        "text": "Artificial intelligence has become wildly popular for many Americans, but people over the age of 45 are trailing those younger than them on AI familiarity, a Fox News poll shows. Fifty-eight percent of registered voters over the age of 45 who were surveyed for the poll say they are not familiar with AI technology such as OpenAI’s ChatGPT. Only 41% of registered voters over 45 reported they are familiar with the technology. The figures stand in stark contrast to younger Americans, with a whopping 65% of registered voters under the age of 45 reporting they are familiar with AI tech, such as ChatGPT. Only 35% of that group reported that they are not familiar AI. The Fox News poll was conducted April 21-24 under the direction of Beacon Research and Shaw & Company Research among 1,004 registered voters nationwide. The voters were randomly selected and spoke with live interviewers on both landlines and cellphones. The poll has a sampling error of plus or minus 3 percentage points. FOX NEWS POLL: MORE SEE BAD THAN GOOD IN AI OpenAI's release of ChatGPT in November was a game-changer for artificial intelligence, becoming the fastest-growing user base with 100 million monthly active users in January. People across the world rushed to use the chatbot, which ​​simulates human-like conversations based on prompts it is given. CLICK HERE FOR TOPLINE AND CROSS TABS The poll found that familiarity with AI technology like ChatGPT has a gender divide. Sixty percent of men reported being familiar with the technology, compared to 42% of women. Only 40% of male registered voters said they were unfamiliar with the technology, compared to 57% of women. REGULATE AI? GOP MUCH MORE SKEPTICAL THAN DEMS THAT GOVERNMENT CAN DO IT RIGHT: POLL Voters with college degrees were more likely to be familiar with AI tech at 57%, compared to 46% of voters with no college degree reporting the same. Voters across the board, by an 8-point margin, reported they believe AI is bad for society instead of good. Voters who were familiar with AI tech, however, were more likely to report artificial intelligence is good for society, at 46%. POLICE USING AI COULD LEAD TO ‘PREDICTIVE’ CRIME PREVENTION ‘SLIPPERY SLOPE,’ EXPERTS ARGUE Seventy-six percent of voters polled, including those familiar and unfamiliar with AI, reported that the government should regulate artificial intelligence, but 59% of the voters said they have no confidence or not much confidence the government can properly regulate it. CLICK HERE TO GET THE FOX NEWS APP \"Americans are unsure what to think of our new robot overlords,\" Daron Shaw, a Republican who conducted the Fox News poll with Democrat Chris Anderson, said. \"They’re skeptical elected leaders are up to the task of placing appropriate limits on this new tech, which probably says something about opinion on the tech and opinion on our leaders.\"",
        "publish_date": "2023-05-03 08:00:09"
    },
    {
        "title": "Have we ventured too far?",
        "text": "Last week, Geoffrey Hinton, the expatriate British computer scientist, in an interview with the New York Times (published on Monday), revealed his growing fear with developments in the field of artificial intelligence (AI). Hinton, who is referred to as the ‘Godfather of AI’ tendered his resignation as the head of the Google Brain research department last week, and joined an ever-expanding list of worried expert voices on the future direction of AI. Initially, Hinton studied experimental psychology at King’s College, Cambridge. As a graduate student at the University of Edinburgh in 1972, Hinton supported the concept of a neural network, an idea which failed to attract the attention of many researchers. The development of the neural network, a mathematical system which learns skills by analyzing data, became his life’s pursuit. After being awarded a PhD from Edinburgh in 1978, Hinton lectured in computer science at Carnegie Mellon University, in Pitts-burgh, USA. In the 1980s, most of the funding available for research in AI came from the US Department of Defense, and, thus, Hinton, who is fervently opposed to the application of artificial intelligence in war, moved to Canada. In 2018, Hinton and two of his colleagues at the University of Toronto, received the Turing Award – the ‘Nobel Prize’ of computing – for their research on neural networks. The trio created a neural network that taught itself to identify common objects such as dogs, cars, and flowers after analyzing thousands of photographs. Five years ago, tech companies such as Google, Microsoft and OpenAI started building neural networks which learned from enormous quantities of digital text called large language models (LLMs) to generate text on their own, including computer programs. This development assists computer programmers and writers to generate and execute ideas more quickly. However, experts have warned that LLMs can learn unwanted and unexpected behaviours, which can spawn false, biased and harmful information. These experts also note that as systems become more powerful they will introduce new risks. Hinton is clearly worried about what his life’s work will generate from herein on. “Maybe what is going on in these systems is a lot better than what is going on in the brain. Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That’s scary,” Hinton opined. So scary that after the San Francisco-based tech company OpenAI released a new version of ChatGPT in March, more than 1,000 (the number has since swollen to over 27,000 the New York Times reported on Monday) technology leaders and researchers signed an open letter calling for a six-month moratorium on the development of new systems because AI presented “profound risks to society and humanity. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable,” the letter read in part. A few days later, 19 current and former leaders of the Association for the Advancement of Artificial Intelligence, a 40-year-old academic society, added their voices of concern over the future of AI, in another letter. Now, having departed from Google after ten years, Hinton is heaping his concerns as well on the table. “The idea that this stuff could actually get smarter than people — a few people believed that,” Hinton observed. “But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.” Following up on the NY Times interview, the BBC News on Monday evening, asked Hinton to elaborate on what he meant when he said “bad players” would try to use AI for “bad things”. He responded, “This is just a worst-case scenario, kind of a nightmare scenario. You can imagine, some bad actor like [Russian President Vladimir] Putin decided to give robots the ability to create their own sub-goals.” Worst-case scenario or not, the ‘Godfather of AI’ has painted a rather bleak picture of the weak grasp humanity has on the reins of control over the immediate direction and distance of AI. Hinton conceded, ”I console myself with the normal excuse: If I hadn’t done it, somebody else would have.” Now saddled with the stark reality of his life’s work, Hinton’s words sound like the echoes of a fellow scientist, the American theoretical physicist Julius Robert Oppenheimer, one of the fathers of the invention of the atomic bomb. Just 11 days after the bombing of Hiroshima, on August 17, 1945, Oppenheimer wrote to the US government expressing his wish for the banning of nuclear weapons. He is famously attributed with uttering a quote from the Bhagavad Gita, “Now I become Death, the destroyer of worlds.” Oppenheimer spent the latter decades of his life as a campaigner for nuclear disarmament. Hinton might not enjoy such an epoch, the genie is out of the bottle and it is too late to return it. Humanity now has another woe of its own creation on the horizon to accompany the ills of climate change.",
        "publish_date": "2023-05-03 08:01:07"
    },
    {
        "title": "LG CNS, Microsoft Korea bolster security business ties - The Korea Herald",
        "text": "Information technology service provider LG CNS said Wednesday it will accelerate business cooperation in the IT security sector with the Korean unit of US tech behemoth Microsoft. The Korean firm said it became Microsoft’s managed security service provider, offering integrated security solutions from consulting to operations. Microsoft considers a firm’s security business capabilities and growth potential to determine its MSSP partner qualification. The news came after the top management meeting between LG CNS Chief Executive Officer Hyun Shin-gyoon and Microsoft Executive Vice President and Chief Commercial Officer Judson Althoff at the US company’s headquarters in Washington last month. The meeting's purpose was to strengthen the two sides’ strategic cooperation in the fields of artificial intelligence and cloud-based digital transformation. Based on the close partnership, LG CNS looks to pursue multifaceted cooperation with Microsoft Korea. The company will design and build security architecture for generative artificial intelligence based on Azure OpenAI services such as ChatGPT. It will also develop a customized managed detection and response platform using Microsoft solutions to analyze and detect threat factors in advance and respond to them, while making aggressive inroads into the cloud security market, LG CNS said. Bae Min, vice president and head of the security and solution business division at LG CNS, said that the company has provided world-class services through its excellent human resources and a comprehensive security system by operating a cyber-control center. Especially since the beginning of this year, LG CNS has increasingly focused on strengthening its security and solutions business. “Through cooperation with Microsoft, we will further advance our security business and solidify our position as a ‘leading security company’ that innovates customer experience,” Bae said.",
        "publish_date": "2023-05-03 08:32:59"
    },
    {
        "title": "\"He Knows What He's Talking About\": Elon Musk On AI 'Godfather' And His Warning",
        "text": "Tesla owner and billionaire Elon Musk has backed Geoffrey Hinton, known as the 'Godfather of AI'. In a tweet on Tuesday, Mr Musk said the 75-year-old, who has warned about AI chatbots, \"knows what he is talking about\". Mr Hinton, who quit Google recently, told the New York Times (NYT) that he did this to speak freely about the technology's dangers, after realising that computers could become smarter than people far sooner than he and other experts had expected. Mr Hinton nurtured the technology for decades before realising that it will cause serious harm. \"Hinton knows what he's talking about,\" Mr Musk said in his tweet, in response to a news article on the topic. Hinton knows what he's talking about— Elon Musk (@elonmusk) May 2, 2023 In the interview with NYT, Mr Hinton said he was worried about AI's capacity to create convincing false images and texts, creating a world where people will \"not be able to know what is true anymore\". \"It is hard to see how you can prevent the bad actors from using it for bad things,\" he said. There are fears that the technology could quickly displace workers, and become a greater danger as it learns new behaviours. \"The idea that this stuff could actually get smarter than people - a few people believed that,\" said Mr Hinton. \"But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\" Mr Musk too has been expressing concern about the rapidly evolving artificial intelligence systems like ChatGPT. Reacting to a conversation a reporter had with the chatbot, in which he called itself \"perfect\", the SpaceX CEO said it \"sounds eerily like the AI in System Shock that goes haywire and kills everyone\". AI was already gaining prominence, touching the everyday lives of several people after being deployed by companies like e-commerce platforms. But after the launch of OpenAI's ChatGPT, there has been a huge increase in human interaction, with users asking it to write essays, speeches and even solve exam questions.",
        "publish_date": "2023-05-03 09:14:59"
    },
    {
        "title": "A CEO Is Spending Rs 2 Lakh A Month On ChatGPT For All His Employees",
        "text": "Artificial intelligence has been rapidly evolving over the last few months, especially since the launch of ChatGPT. OpenAI's chatbot is being widely used with users asking it to write essays, speeches and even solve exam questions. There are fears that AI will take away millions of jobs and many tech entrepreneurs, including Elon Musk, have raised a voice against its spread. But Akash Nigam, the CEO of start-up Genies, has opted to spend $2,400 (Rs 1.96 lakh) a month on ChatGPT Plus subscriptions for every employee at the company, according to Insider. Since March, Mr Nigam has pushed all the departments of his company -Engineering, Product, Finance, Design, R&D and Accounting - to familiarise themselves with ChatGPT to increase productivity, the outlet further said. \"I'm a pretty frugal, stingy person. But in my mind, this is for the health and growth of the company,\" he told Insider. Mr Nigam said he has already seen many tasks get accelerated after all 120 employees of his company started using the AI system. Genies' various departments use ChatGPT to make day-to-day tasks easy. The R&D group, for example, has used the AI-powered chatbot to solve math and coding problems and create presentation scripts. The staff members have also used it to brainstorm ideas for projects, draft legal documents like corporate regulations, and research technological issues. Genies has also been organising unofficial workshops where employees who are pro in using ChatGPT teach their less experienced colleagues how to use AI to complete various tasks. While there are concerns around the rapid spread of AI, a recent research funded by Stanford University and Massachusetts Institute of Technology found that 14 per cent of employees who used ChatGPT in their workflow saw an increase in productivity. The study also found that the least experienced and least skilled workers were able to complete tasks 35 per cent faster.",
        "publish_date": "2023-05-03 08:21:00"
    },
    {
        "title": "AI驚動白宮！美國出手召OpenAI、Google、微軟開「國安會議」",
        "text": "即時 熱門 政治 軍武 社會 生活 健康 國際 地方 蒐奇 影音 財經 娛樂 汽車 時尚 體育 3 C 評論 藝文 玩咖 食譜 地產 專區 TAIPEI TIMES 求職 爆 Search 自由電子報 3C科技 3C首頁 影音專區 智慧手機 實用秘技 科技趣聞 網路社群 好攝相機 電腦應用 家電娛樂 粉絲團 自由影音 即時 熱門 政治 軍武 社會 生活 健康 國際 地方 蒐奇 財經 娛樂 藝文 汽車 時尚 體育 3 C 評論 玩咖 食譜 地產 專區 服務 自由電子報APP 自由電子報粉絲團 自由電子報Line 自由電子報Twitter 熱門新訊 已經加好友了，謝謝 歡迎加入【自由3C科技】 按個讚　心情好 已經按讚了，謝謝。 3C 〉 產業動態 〉 產業新聞 AI驚動白宮！美國出手召OpenAI、Google、微軟開「國安會議」 2023/05/03 15:19 文／記者吳佩樺 AI發展對社會的衝擊，也讓美政府相當重視。（圖／美聯社） 科技界大打AI戰，生成式AI在全球掀起熱潮，但也引發相當大的爭議，除了假新聞、資安等，還有失業問題更是讓人擔憂，路透社報導，美國白宮已經出手召集各大科技巨頭召開AI會議。 美國將在本週四(美國時間5月4日)集結OpenAI、Google、微軟、Anthropic等公司CEO，一同參與AI會議，該會議將由美國副總統賀錦麗、白宮幕僚長Jeff Zients以及政府高層跟各大CEO會面。 請繼續往下閱讀... 雖然沒有明確指出，這場AI會議的主軸為何，但外界認為將圍繞在AI可能帶來的社會衝擊。據悉，美總統拜登對科技公司的期望是「向大眾提供產品之前，必須確保產品是安全的。」 此外，被譽為「AI教父」的Geoffrey Hinton也投下震憾彈，在近日辭去Google 副總裁的職務，原因是擔憂AI發展恐對未來人類帶來不利影響。 《你可能還想看》 Google「教父級」AI 大神震撼離職 ！擔憂：人工智慧會比人類更聰明 不用抽 不用搶 現在用APP看新聞 保證天天中獎　 點我下載APP　 按我看活動辦法 AI 白宮 美國 OpenAI Google 微軟 國安 會議 產業新聞 相關新聞 Google Pixel 7a官方宣傳素材大量流出！一張表看懂5大升級 Google新代中階Pixel 7預計在下週的Google I／O大會上發表，外觀和主要規格已經曝光的差不多，現在就連官...... 蘋果、Google 罕見聯手「訂規則」！原因恐是 AirTag 掀起的隱私爭議 蘋果、Google 罕見聯手！而最大原因指向蘋果防丟器 AirTag，在業界吹起浪潮後，卻屢次遭到有心人士利用在追蹤、偷...... 果粉聽了森77！iPhone遭中國手機CEO狠批三大缺點 從華為脫離的榮耀Honor，逐漸成為中國手機品牌的新標竿，據中國媒體報導，在日前舉行的Honor深圳研發實驗室開放日，H...... 熱門文章 Sony失守、小米跌出榜外！台灣手機十大品牌排行榜單「黑馬」竄出 徠卡也救不了小米！全球5大手機品牌最新出貨量揭曉 比手機更便宜！華碩遊戲掌機 ROG Ally「平價款售價」曝光　玩家嗨爆 下月發新旗艦！Sony集團社長卻「悲觀」看全球手機市場 HTC 首度升級 1 億畫素！未上市「中高階新機」真實照片現身 HTC 未上市「中高階新機」首次現身跑分平台！跑分成績、型號皆曝光 看更多！加入3C科技粉絲團 網友回應",
        "publish_date": "2023-05-03 09:19:59"
    }
]