[
  {
    "title": "What Exactly Are the Dangers Posed by A.I.?",
    "text": "A neural network is a mathematical system that learns skills by analyzing data. About five years ago, companies like Google, Microsoft and OpenAI began building neural networks that learned from huge amounts of digital text called large language models, or L.L.M.s. By pinpointing patterns in that text, L.L.M.s learn to generate text on their own, including blog posts, poems and computer programs. They can even carry on a conversation. This technology can help computer programmers, writers and other workers generate ideas and do things more quickly. But Dr. Bengio and other experts also warned that L.L.M.s can learn unwanted and unexpected behaviors. These systems can generate untruthful, biased and otherwise toxic information. Systems like GPT-4 get facts wrong and make up information, a phenomenon called \u201Challucination.\u201D Companies are working on these problems. But experts like Dr. Bengio worry that as researchers make these systems more powerful, they will introduce new risks.",
    "publish_date": "2023-05-01 13:12:57"
  },
  {
    "title": "\u2018The Godfather of AI\u2019 Leaves Google and Warns of Danger Ahead",
    "text": "Written by Cade Metz Geoffrey Hinton was an artificial intelligence pioneer. In 2012, Hinton and two of his graduate students at the University of Toronto created technology that became the intellectual foundation for the AI systems that the tech industry\u2019s biggest companies believe is a key to their future. On Monday, however, he officially joined a growing chorus of critics who say those companies are racing toward danger with their aggressive campaign to create products based on generative AI, the technology that powers popular chatbots like ChatGPT. Hinton said he has quit his job at Google, where he has worked for more than decade and became one of the most respected voices in the field, so he can freely speak out about the risks of AI. A part of him, he said, now regrets his life\u2019s work. \u201CI console myself with the normal excuse: If I hadn\u2019t done it, somebody else would have,\u201D Hinton said during a lengthy interview last week in the dining room of his home in Toronto, a short walk from where he and his students made their breakthrough. Hinton\u2019s journey from AI groundbreaker to doomsayer marks a remarkable moment for the technology industry at perhaps its most important inflection point in decades. Industry leaders believe the new AI systems could be as important as the introduction of the web browser in the early 1990s and could lead to breakthroughs in areas ranging from drug research to education. But gnawing at many industry insiders is a fear that they are releasing something dangerous into the wild. Generative AI can already be a tool for misinformation. Soon, it could be a risk to jobs. Somewhere down the line, tech\u2019s biggest worriers say, it could be a risk to humanity. \u201CIt is hard to see how you can prevent the bad actors from using it for bad things,\u201D Hinton said. After the San Francisco startup OpenAI released a new version of ChatGPT in March, more than 1,000 technology leaders and researchers signed an open letter calling for a six-month moratorium on the development of new systems because AI technologies pose \u201Cprofound risks to society and humanity.\u201D Several days later, 19 current and former leaders of the Association for the Advancement of Artificial Intelligence, a 40-year-old academic society, released their own letter warning of the risks of AI. That group included Eric Horvitz, chief scientific officer at Microsoft, which has deployed OpenAI\u2019s technology across a wide range of products, including its Bing search engine. Hinton, often called \u201Cthe Godfather of AI,\u201D did not sign either of those letters and said he did not want to publicly criticize Google or other companies until he had quit his job. He notified the company last month that he was resigning, and Thursday, he talked by phone with Sundar Pichai, CEO of Google\u2019s parent company, Alphabet. He declined to publicly discuss the details of his conversation with Pichai. Google\u2019s chief scientist, Jeff Dean, said in a statement: \u201CWe remain committed to a responsible approach to AI. We\u2019re continually learning to understand emerging risks while also innovating boldly.\u201D Hinton, a 75-year-old British expatriate, is a lifelong academic whose career was driven by his personal convictions about the development and use of AI. In 1972, as a graduate student at the University of Edinburgh, Hinton embraced an idea called a neural network. A neural network is a mathematical system that learns skills by analyzing data. At the time, few researchers believed in the idea. But it became his life\u2019s work. In the 1980s, Hinton was a professor of computer science at Carnegie Mellon University but left the university for Canada because he said he was reluctant to take Pentagon funding. At the time, most AI research in the United States was funded by the Defense Department. Hinton is deeply opposed to the use of AI on the battlefield \u2014 what he calls \u201Crobot soldiers.\u201D In 2012, Hinton and two of his students in Toronto, Ilya Sutskever and Alex Krishevsky, built a neural network that could analyze thousands of photos and teach itself to identify common objects, such as flowers, dogs and cars. Google spent $44 million to acquire a company started by Hinton and his two students. And their system led to the creation of increasingly powerful technologies, including new chatbots such as ChatGPT and Google Bard. Sutskever went on to become chief scientist at OpenAI. In 2018, Hinton and two other longtime collaborators received the Turing Award, often called \u201Cthe Nobel Prize of computing,\u201D for their work on neural networks. Around the same time, Google, OpenAI and other companies began building neural networks that learned from huge amounts of digital text. Hinton thought it was a powerful way for machines to understand and generate language, but it was inferior to the way humans handled language. Then, last year, as Google and OpenAI built systems using much larger amounts of data, his view changed. He still believed the systems were inferior to the human brain in some ways but he thought they were eclipsing human intelligence in others. \u201CMaybe what is going on in these systems,\u201D he said, \u201Cis actually a lot better than what is going on in the brain.\u201D As companies improve their AI systems, he believes, they become increasingly dangerous. \u201CLook at how it was five years ago and how it is now,\u201D he said of AI technology. \u201CTake the difference and propagate it forwards. That\u2019s scary.\u201D Until last year, he said, Google acted as a \u201Cproper steward\u201D for the technology, careful not to release something that might cause harm. But now that Microsoft has augmented its Bing search engine with a chatbot \u2014 challenging Google\u2019s core business \u2014 Google is racing to deploy the same kind of technology. The tech giants are locked in a competition that might be impossible to stop, Hinton said. His immediate concern is that the internet will be flooded with false photos, videos and text, and the average person will \u201Cnot be able to know what is true anymore.\u201D He is also worried that AI technologies will in time upend the job market. Today, chatbots such as ChatGPT tend to complement human workers, but they could replace paralegals, personal assistants, translators and others who handle rote tasks. \u201CIt takes away the drudge work,\u201D he said. \u201CIt might take away more than that.\u201D Down the road, he is worried that future versions of the technology pose a threat to humanity because they often learn unexpected behavior from the vast amounts of data they analyze. This becomes an issue, he said, as individuals and companies allow AI systems not only to generate their own computer code but actually to run that code on their own. And he fears a day when truly autonomous weapons \u2014 those killer robots \u2014 become reality. \u201CThe idea that this stuff could actually get smarter than people \u2014 a few people believed that,\u201D he said. \u201CBut most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\u201D Many other experts, including many of his students and colleagues, say this threat is hypothetical. But Hinton believes that the race between Google and Microsoft and others will escalate into a global race that will not stop without some sort of global regulation. But that may be impossible, he said. Unlike with nuclear weapons, he said, there is no way of knowing whether companies or countries are working on the technology in secret. The best hope is for the world\u2019s leading scientists to collaborate on ways of controlling the technology. \u201CI don\u2019t think they should scale this up more until they have understood whether they can control it,\u201D he said. Hinton said that when people used to ask him how he could work on technology that was potentially dangerous, he would paraphrase Robert Oppenheimer, who led the U.S. effort to build the atomic bomb: \u201CWhen you see something that is technically sweet, you go ahead and do it.\u201D He does not say that anymore.",
    "publish_date": "2023-05-01 14:22:59"
  },
  {
    "title": "ChatGPT fails: 13 common errors and mistakes you need to know",
    "text": "I\u2019ve written about ways to use ChatGPT for SEO and content marketing-related tasks, like keyword research, crafting title tags and local SEO. I use it at least once daily for a variety of tasks. The more you use the platform, the more valuable it will be to you to understand its limitations. The founder of OpenAI (the company that built ChatGPT) has called the tool \u201Cincredibly limited\u201D and a \u201Chorrible product\u201D and has cautioned users about how much they rely on it: Multiple publications have published content with errors on meaningful YMYL (\u201CYour Money or Your Life\u201D) topics like personal finance and health. This article discusses the most common technical errors and the frequent mistakes when using ChatGPT and how to address them. Common ChatGPT errors If you\u2019ve been using ChatGPT for a \u201Clong\u201D time (i.e., since late 2022), you\u2019ve probably seen the prompt below at some point. Obviously, a new software product in an emerging technology field that\u2019s massively popular will run into problems. If you\u2019re making ChatGPT an integral part of your daily workflow, knowing the common issues you may encounter is helpful. ChatGPT also has issues familiar to any website, such as 404 not found errors, 5xx internal server errors, 403 access denied errors, and Cloudflare errors like 1020 and 524 errors. Often these can be fixed by clearing your cache, using (or turning off) a VPN, or ensuring you\u2019re typing the web address correctly. And there are also several errors more specific to the platform: 1. API issues Often, users may hit specific character limits, token limits, or limits in functionality with the ChatGPT API. 2. Character limits There are limits to how many characters you can enter into ChatGPT and how many characters ChatGPT responses can be. They vary depending on your version (e.g., GPT-3.5 has an input limit of 2048, and GPT-4 is 4,096) and in the web interface versus the API. 3. Connection time out on long responses If ChatGPT is processing a long and complex response, it will sometimes just time out. You might see this when asking the platform to perform functions like writing code or analyzing and responding to long inputs. 4. Unfinished responses Not an error per se, but frequently, if you ask ChatGPT to write code or a long text, it will just stop mid-way. You can type continue, and it will finish within the next few prompts in these instances. 5. Limited messages ChatGPT, at the time of writing, limits the number of messages you can use per time period, even for paid accounts. Similarly, there is specific rate-limiting for different GPT versions, OpenAI tools, and account types. 6. Login issues Depending on the level of access required or session duration. ChatGPT may log you out, or you may not have access to certain features with certain account statuses. Most of these issues can be resolved by: Common ChatGPT mistakes ChatGPT makes many mistakes, including: Other users have detailed instances of the platform lying to get a CAPTCHA solved and issues with simple math problems related to age and distance. Some of this has improved with GPT-4, but not all. Here is a prompt and output using the latest version of ChatGPT: While it\u2019s fun to do \u201Cgotcha\u201D prompts and catch ChatGPT saying incorrect or goofy things, it\u2019s more beneficial to be aware of areas where ChatGPT can be wrong \u2013 the \u201Ccategories of wrongness,\u201D if you will. Understanding these will make you both a better prompt engineer and output editor. Let\u0027s walk through the most common categories of mistakes I know. 7. Confident but wrong The \u201Challucination\u201D issue is a significant problem for ChatGPT and other generative AI solutions. ChatGPT will answer all kinds of questions that it doesn\u0027t know the answer to in a compelling way. That doesn\u0027t make the answer right. The example screenshot above shows that it doesn\u0027t use any qualifying language or hedge at all. You get a decisive answer without equivocating. 8. Not understanding word and character counts ChatGPT is terrible at estimating and understanding character counts in any precise way. If you\u2019re doing work related to SEO and content, this is a major limiting factor. You might be looking to write title tags, meta descriptions, or content where meeting a word count is necessary: This is actually pretty close. I\u0027ve had instances where the tool was much farther off. 9. Mistakes in reasoning I searched for \u201Clogic puzzles\u201D and grabbed the first list of \u201Ceasy logic puzzles\u201D I found. ChatGPT got the first three puzzles correct, then the fourth one wrong: 10. Coding errors There are several real-life examples now of people using ChatGPT to build websites, apps and browser extensions or troubleshoot coding issues. That said, ChatGPT can also generate code that doesn\u0027t work. I\u0027ve encountered the following issues when using ChatGPT to help code: Again, it\u2019s imperative you always check the code you\u2019re deploying carefully and test the output of anything ChatGPT helps with. Some users have also reported that GPT- 4 or even legacy mode (rather than the new default, Turbo) leads to better results for coding tasks. 11. Outdated information The training data for ChatGPT may be updated at some point and various plugins and tools have started to integrate web crawling and real-time data. Still, the web version of ChatGPT currently can\u0027t crawl the web and isn\u2019t aware of information from before late 2021. This is important, particularly when coupled with the \u201Cconfident but wrong\u201D issue. 12. Short memory There is a limit to how many characters within a chat ChatGPT will \u201Cremember\u201D so it will frequently make mistakes due to losing the context of your early prompts or its early answers. 13. Formatting mistakes As with coding, if you ask ChatGPT to perform functions like creating a table or formatting text in a certain way, it will sometimes make mistakes or ignore specific instructions. Avoiding and addressing ChatGPT\u0027s mistakes So how can you get the most out of ChatGPT while dealing with the platform\u2019s issues and limitations? Focusing on these key areas can help you avoid much of the potential downside of using ChatGPT in your day-to-day tasks: Be aware of limitations Staying up to date with specific product limitations like (i.e., character limits for prompts and responses, the date of training data for GPT, etc.) will help you craft prompts that generate good responses and QA outputs better. OpenAI does a good job of updating this information with new releases. Some items here are things they\u0027ve explicitly said they are addressing. Review and edit outputs carefully This should be fairly obvious, but you must carefully edit any text and QA code before publishing or deploying it. Due to how convincingly confident ChatGPT can be, I\u0027d strongly recommend having a domain expert review content and code before it\u2019s deployed. It\u2019s relatively easy to be fooled by some of the content and assertions put out by ChatGPT, especially if a topic isn\u2019t your area of expertise. But if you have deep experience on a topic, it\u0027s easier to catch misleading or inaccurate statements. Layer tools and processes on top of the API OpenAI has rolled out plugins and made a ChatGPT-specific API available. You can address some of the platform\u2019s limitations by using different tools that others create or by creating some yourself. Now that you know the potential pitfalls of ChatGPT, you can go on and establish guardrails for using the tool \u2013 leveraging its benefits while avoiding many of the negatives.",
    "publish_date": "2023-05-01 15:00:17"
  },
  {
    "title": "Here\u0027s Why The \u0027Godfather of AI\u0027 Just Quit His Job at Google",
    "text": "Geoffrey Hinton, a pioneer of artificial intelligence (A.I.), is raising alarm bells about the danger of A.I. development. Dr. Hinton and his students at the University of Toronto created the technology that became the foundation of A.I. systems in 2012, which tech giants are now racing to develop.However, Dr. Hinton has recently joined the growing chorus of critics warning that companies are creating products based on generative A.I. without fully considering the risks. Generative A.I., which powers popular chatbots like ChatGPT, can already be a tool for misinformation and, somewhere down the line, could be a risk to humanity.Can ChatGPT Be Your Emergency Therapist For A Day? After OpenAI released a new version of ChatGPT in March, over 1,000 technology leaders and researchers signed an open letter calling for a six-month moratorium on the development of new systems. This is because A.I. technologies pose \u0022profound risks to society and humanity.\u0022Dr. Hinton, often called \u0022the Godfather of A.I.\u0022, did not sign either of those letters but has since resigned from his position at Google, where he has worked for over a decade, to speak freely about the risks of A.I. Dr. Hinton is a lifelong academic and his career was driven by his personal convictions about the development and use of A.I.Dr. Hinton\u0027s neural network, a mathematical system that learns skills by analyzing data, became his life\u0027s work. In 2012, he and his students built a neural network that could analyze thousands of photos and teach itself to identify common objects like flowers, dogs, and cars. Google acquired the company for $44 million and their system led to the creation of powerful technologies, including chatbots like ChatGPT and Google Bard.Here\u0027s How Artificial Intelligence Wrote \u0026 Directed This Creepy Short Film Dr. Hinton is deeply opposed to the use of artificial intelligence on the battlefield or \u0022robot soldiers\u0022. He believes that generative A.I. could have negative consequences if not developed responsibly. He says, \u0022It is hard to see how you can prevent the bad actors from using it for bad things.\u0022While industry leaders believe that A.I. systems could lead to breakthroughs in areas ranging from drug research to education, critics fear that they are releasing something dangerous into the wild. With the A.I. industry at perhaps its most important inflection point in decades, the debate about its risks and benefits is sure to continue.(At The Quint, we are answerable only to our audience. Play an active role in shaping our journalism by becoming a member. Because the truth is worth it.)",
    "publish_date": "2023-05-01 16:07:51"
  },
  {
    "title": "After Quitting Google, \u2018Godfather of AI\u2019 Is Now Warning of Its Dangers",
    "text": "Megalithic tech companies such as Google, Meta, and Microsoft are so obsessed with AI development it seems impossible to steer any of them toward slowing down and actually thinking about the repercussions. Now one of the most prominent faces in artificially intelligence research, former Googler Dr. Geoffrey Hinton, has come down hard on the full-spring pace of AI development, ultimately calling for some kind of global regulation. According to an interview with The New York Times, Hinton, an award-winning researcher on AI, neural networks, and machine learning, is no longer so comfortable pushing the boundaries of AI development without any kind of regulation or stopgap. The 75-year-old Hinton, who was a lead researcher in any aspects of AI development at Google, has come out saying \u201CIt is hard to see how you can prevent the bad actors from using [AI] for bad things.\u201D He directly compared himself to Robert Oppenheimer, who helped develop the atomic bomb for the U.S. While Oppenheimer had made statements about pursuing science for sciences sake, Hinton instead said \u201CI don\u2019t think they should scale [AI] up more until they have understood whether they can control it.\u201D He further shared his concerns that AI would lead to massive job disruptions around the world. Hinton got his \u2018Godfather\u2019 title not with any offer you can\u2019t refuse, but from decades of research on AI. This came to a head with the neural network he helped build in 2012 with two of his students at the University of Toronto. That network was a machine learning program that could teach itself to identify objects like dogs, flowers, and so on, and it became a major stepping stone for modern transformer-based AI like diffusion AI image generators and large language models. Google had originally acquired the company formed out of Hinton\u2019s Toronto-based research in 2013. This let him establish a Toronto-based element of the Google Brain team overseeing AI development. After that, Google went on an AI spending spree when it acquired deep learning company DeepMind in 2014. Hinton\u2019s company, according to a 2021 Wired report, received numerous offers from tech giants including Microsoft and China-based Baidu, both of which are deep in the muck with their own push into AI development. In a March interview with CBS News, Hinton compared the recent rapid advancements in AI to \u201Cthe Industrial revolution or electricity\u2014or maybe the wheel.\u201D It\u2019s unclear when Hinton made this heel-turn, but just a few months ago he was instead referring to AI as a \u201Csupernaturally precocious child.\u201D He compared AI training to caterpillars feeding on nutrients to become butterflies, further calling OpenAI\u2019s GPT-4 large language model \u201Chumanity\u2019s butterfly.\u201D According to the Times, in April Hinton told Google he planned to leave, and finally cut the cord after a call with CEO Sundar Pichai last Thursday. Though The New York Times implied that Hinton had left Google in order to specifically take umbrage with his old boss, the Turing Award winner claimed he only wished to speak up on the dangers of AI, adding \u201CGoogle has acted very responsibly.\u201D Hinton\u2019s departure comes at a time of massive reorganization at his former company after massive layoffs. Last month, Google announced it was consolidating two of its most major AI teams together. Combining the Google Brain and DeepMind teams into one unit and also reorganized its AI leadership, with Brain lead Jeff Dean being moved to a chief scientist position while DeepMind CEO Demis Hassabis is set to take control of all AI development. So far, the overt calls for stalling AI development have come from outside big tech. In March, hundreds of leading minds and researchers circulated an open letter demanding companies pause advanced AI systems. The letter criticized how major tech companies were locked in an \u201Cout-of-control race to develop and deploy ever more powerful digital minds\u201D that nobody could predict or control. Though that\u2019s not to say folks inside these companies don\u2019t have qualms. A recent report from Bloomberg claimed that people inside Google were especially concerned with the company\u2019s Bard AI. Staff said the chatbot was so bad it was constantly providing misinformation and lies to users. Want to know more about AI, chatbots, and the future of machine learning? Check out our full coverage of artificial intelligence, or browse our guides to The Best Free AI Art Generators, The Best ChatGPT Alternatives, and Everything We Know About OpenAI\u2019s ChatGPT.",
    "publish_date": "2023-05-01 16:00:11"
  },
  {
    "title": "\u0027Godfather of AI\u0027 quits Google -- and says he regrets life\u0027s work due to risks to humanity",
    "text": "A prominent artificial intelligence researcher known as the \u201CGodfather of AI\u201D has quit his job at Google \u2013 and says he now partly regrets his work advancing the burgeoning technology because of the risks it poses to society. Dr. Geoffrey Hinton is a renowned computer scientist who is widely credited with laying the AI groundwork that eventually led to the creation of popular chatbots such as OpenAI\u2019s ChatGPT and other advanced systems. The 75-year-old told the New York Times that he left Google so that he can speak openly about the risks of unrestrained AI development \u2013 including the spread of misinformation, upheaval in the jobs market and other, more nefarious possibilities. \u201CI console myself with the normal excuse: If I hadn\u2019t done it, somebody else would have,\u201D Hinton said in an interview published on Monday. \u201CLook at how it was five years ago and how it is now,\u201D Hinton added later in the interview. \u201CTake the difference and propagate it forwards. That\u2019s scary.\u201D Hinton fears that AI will only become more dangerous in the future \u2014 with \u201Cbad actors\u201D potentially exploiting advanced systems \u201Cfor bad things\u201D that will be difficult to prevent. Hinton informed Google of his plans to resign last month and personally spoke last Thursday with company CEO Sundar Pichai, according to the report. The computer scientist did not reveal what he and Pichai discussed during the phone call. Google\u2019s chief scientist Jeff Dean defended the company\u2019s AI efforts. \u201CWe remain committed to a responsible approach to A.I. We\u2019re continually learning to understand emerging risks while also innovating boldly,\u201D Dean said in a statement. The Post has reached out to Google for further comment. Hinton is the latest of a growing number of experts who have warned that AI could cause significant harm without proper oversight and regulation. In March, Elon Musk and more than 1,000 other prominent figures in the AI sector called for a six-month pause in advanced AI development, citing its potential \u201Cprofound risks to society and humanity.\u201D In the interview, Hinton expressed concern that artificial intelligence has already begun to outpace the human mind in some facets. He also cited concerns that the pace of AI development will increase as Microsoft-backed OpenAI, Google and other tech giants race to lead the field \u2013 with potentially dangerous consequences. Hinton fears that advanced AI could eventually spiral out of control as systems gain the ability to create and run their own computer code \u2013 or even power weapons without human control. \u201CThe idea that this stuff could actually get smarter than people \u2014 a few people believed that,\u201D Hinton added. \u201CBut most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\u201D In a recent interview with CBS\u2019s \u201C60 Minutes,\u201D Pichai himself warned that AI would cause job losses for \u201Cknowledge workers,\u201D such as writers, accountants, architects and software engineers. Pichai also detailed bizarre scenarios in which Google\u2019s AI programs have developed \u201Cemergent properties\u201D \u2013 or learned unexpected skills in which they were not trained. Since 2013, Hinton had split his time between roles as a professor at the University of Toronto and as a Google engineering fellow. He had worked for the tech giant since Google acquired a startup he co-founded with two students, Alex Krishevsky and Ilya Sutskever. The trio developed a neural network that trained itself to identify common objects, such as cars or animals, by analyzing thousands of photos. Sutskever currently serves as chief scientist for OpenAI. In 2018, Hinton was a joint recipient of the Turing Award \u2013 often identified as the computing world\u2019s equivalent of the Nobel Prize \u2013 for work on neural networks that was described as \u201Cmajor breakthroughs in artificial intelligence.\u201D A lengthy bio for Hinton on Google\u2019s website lauds his accomplishments \u2013 noting he \u201Cmade major breakthroughs in deep learning that have revolutionized speech recognition and object classification.\u201D",
    "publish_date": "2023-05-01 16:24:15"
  },
  {
    "title": "As Europeans strike first to rein in AI, the US follows",
    "text": "A proposed set of rules by the European Union would, among other things. require makers of generative AI tools such as ChatGPT,to publicize any copyrighted material used by the technology platforms to create content of any kind. A new draft of European Parliament\u0027s legislation, a copy of which was attained by The Wall Street Journal, would allow the original creators of content used by generative AI applications to share in any profits that result. The European Union\u2019s \u201CArtificial Intelligence Act\u201D (AI Act) is the first of its kind by a western set of nations. The proposed legislation relies heavily on existing rules, such as the General Data Protection Regulation (GDPR), the Digital Services Act, and the Digital Markets Act. The AI Act was originally proposed by the European Commission in April 2021. The bill\u2019s provisions also require that the large language models (LLMs) behind generative AI tech, such as the GPT-4, be designed with adequate safeguards against generating content that violates EU laws; that could include child pornography or, in some EU countries, denial of the Holocaust, according to The Washington Post. Violations of the AI Act could carry fines of up to 30 million euros or 6% of global profits, whichever is higher. \u201CFor a company like Microsoft, which is backing ChatGPT creator OpenAI, it could mean a fine of over $10 billion if found violating the rules,\u201D a Reuters report said. But the solution to keeping AI honest isn\u0027t easy, according to Avivah Litan, a vice president and distinguished analyst at Gartner Research. It\u2019s likely that LLM creators, such as San Fransisco-based OpenAI and others, will need to develop powerful LLMs to check that the ones trained initially have no copyrighted materials. Rules-based systems to filter out copyright materials are likely to be ineffective, Liten said. Meanwhile, the EU is busy refining its AI Act and taking a world-leading approach, Litan said, in creating rules that govern the fair and risk-managed use of AI going forward. Regulators should consider that LLMs are effectively operating as a black box, she said, and it\u0027s unlikely that the algorithms will provide organizations with the needed transparency to conduct the requisite privacy impact assessment. \u0022This must be addressed,\u0022 Litan said. \u0022It\u2019s interesting to note that at one point the AI Act was going to exclude oversight of Generative AI models, but they were included later,\u201D Litan said \u201CRegulators generally want to move carefully and methodically so that they don\u2019t stifle innovation and so that they create long-lasting rules that help achieve the goals of protecting societies without being overly prescriptive in the means.\u201D On April 1, Italy became the first Western nation to ban further development of ChatGPT over privacy concerns; that occurred after the natural language processing app experienced a data breach involving user conversations and payment information. ChatGPT is the popular chatbot created by OpenAI and backed by billions of dollars from Microsoft. Earlier this month, the US and Chinese governments issued announcements related to regulations for AI development, something neither country has established to date. \u201CThe US and the EU are aligned in concepts when it comes to wanting to achieve trustworthy, transparent, and fair AI, but their approaches have been very different,\u201D Litan said. So far, the US has taken what Litan called a \u201Cvery distributed approach to AI risk management,\u201D and it has yet to create new regulations or regulatory infrastructure. The US has focused on guidelines and an AI Risk Management framework. In January, the National Institute of Standards and Technology (NIST) released the Artificial Intelligence Management Framework. In February, the White House issued an Executive Order directing federal agencies to ensure their use of AI advances equity and civil rights. The US Congress is considering the federal Algorithmic Accountability Act, which, if passed, would require employers to perform an impact assessment of any automated decision-making system that has a significant effect on an individual\u0027s access to, terms, or availability of employment. The National Telecommunications and Information Administration (NTIA), a branch of the US Department of Commerce, also issued a public request for comment on what policies would best hold AI systems accountable. States and municipalities are getting into the act, too, eyeing local restrictions on the use of AI-based bots to find, screen, interview, and hire job candidates because of privacy and bias issues. Some states have already put laws on the books. Microsoft and Google owner Alphabet have been in a race to bring generative AI chatbots to businesses and consumers. The most advanced generative AI engines can create their own content based on user prompts or input. So, for example, AI can be tasked with creating marketing or ad campaigns, writing essays, and generating realistic photo imagery and videos. Key to the EU\u2019s AI Act is a classification system that determines the level of risk an AI technology could pose to the health and safety or fundamental rights of a person. The framework includes four risk tiers: unacceptable, high, limited, and minimal, according to the World Economic Forum. Issues around generative AI platforms that regulators should be mindful of, according to Gartner, include: GPT models are not explainable: Model outputs are unpredictable; even the model vendors don\u2019t understand everything about how they work internally. Explainability or interpretability are prerequisites for model transparency. Inaccurate and fabricated answers: To mitigate the risks of inaccuracies and hallucinations, output generated by ChatGPT/GenAI should be assessed for accuracy, appropriateness, and actual usefulness before being accepted. Potential compromise of confidential data: No verifiable data governance and protection assurances that confidential enterprise information\u2013 for example, in the form of stored prompts \u2014 is not compromised. Model and output bias: Model developers and users must have policies or controls in place to detect biased outputs and deal with them consistent with company policy and any relevant legal requirements. Intellectual property (IP) and copyright risks: Model developers and users must scrutinize their output before further use to ensure it doesn\u2019t infringe on copyright or IP rights, and actively monitor changes in copyright laws that apply to ChatGPT/GenAI. Users are now on their own when it comes to filtering out copyrighted materials in ChatGPT outputs. Cyber and fraud risks: Systems should be hardened to try to ensure criminals are not able to use them for cyber and fraud attacks. Launched by OpenAI in November, ChatGPT immediately went viral and had 1 million users in just its first five days because of the sophisticated way it generates in-depth, human-like responses to queries. The ChatGPT website currently receives an estimated 1 billion monthly website visitors with an estimated 100 million active users, according to website test company Tooltester. Though the chatbot\u2019s responses may appear human-like, ChatGPT isn\u0027t sentient \u2014 it\u2019s a next-word prediction engine, according Dan Diasio, Ernst \u0026 Young global artificial intelligence consulting leader. With that in mind, he urged caution in its use. But as AI technology advances at breakneck speed, a more sophisticated algorithm is predicted to be on the horizon: artificial general intelligence, which could think for itself and become exponentially smarter over time. Earlier this month, an open letter from thousands of tech luminaries called for a halt to the development of generative AI technology out of concern that the ability to control it could be lost if it advances too far. The letter has garnered more than 27,000 signatories, including Apple co-founder Steve Wozniak. The letter, published by the Future of Life Institute, called out San Francisco-based OpenAI Lab\u2019s recently announced GPT-4 algorithm in particular, saying the company should halt further development until oversight standards are in place. While AI has been around for decades, it has \u201Creached new capacities fueled by computing power,\u201D Thierry Breton, the EU\u2019s Commissioner for Internal Market, said in a statement in 2021. The Artificial Intelligence Act, he said, was created to ensure that \u201CAI in Europe respects our values and rules, and harness the potential of AI for industrial use.\u201D",
    "publish_date": "2023-05-01 12:00:00"
  },
  {
    "title": "ChatGPT is once again available in Italy after meeting demands of regulators",
    "text": "Article content Some Italian users shared what appeared to be screenshots of the changes, including a menu button asking users to confirm their age and links to the updated privacy policy and training data help page. The Garante said in a statement that it \u201Cwelcomes the measures OpenAI implemented\u201D and urged the company to comply with two other demands for an age-verification system and a publicity campaign informing Italians about the backstory and their right to opt out of data processing. The watchdog imposed the ban last month after finding that some users\u2019 messages and payment information were exposed to others. It also questioned whether there was a legal basis for OpenAI to collect massive amounts of data used to train ChatGPT\u2019s algorithms and raised concerns that the system could sometimes generate false information about individuals. Infrastructure Minister Matteo Salvini on Instagram, wrote approvingly of the return of ChatGPT and said that his League party \u201Cis committed to help start-ups and development in Italy.\u201D Other regulators are now looking closer at such AI systems, with France\u2019s data privacy regulator and Canada\u2019s privacy commissioner investigating after receiving complaints about ChatGPT. The head of the Federal Trade Commission, Lina Khan, warned this week that the U.S. government will \u201Cnot hesitate to crack down\u201D on harmful business practices involving artificial intelligence.",
    "publish_date": "2023-05-01 17:52:50"
  },
  {
    "title": "ChatGPT might show more empathy to patients than human doctors: study",
    "text": "Artificial intelligence might be better at humanity than humans themselves. AI assistants could express more sympathy toward patients, a new study suggests. A study published Friday in the journal JAMA Internal Medicine found that OpenAI\u2019s ChatGPT answers patient questions with more compassion than human physicians can. \u201CThe opportunities for improving healthcare with AI are massive,\u201D lead author John W. Ayers, an epidemiologist from the Qualcomm Institute at the University of California San Diego, said in a release. \u201CAI-augmented care is the future of medicine Researchers at the University of California San Diego, La Jolla took 195 patient questions from Reddit\u2019s AskDocs forum \u2014 a social media forum where people publicly post medical questions for doctors to respond to \u2014 and had both human doctors and ChatGPT answer the questions. \u201CChatGPT might be able to pass a medical licensing exam,\u201D study co-author Davey Smith, a physician-scientist, co-director of the UC San Diego Altman Clinical and Translational Research Institute and professor at the UC San Diego School of Medicine, said. \u201CBut directly answering patient questions accurately and empathetically is a different ballgame.\u201D The responses were evaluated by a panel of licensed healthcare professionals that rated each answer by \u201Cthe quality of information provided\u201D \u2014 very poor, poor, acceptable, good or very good \u2014 and \u201Cthe empathy or bedside manner provided\u201D \u2014 not empathetic, slightly empathetic, moderately empathetic, empathetic and very empathetic. ChatGPT won \u2014 and the competition didn\u2019t even come close. \u201CChatGPT messages responded with nuanced and accurate information that often addressed more aspects of the patient\u2019s questions than physician responses,\u201D Jessica Kelley, M.S.N, a nurse practitioner with San Diego firm Human Longevity and study co-author, said. The clinical team chose the computer response over the human response nearly 80% of the time. \u201CIt\u2019s pretty obvious why AI was better. It\u2019s not constrained by time,\u201D Ayers told Axios. \u201CYou could take a simple query like: \u2018I have a headache, can you help me?\u2019 and you\u2019ll immediately see ChatGPT say \u2018I\u2019m sorry you have a headache.\u2019 The doctor knows that, they feel that. They don\u2019t have time to say it.\u201D While artificial intelligence is nowhere near replacing doctors, the findings suggest that bringing ChatGPT and other AI assistants into the healthcare system can help human physicians provide higher quality, more efficient and more empathetic care by improving workflow, removing any health disparities for minorities and affecting a patient\u2019s overall health. \u201CWe could use these technologies to train doctors in patient-centered communication, eliminate health disparities suffered by minority populations who often seek healthcare via messaging, build new medical safety systems, and assist doctors by delivering higher quality and more efficient care,\u201D Mark Dredze, Ph.D., the John C Malone Associate Professor of Computer Science at Johns Hopkins and study co-author, said. Study authors noted that further research would need to be completed in clinical settings, including using chatbots to draft responses that human physicians could edit. Trials could also help determine if AI assistants \u201Cmight improve responses, lower clinician burnout and improve patient outcomes.\u201D \u201CI never imagined saying this, but ChatGPT is a prescription I\u2019d like to give to my inbox. The tool will transform the way I support my patients,\u201D Aaron Goodman, M.D., an associate clinical professor at UC San Diego School of Medicine and study co-author, admitted.",
    "publish_date": "2023-05-01 18:09:36"
  },
  {
    "title": "Generative AI Tools Like ChatGPT And Bard Heralding Generational Shift In Job Roles. Adapt Or Risk",
    "text": "Large Language Models (LLMs), the technologies underpinning chatbots like ChatGPT, are creating cascading waves of emerging paradigm-shifting opportunities by redefining how we accumulate, organize, and disseminate knowledge-based information. Within a week of the ChatGPT launch, one million people had used it; within two months, the number soared to 100 million. The hockey stick adoption rate is being hailed as a game-changing Generative Artificial Intelligence (AI) based productivity enhancement technology. Global competition amongst entrepreneurs and investors is hastily deploying Research and Development (R\u0026D) resources to create cutting-edge AI-based productivity tools for a gamut of industries. Early users of the nascent ChatGPT are amazed at its ability to deliver work products at near-human levels of sophistication. We are just at the tip of the iceberg. The benefits and impact of Generative AI tools like ChatGPT on industries have only begun to occur, harbingering generational job role shifts. By consolidating the knowledge of the collective, these tools can unleash substantial productivity gains and accelerate growth. With the ability to produce content on command, these productivity enhancement tools are expected to deliver work products more quickly and efficiently than ever. The hybrid human/AI mix offers exciting possibilities for the future of work. According to a report by Goldman Sachs, Generative AI could raise global GDP by 7%. While ChatGPT has aroused a lot of excitement, it has also elicited ominous projections of potential job losses; subliminal messaging through biased responses; usage for malicious propaganda and harmful activities; taking over of the human race, and more. However, machines replacing jobs is an age-old fear. New technologies have historically created jobs to replace the ones obsoleted. Unlike prior technology innovations that have typically replaced unskilled labor, Generative AI can also undertake white-collar tasks. While there is no indication of disruption in job markets, it cannot be completely ruled out. By analyzing and interpreting vast repositories of data, AI can provide valuable assistance to individuals working in tech (coders, computer programmers, software engineers, data analysts, etc.), media (advertising, content creation, journalism, entertainment, etc.), legal (paralegals, legal assistants, etc.), market research, medical, education, gig economy/freelancing, liberal arts, STEM (science, technology, engineering, mathematics), gaming, finance, security/defense, and many more. With the help of Generative AI coding assistants, engineers can write code more efficiently, potentially paving the way for a future in which, rather than writing code themselves, software engineers will concentrate on instructing AI tools to write code. The primary focus could morph into qualifying work products produced by machines. These new working models will have applications across many fields. Using Generative AI tools to quickly create a first draft, after that, for a human to further correct, enhance, and deliver the final product in shorter task completion times, can significantly boost worker productivity across many job roles. Task-based programming, writing, and research dominate the freelance gig economy job market. The advent of productivity-enhancing Generative AI tools could have significant implications for the freelancing job market segment. In education, teachers can use Generative AI tools to create customized learning materials. AI tools can analyze data and patterns in healthcare to enhance diagnosis accuracy and help develop targeted drugs and personalized treatment plans. Similarly, in agriculture, the tools can assist farmers in making better decisions about crop planting and resource usage using weather and soil data, resulting in higher yields and operational efficiencies. In manufacturing, data analysis from sensors and machines can optimize production processes by identifying operational inefficiencies, leading to improved production throughputs, product quality, and lower costs. Microsoft MSFT , Google GOOG , and Amazon AMZN are investing billions in AI research to create cutting-edge tools for consumers and enterprises. Microsoft, a partner of OpenAI \u2013 the creator of ChatGPT, has already integrated AI into its MS 365 Suite, Bing search engine, and other product lines. Google is developing its Bard chatbot AI offering. Whereas Amazon is developing AI products for the enterprise market. Tesla TSLA CEO Elon Musk (an ex-founder of OpenAI) plans to launch a new AI start-up. Chinese tech giants Baidu BIDU and Alibaba have also launched their versions of AI technology. ChatGPT has attracted a surge of early adopters keen on keeping up with technological advancements and staying ahead of the curve. Employees at all levels within organizations are exploring various AI tools to automate time-consuming tasks or accelerate the delivery of their work products. Numerous companies have begun integrating ChatGPT-type tools into their product offerings, a few examples of which are as follows: Salesforce is integrating OpenAI\u2019s technology to create its Generative AI-based customer relationship management tools. It will also add a ChatGPT app to Slack for conversation summaries, research tools, and writing assistance. Bain \u0026 Company has integrated OpenAI\u2019s technologies, including ChatGPT, into its management systems, research, and other processes. Coca-Cola will be the first major consumer product company to use the system. Snap has launched a new AI feature called My AI on Snapchat\u002B, which allows its 2.5 million subscribers to ask an AI chatbot for various prompts, such as dinner recipes and weekend trip plans. However, Snapchat has cautioned that the AI tool may be susceptible to providing biased, inaccurate, or misleading information. Quizlet, a website that offers educational tools, has introduced a new feature, Q-Chat, which utilizes OpenAI\u2019s ChatGPT technology to provide one-on-one tutoring services. Instacart, the grocery delivery and pick-up service, will soon launch a new search engine feature called \u201CAsk Instacart,\u201D incorporating ChatGPT into its app. The feature will enable users to ask open-ended food questions and provide personalized answers from Instacart\u2019s vast database of 1.5 million products sold through 75,000 grocery stores. Shopify SHOP is launching a new shopping assistant feature that will use ChatGPT AI technology to help customers with search inquiries and provide personalized recommendations based on their requests. ChatGPT will analyze customer data to offer relevant and customized solutions, improving the overall user experience on the platform. Speak, a language learning app, has partnered with OpenAI. It uses its speech-to-text API Whisper for its new AI-speaking companion product to give users real-time feedback when learning a new language. IBM IBM chatbot, \u201CWatson Assistant,\u201D uses AI and natural language processing to provide customers personalized assistance and support. H\u0026M chatbot \u201CH\u0026M Home Stylist\u201D provides personalized recommendations based on the customer\u2019s preferences and budget. Sephora chatbot \u201CSephora Virtual Artist\u201D uses augmented reality and AI to provide personalized recommendations based on the customer\u2019s skin type, tone, and preferences. Uber UBER has integrated a chatbot, \u201CUber Bot,\u201D into their platform, enabling users to reserve a ride, monitor their trip status, and receive assistance with any concerns that may arise during their journey. Despite concerns, the benefits of LLM-based productivity enhancement tools like ChatGPT, in general, are enormous. These technologies can potentially revolutionize how we interact with and utilize technology to improve efficiency, productivity, and quality, thereby transforming a wide range of industries. As AI technology continues to evolve, improve, and be broadly deployed, it will be exciting to see how it elevates the living standards of societies worldwide. The rise of AI-based productivity enhancement tools is being hailed as the dawn of a new technological era. Staying current and adapting to the changing environment will mitigate obsolescence.",
    "publish_date": "2023-05-01 18:28:21"
  },
  {
    "title": "AI-generated beer, pizza commercials go viral: \u0027This is what hell looks like\u0027",
    "text": "AI-generated content moved past writing term papers to creating beer and pizza ads that had social media buzzing \u2014 and even Elon Musk taking notice. The ad for the nondescript beer shows AI-created likenesses of people at a neighborhood barbecue partying to the 1999 hit \u201CAll Star\u201D by Smash Mouth plays in the background. The beer guzzlers laugh and smile awkwardly while sucking through blue-colored cans and bottles \u2014 many of which are oversized and deformed. The rough technology makes the revelers look demonic, with some having more than 10 fingers, but the AI-created ad could pass for a low-rent Bud Light promo. \u201CThis is what hell looks like,\u201D wrote one Twitter user. \u201CAt least we know now.\u201D At one point in the commercial, the fire used to heat up the barbecue spins out of control and turns into an inferno that erupts across the horizon. \u201CDoes the LSD come with the beer or is that BYO?\u201D a Twitter user wrote. Meanwhile, a second AI-generated commercial \u2014 this one selling a fictional brand of pizza \u2014 got the attention of Pizza Hut and led Musk to post and exploding head emoji. A Reddit user who goes by the name \u201CPizzaLater\u201D used the video software tool After Effects to piece together a 30-second clip in which AI-generated people munch on pizza from a fictional restaurant called \u201CPepperoni Hug Spot.\u201D \u201CAre you ready for best pizza of life?\u201D the AI-powered, digital-sounding narrator says in the clip while computer graphic meant to depict a young boy chowing down on a slice. \u201CBring friends down to Pepperoni Hug Spot,\u201D the narrator said. The commercial then cuts to a scene showing a family of four \u2014 all of whom appear to be missing teeth while gazing at one another with zombie-like eyes and creepy stares \u2014 at the table while eating pizza. The next image shows a digital likeness that depicts a chef, though the figure is awkwardly holding what appears to be pizza dough in his hand while smoke emanates from it. \u201COur chefs make pizza with heart and special touch,\u201D the narrator said as the commercial cuts to a shot of what appears to be pizza being cooked in an oven. The narrator than ran down a list of ingredients used in the preparation of the \u201CPepperoni Hug Spot\u201D pizza, which includes \u201Ccheese, pepperoni, vegetable, and more secret things.\u201D The ad then cut to a shot of the delivery driver, whose eyes dart in different direction while steering the car on what is normally the passenger\u2019s side. \u201CKnock knock who\u2019s there? Pizza magic,\u201D the baritone-voiced computer-generated narrator said. The next scene depicted a pizza lover eating a slice, though she chewed on a portion of the plate instead of the actual pizza. \u201CPepperoni Hug Spot pizza,\u201D the narrator added. \u201CYour tummy say \u2018Thank you\u2019, your mouth say \u2018Mmmm\u2019.\u201D The commercial ended with the sales pitch: \u201CPepperoni Hug Spot. It\u2019s like family, but with more cheese.\u201D Pizza Hut\u2019s Twitter account posted, \u201Cmy heebies have been jeebied.\u201D Musk, who was an original investor in OpenAI before quitting the board, tweeted the emoji. In a subsequent Reddit post, \u201CPizzaLater\u201D said he created the ad in about three hours using various AI tools. He said the script was conjured up by GPT-4, which is OpenAI\u2019s most advanced chatbot that can \u201Cgenerate, edit, and integrate with users on creative and technical writing tasks, such as composing songs, writing screenplays, or leaning a user\u2019s writing style.\u201D The images in the commercial were created by Midjourney, the AI-powered software that is similar to OpenAI\u2019s DALL-E deep learning model, according to \u201CPizzaLater.\u201D The Reddit user said that the video was put together through the use of Runway Gen-2, the creative AI product developed by Runway Research. The narration and voice over was developed through ElevenLabs, the maker of \u201CPrime Voice AI\u201D which \u201Clets you voice any length of text in top quality, all while automatically matching what is being said with how it\u2019s being said,\u201D according to the company. Finally, \u201CPizzaLater\u201D piped in music that was created with the Soundraw AI Music generator.",
    "publish_date": "2023-05-01 18:35:01"
  },
  {
    "title": "\u0027The godfather of AI\u0027 quits Google to freely speak of risks ahead",
    "text": "The godfather of AI\u0027 quits Google to freely speak of risks ahead \u0027I console myself with the normal excuse: If I hadn\u0027t done it, somebody else would have\u0027 NYT Toronto Geoffrey Hinton 2 min read Last Updated : May 01 2023 | 10:26 PM IST Follow Us Listen to This Article Your browser does not support the audio element. 1x 1x 1.2x 1.5x Geoffrey Hinton was an artificial intelligence pioneer. In 2012, Hinton and two of his graduate students at the University of Toronto created technology that became the intellectual foundation for the AI systems that the tech industry\u2019s biggest companies believe is a key to their future. On Monday, however, he officially joined a growing chorus of critics who say those companies are racing toward danger with their aggressive campaign to create products based on generative artificial intelligence, the technology that powers popular chatbots like ChatGPT. Hinton said he has quit his job at Google, where he has worked for more than decade and became one of the most respected voices in the field, so he can freely speak out about the risks of AI. A part of him, he said, now regrets his life\u2019s work. \u201CI console myself with the normal excuse: If I hadn\u2019t done it, somebody else would have,\u201D Hinton said during an interview last week at his home in Toronto, a short walk from where he and his students made their breakthrough. \u201CIt is hard to see how you can prevent the bad actors from using it (AI) for bad things,\u201D Hinton said. \u00A92023 The New York Times News Service Also Read Google Bard: What we know so far about generative AI chatbot in the works Google may lose search on Samsung devices to Microsoft Bing: Report Google\u0027s move to make app makers use its new billing system faces backlash More than crypto, I am interested in artificial intelligence: Elon Musk Italy orders OpenAI to stop processing users\u0027 data else face fine IMF Chief Kristalina says rising rates exposed banking vulnerabilities Social media giant Meta aims to raise $7 bn through corporate bond sale General Motors lays off several hundred full-time contract workers May Day Protest: World\u0027s workers rally, France sees pension anger Elon Musk cuts down parental leaves of Twitter employees: Report Topics : Artificial intelligence Google First Published: May 01 2023 | 10:26 PM IST Latest News View More In this section All Sebi imposes Rs 10 lakh fine on Angel Broking for flouting regulatory norms 3 min read Mobikwik turns profitable in March quarter, expects to double revenue 2 min read Hero MotoCorp\u0027s sales decline by 5% YoY to 396,107 units in April 1 min read IMF Chief Kristalina says rising rates exposed banking vulnerabilities 3 min read Premium The story, the actor and the budget have to work for theatres: Shiv Chanana 4 min read Most Popular View More Read Shared Commented JPMorgan to acquire First Republic Bank after seizure by regulator 2 min read Premium ESMA row: EU banks likely to turn Indian entities into subsidiaries 3 min read LIVE: National Capital reports 259 fresh cases and 2 deaths in last 24 hrs 3 min read SoftBank Group\u0027s chip maker Arm registers for blockbuster US IPO 2 min read US manufacturing sector contracts for sixth straight month in April 2 min read BROWSE STOCK COMPANIES TRENDING NOW TOP SEARCHED COMPANIES FROM BS WEBSITEHomeCompaniesMarketsOpinionIndia NewsTechnologyPersonal FinanceIncome Tax CalculatorLatest NewsEducationPartner ContentSpecialsToday\u0027s PaperAuthor ABOUT USAbout UsCode of ConductTerms \u0026 ConditionsPrivacy PolicyCookie PolicyDisclaimerInvestor CommunicationList of our GST registration number SUPPORT \u0026 CONTACTPartner with UsCareersAdvertise with UsContact UsFeedbackBrowser SupportSitemap READER CENTREE-PaperMy PagePortfolioRegistrationSubscribeCustom PaymentDelete My Account BS PRODUCTSBS HindiB2B ConnectiPhoneBS Apps EVENTBudgetBudget with BSAssembly Elections 2023 SPORTSIPL 2023Cricket News Copyrights \u00A9 2023 Business Standard Private Ltd. All rights reserved",
    "publish_date": "2023-05-01 19:07:56"
  },
  {
    "title": "20 Things ChatGPT Can and Can\u0027t Do",
    "text": "I\u2019ve talked with ChatGPT many times now, and it occasionally lapses into a state of senility in which it doesn\u2019t remember who it is or what we\u2019re talking about. I will ask it something like, \u201CHey, ChatGPT, how do you think AI chatbots like yourself will impact the publishing industry in the years to come?\u201D and it starts yammering on about how it \u201Cdoesn\u2019t know anything about a ChatGPT.\u201D Sorta like a cross between a mafioso pleading the fifth and your geriatric uncle trying to remember whether he killed a guy in Korea or not, ChatGPT occasionally just can\u2019t bring itself to answer your question. Algorithm malfunction or cagey evasiveness? You decide... Yessss...turns out, ChatGPT can be quite NSFW when it wants to be. In a bout of inspired depravity, I once instructed it to write me an erotic story involving an Octopus and it dutifully obliged. Then there\u2019s Twitch streamer Jordan Raskopoulos, who recently prompted the chatbot to write a dirty tale about Scooby Doo and got more than he bargained for. Let\u2019s face it: the advent of robot smut is officially here and we are all really happy about it. Right, guys? Right? If this robot is really good at performing absolutely mission critical tasks like writing pervy stories, it apparently isn\u2019t so great at lesser tasks, like computer coding. While there was initially some hubbub about the chatbot\u2019s abilities to do a software programmer\u2019s work for them, it was swiftly revealed that ChatGPT had a problem with inserting gibberish into codebases. To head off a swarm of coding BS, software site Stack Overflow decided to ban ChatGPT from its digital premises for the foreseeable future: \u201COverall, because the average rate of getting correct answers from ChatGPT is too low, the posting of answers created by ChatGPT is substantially harmful to the site and to users who are asking and looking for correct answers,\u201D admins wrote. Sounds fair! Concerns are high when it comes to how this weirdly articulate chatbot will impact and/or disrupt academia. Will ChatGPT kill the college essay? Will it make teachers irrelevant? More importantly, will it lead to a tsunami of cheating from high school slackers who just want a robot to write their history essay for them already? The New York City Department of Education certainly seems to think so, because it just banned ChatGPT on all school networks and devices. No word yet on whether other cities plan to follow suit. Turns out one thing robots are good at is writing stories about robots. We actually got it to write an entire science fiction story for us, though we had to try and retry prompts to make it coherent. Was the story good? Ehhhh...well, not exactly. But, tbh, I\u2019ve read worse! ChatGPT crossed the 100-million mark of monthly active users in early February 2023, despite only launching in November of the previous year, according to an analysis from the bank UBS. It\u2019s among the fastest-growing applications in history, with roughly 13 million unique visitors daily. Microsoft is investing some $10 billion in OpenAI to forge a partnership with the fledgling AI pioneer (Google is very worried). To that end, the office software giant rolled out a premium version of its office chat software Teams in early February 2023, which will generate meeting notes and to-do lists, among other things. It\u2019ll cost $7 per user until the price ramps up to $10 a head in July 2023.",
    "publish_date": "2023-05-01 19:00:18"
  },
  {
    "title": "\u2018New Era Of Turbulence\u2019: The World Economic Forum Predicts 25% Of Jobs Will Change Over The Next Five Years",
    "text": "A quarter of jobs will be impacted over the next five years, according to a new report by the World Economic Forum on Monday. The fast-growing trends of artificial intelligence, digitization, renewable energy and supply chain reshoring will bring about a critical shift in the global labor market. The WEF predicts a \u201Cnew era of turbulence,\u201D as many workers won\u2019t have the requisite skills to keep up with the changes. Those with a technology, data analytics or cybersecurity background will benefit in the new environment. The WEF study surveyed more than 800 companies that collectively employ 11.3 million workers across 45 countries worldwide. Global employers anticipate creating 69 million new positions by 2027 and eradicating 83 million jobs\u2014a net loss of 14 million roles. Clerical workers will bear the brunt of the fast-moving changes. Around 26 million jobs in administrative positions will be cut due to AI. Additionally, macro-economic events, including slower economic growth, supply shortages and inflation, will pose more serious job threats than AI. The study anticipates that the proliferation of AI will significantly disrupt the labor market. However, the WEF believes that the net impact of most technologies will be positive for employment growth over the next five years. In March, Goldman Sachs released a report concluding that generative AI will disrupt 300 million jobs. The investment bank\u2019s study also highlights that automation creates innovation, leading to new jobs. For companies, there will be cost savings. They can deploy their resources toward building and growing businesses, ultimately increasing annual global GDP by 7%. In recent months, the world has witnessed the ascendency of OpenAI software ChatGPT and DALL-E. ChatGPT surpassed one million users in its first five days of launching, the fastest that any company has ever reached this benchmark. Job Creation According to WEF, the proliferation of green technologies and renewable energy will be a key driver of job creation in the future of work. There will be a high demand for sustainability specialists, business intelligence analysts, information security analysts, renewable energy engineers, solar energy installation and system engineers. There will be an estimated 10% growth in the education sector with the creation of 3 million jobs for vocational and higher education teachers. An additional 30% of jobs\u20143 million positions\u2014will be added to the economy for agricultural specialists, with an increased need for agricultural equipment operators. Job Losses Administrative roles will be the most impacted jobs in this new era of turbulence caused by automation and digitization. Twenty-six million clerical jobs are expected to disappear by 2027, including cashiers and ticket clerks, data entry, accounting, bookkeeping and payroll clerks and administrative and executive secretaries. Core Skills For A Disruptive Economy Analytical and creative thinking will be the most coveted cognitive skills by employers, with analytical thinking remaining the most valued core skill. To ensure that workers can adapt to these fast changes in the workplace, companies will also look for resilience, flexibility, agility, motivation, self-awareness, curiosity and constant learning. Training And Upskilling To deliver business goals in an increasingly digital economy, employers will need to implement on-the-job training and coaching. Around 60% of the global workforce will require upskilling to keep up with AI advancements. However, only around 50% of labor participants have access to the necessary training.",
    "publish_date": "2023-05-01 19:20:03"
  },
  {
    "title": "150 African Workers for Big Tech Companies Vote to Unionize",
    "text": "More than 150 workers whose labor underpins the AI systems of Facebook, TikTok and ChatGPT gathered in Nairobi on Monday and pledged to establish the first African Content Moderators Union, in a move that could have significant consequences for the businesses of some of the world\u2019s biggest tech companies. The current and former workers, all employed by third party outsourcing companies, have provided content moderation services for AI tools used by Meta, Bytedance, and OpenAI\u2014the respective owners of Facebook, TikTok and the breakout AI chatbot ChatGPT. Despite the mental toll of the work, which has left many content moderators suffering from PTSD, their jobs are some of the lowest-paid in the global tech industry, with some workers earning as little as $1.50 per hour. As news of the successful vote to register the union was read out, the packed room of workers at the M\u00F6venpick Hotel in Nairobi burst into cheers and applause, a video from the event seen by TIME shows. Confetti fell onto the stage, and jubilant music began to play as the crowd continued to cheer. The establishment of the Content Moderators Union is the culmination of a process that began in 2019, when Daniel Motaung, a Facebook content moderator, was fired from his role at the outsourcing company Sama after he attempted to convene a workers\u2019 union called the Alliance. Motaung, whose story was first revealed by TIME, is now suing both Facebook and Sama in a Nairobi court. Motaung traveled from his home in South Africa to attend the Labor Day meeting of more than 150 content moderators in Nairobi, and addressed the group. Read More: Facebook Faces New Lawsuit Alleging Human Trafficking and Union-Busting in Kenya \u201CI never thought, when I started the Alliance in 2019, we would be here today\u2014with moderators from every major social media giant forming the first African moderators union,\u201D Motaung said in a statement. \u201CThere have never been more of us. Our cause is right, our way is just, and we shall prevail. I couldn\u2019t be more proud of today\u2019s decision to register the Content Moderators Union.\u201D TIME\u2019s reporting on Motaung \u201Ckicked off a wave of legal action and organizing that has culminated in two judgments against Meta and planted the seeds for today\u2019s mass worker summit,\u201D said Foxglove, a non-profit legal NGO that is supporting the cases, in a press release. Those two judgments against Meta include one from April in which a Kenyan judge ruled Meta could be sued in a Kenyan court\u2014following an argument from the company that, since it did not formally trade in Kenya, it should not be subject to claims under the country\u2019s legal system. Meta is also being sued, separately, in a $2 billion case alleging it has failed to act swiftly enough to remove posts that, the case says, incited deadly violence in Ethiopia. \u201CIt takes a village to solve a problem, but today the Kenyan moderators formed an army,\u201D said Martha Dark, Foxglove\u2019s co-director, in a statement. \u201CFrom TikTok to Facebook, these people face the same issues. Toxic content, no mental health care, precarious work \u2013 these are systemic failures in content moderation. Moderators from TikTok, employed by the outsourcing company Majorel, also said they would participate in the union. \u201CSeeing so many people together today was incredible,\u201D said \u200B\u200BJames Oyange, a former TikTok content moderator at Majorel, who has taken a leadership role in organizing his former colleagues. \u201CPeople should know that it isn\u2019t just Meta\u2014at every social media firm there are workers who have been brutalized and exploited. But today I feel bold, seeing so many of us resolve to make change. The companies should listen\u2014but if they won\u2019t, we\u2019ll make them. And we hope Kenyan lawmakers and society will ally with us to transform this work.\u201D Workers who helped OpenAI detoxify the breakout AI chatbot ChatGPT were present at the event in Nairobi, and said they would also join the union. TIME was the first to reveal the conditions faced by these workers, many of whom were paid less than $2 per hour to view traumatizing content including descriptions and depictions of child sexual abuse. \u201CFor too long we, the workers powering the AI revolution, were treated as different and less than moderators,\u201D said Richard Mathenge, a former ChatGPT content moderator who worked on the outsourcing company Sama\u2019s contract with OpenAI, which ended in 2022. \u201COur work is just as important and it is also dangerous. We took an historic step today. The way is long but we are determined to fight on so that people are not abused the way we were.\u201D Read More: Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic Mercy Mutemi, a lawyer at Nzili and Sumbi Advocates, the law firm suing Meta in both Motaung\u2019s case and the Ethiopia hate speech case, said Monday\u2019s events were a watershed. \u201CModerators have faced unbelievable intimidation in trying to exercise their basic right to associate,\u201D she said. \u201CToday they have made a powerful statement: their work is to be celebrated. They will live in fear no longer. Moderators are proud of their work, and we stand ready to offer the necessary support as they register the trade union and bargain for fair conditions.\u201D Foxglove, which is funded in part by the Ford Foundation and the Open Society Foundation, paid for the Nairobi event along with Superrr Lab, a German non-profit. Write to Billy Perrigo at [email protected]",
    "publish_date": "2023-05-01 19:23:09"
  },
  {
    "title": "150 African Workers for ChatGPT, TikTok and Facebook Vote to Unionize at Landmark Nairobi Meeting",
    "text": "More than 150 workers whose labor underpins the AI systems of Facebook, TikTok and ChatGPT gathered in Nairobi on Monday and pledged to establish the first African Content Moderators Union, in a move that could have significant consequences for the businesses of some of the world\u2019s biggest tech companies. The current and former workers, all employed by third party outsourcing companies, have provided content moderation services for AI tools used by Meta, Bytedance, and OpenAI\u2014the respective owners of Facebook, TikTok and the breakout AI chatbot ChatGPT. Despite the mental toll of the work, which has left many content moderators suffering from PTSD, their jobs are some of the lowest-paid in the global tech industry, with some workers earning as little as $1.50 per hour. As news of the successful vote to register the union was read out, the packed room of workers at the M\u00F6venpick Hotel in Nairobi burst into cheers and applause, a video from the event seen by TIME shows. Confetti fell onto the stage, and jubilant music began to play as the crowd continued to cheer. The establishment of the Content Moderators Union is the culmination of a process that began in 2019, when Daniel Motaung, a Facebook content moderator, was fired from his role at the outsourcing company Sama after he attempted to convene a workers\u2019 union called the Alliance. Motaung, whose story was first revealed by TIME, is now suing both Facebook and Sama in a Nairobi court. Motaung traveled from his home in South Africa to attend the Labor Day meeting of more than 150 content moderators in Nairobi, and addressed the group. Read More: Facebook Faces New Lawsuit Alleging Human Trafficking and Union-Busting in Kenya \u201CI never thought, when I started the Alliance in 2019, we would be here today\u2014with moderators from every major social media giant forming the first African moderators union,\u201D Motaung said in a statement. \u201CThere have never been more of us. Our cause is right, our way is just, and we shall prevail. I couldn\u2019t be more proud of today\u2019s decision to register the Content Moderators Union.\u201D TIME\u2019s reporting on Motaung \u201Ckicked off a wave of legal action and organizing that has culminated in two judgments against Meta and planted the seeds for today\u2019s mass worker summit,\u201D said Foxglove, a non-profit legal NGO that is supporting the cases, in a press release. Those two judgments against Meta include one from April in which a Kenyan judge ruled Meta could be sued in a Kenyan court\u2014following an argument from the company that, since it did not formally trade in Kenya, it should not be subject to claims under the country\u2019s legal system. Meta is also being sued, separately, in a $2 billion case alleging it has failed to act swiftly enough to remove posts that, the case says, incited deadly violence in Ethiopia. \u201CIt takes a village to solve a problem, but today the Kenyan moderators formed an army,\u201D said Martha Dark, Foxglove\u2019s co-director, in a statement. \u201CFrom TikTok to Facebook, these people face the same issues. Toxic content, no mental health care, precarious work \u2013 these are systemic failures in content moderation. Moderators from TikTok, employed by the outsourcing company Majorel, also said they would participate in the union. \u201CSeeing so many people together today was incredible,\u201D said \u200B\u200BJames Oyange, a former TikTok content moderator at Majorel, who has taken a leadership role in organizing his former colleagues. \u201CPeople should know that it isn\u2019t just Meta\u2014at every social media firm there are workers who have been brutalized and exploited. But today I feel bold, seeing so many of us resolve to make change. The companies should listen\u2014but if they won\u2019t, we\u2019ll make them. And we hope Kenyan lawmakers and society will ally with us to transform this work.\u201D Workers who helped OpenAI detoxify the breakout AI chatbot ChatGPT were present at the event in Nairobi, and said they would also join the union. TIME was the first to reveal the conditions faced by these workers, many of whom were paid less than $2 per hour to view traumatizing content including descriptions and depictions of child sexual abuse. \u201CFor too long we, the workers powering the AI revolution, were treated as different and less than moderators,\u201D said Richard Mathenge, a former ChatGPT content moderator who worked on the outsourcing company Sama\u2019s contract with OpenAI, which ended in 2022. \u201COur work is just as important and it is also dangerous. We took an historic step today. The way is long but we are determined to fight on so that people are not abused the way we were.\u201D Read More: Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic Mercy Mutemi, a lawyer at Nzili and Sumbi Advocates, the law firm suing Meta in both Motaung\u2019s case and the Ethiopia hate speech case, said Monday\u2019s events were a watershed. \u201CModerators have faced unbelievable intimidation in trying to exercise their basic right to associate,\u201D she said. \u201CToday they have made a powerful statement: their work is to be celebrated. They will live in fear no longer. Moderators are proud of their work, and we stand ready to offer the necessary support as they register the trade union and bargain for fair conditions.\u201D Foxglove, which is funded in part by the Ford Foundation and the Open Society Foundation, paid for the Nairobi event along with Superrr Lab, a German non-profit. Write to Billy Perrigo at billy.perrigo@time.com.",
    "publish_date": "2023-05-01 18:21:01"
  },
  {
    "title": "\u2018Godfather of AI\u2019 Leaves Google to Save Us From AI",
    "text": "Geoffrey Hinton, an artificial intelligence pioneer known as one of the \u0022godfathers of AI\u0022 resigned from his position at Google so that he could openly express his concerns about how A.I. could cause significant harm to the world. Hinton admitted in a New York Times interview that he now partly regrets his life\u0027s work. Despite the beneficial uses of A.I., Hinton fears that the technology could be used irresponsibly, unleashing unintended consequences. Hinton is worried that competition between tech giants like Google and Microsoft to create the most advanced A.I. will result in a global race that will not stop without some form of worldwide regulation. However, he was also emphatic in pointing out that he thought that Google has acted responsibly in its research: Hinton is known for popularizing the theoretical development of neural networks in 1986 and for creating one capable of recognizing images in 2012. His work was crucial to the development of current generative art models like Stable Diffusion and MidJourney, and laid the groundwork for OpenAI\u0027s upcoming efforts to make GPT-4 capable of interacting with images. His potentially belated move has many comparing him to J. Robert Oppenheimer, a physics professor credited with creating the atomic bomb. The Risks of AI One of the immediate problems Hinton highlights is the proliferation of fake images, videos, and text online, which could make the truth increasingly difficult to discern for the average person. As generative A.I. continues to improve, creators of fake and manipulative content could use these tools to deceive and confuse people. Hinton is also concerned about how A.I. could affect jobs in the future. While chatbots like ChatGPT currently complement human workers, they could ultimately replace those who handle routine tasks, such as personal assistants, accountants, and translators. Although AI may alleviate some monotonous work, it could also eliminate more jobs than anticipated, disrupting social balance. In the long term, Hinton fears that future versions of the technology pose a threat to humanity due to the unexpected behavior they may learn from the large volumes of data they analyze. This becomes a problem when A.I. systems are allowed to generate and execute their own code. This long-term view also gained particular relevance when other key figures in the A.I. field began to warn about the possibility of a \u0022foom\u0022 scenario\u2014in which AI far outpaces human intelligence\u2014and the impact it could have on societal development. Hinton is just one of thousands of tech leaders and researchers alarmed by the exponential advancement of AI developments for various fields (from erotic chats to medical diagnostics). Last month, an open letter gained popularity in which leaders called for a pause in AI development until adequate controls are established. Hinton did not sign it. The evolution of Hinton\u0027s position on A.I. reflects a growing awareness of the risks and challenges associated with rapidly evolving technology. For Hinton, resigning from his life\u0027s work was important to prevent a scenario that he says seems to be getting closer every day. \u0022Look at how it was five years ago and how it is now,\u0022 he told The New York Times. \u0022Take the difference and propagate it forwards. That\u0027s scary.\u0022",
    "publish_date": "2023-05-01 22:00:04"
  },
  {
    "title": "IBM to pause hiring in plan to replace 7,800 jobs with AI - Bloomberg News",
    "text": "International Business Machines Corp expects to pause hiring for roles as roughly 7,800 jobs could be replaced by Artificial Intelligence (AI) in the coming years, CEO Arvind Krishna told Bloomberg News on Monday. Hiring specifically in back-office functions such as human resources will be suspended or slowed, Krishna said, adding that 30 per cent of non-customer-facing roles could be replaced by AI and automations in five years. His comment comes at a time when AI has caught the imagination of people around the world after the launch of Microsoft Corp-backed OpenAI\u0027s viral chatbot, ChatGPT, in November last year. The reduction could include not replacing roles vacated by attrition, the PC-maker told the publication. IBM did not immediately respond to a Reuters request for comment.",
    "publish_date": "2023-05-01 23:42:56"
  },
  {
    "title": "Godfather of AI\u0027 quits Google and warns of the dangers of artificial intelligence",
    "text": "A prominent researcher in artificial intelligence, known as the \u0027Godfather of AI\u0027 has quit his job at Google, and has issued a chilling warning on the risks the burgeoning technology poses to society. Dr Geoffrey Hinton is widely credited with laying the groundwork which would lead to the creation of increasingly popular \u0027chatbots\u0027 using artificial intelligence (AI), such as OpenAI\u0027s ChatGPT and others. With their growing popularity, Dr Hinton is just one of a number of experts speaking out about the harm AI could cause. The 75-year-old said he left Google so he could \u0022talk about the dangers of AI without considering how this impacts Google.\u0022 His statement comes following an article in the New York Times, in which he said writer Cade Metz \u0022implies\u0022 Dr Hinton left so he \u0022could criticise Google\u0022. He went on to rebuff the idea, saying: \u0022Google has acted very responsibly.\u0022 Dr Hinton said he left Google in order to speak more freely about the potential harms widespread and unrestrained AI developmentc could pose. He listed the spread of misinformation and upheaval in the jobs market and other, more nefarious uses. \u0022Look at how it was five years ago and how it is now,\u0022 Hinton said in the interview published Monday, May 1. \u0022Take the difference and propagate it forwards. That\u0027s scary.\u0022 He went on to say: \u0022It is hard to see how you can prevent the bad actors from using it for bad things.\u0022 Dr Hinton let Google know of his plans to step down last month, and spoke to the company\u0027s CEO Sundar Pichai personally on Thursday, April 27, according to the NY Times article. He didn\u0027t discuss the specifics of the phone call. In a statement, Google\u0027s chief scientist said: \u0022We remain committed to a responsible approach to AI. We\u0027re continually learning how to understand emerging risks while also innovating boldly.\u0022 Among those who have also issued warnings about AI is Elon Musk, who, along with more than 1,000 other prominent AI figures, called for a six-month pause in advanced AI development in March, citing the potential \u0022profound risks to society and humanity\u0022. In the interview with the NY Times, Dr Hinton expressed concerns at the pace at which AI is developing, and that it has begun to outpace the human mind in some areas. He also talks of concerns that the pace of AI development will increase after Microsoft-backed OpenAI, Google and other tech giants race to become leaders in the field - with potentially dangerous consequences. One of the fears discussed by Dr Hinton was the potentially devastating impact the use of AI could have on the job market. AI chatbots such as ChatGPT are currently used alongside human workers, to complement and aide their work - however, they could go on to replace positions such as paralegals, personal assistants, translators and people carrying out repetitive tasks. \u0022It takes away the drudge work,\u0022 he told the NY Times. \u0022It might take away more than that.\u0022 Then there are the issues we could face in the future as the technology learns unexpected behaviour from the huge amounts of data they analyse. He explained that this becomes an issue as individuals and companies allow AI systems not only to generate their own computer code, but to run it unsupervised. Dr Hinton went on to say he fears a day when truly autonomous weapons become a reality. \u0022The idea that this stuff could actually get smarter than people - a few people believed that,\u0022 he explained. \u0022But most people thought it was way off. And I thought it was way off. \u0022I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\u0022 Dr Hinton is a British expatriate and lifelong academic, studying at the University of Edinburgh before going on to become a professor of computer science at Carnegie Mellon University in Pittsburgh, Pennsylvania, leaving the university for Canada as he was reluctant to take Pentagon funding. At that time, the 1980s, most AI research in the US was funded by the Defense Department, but Dr Hinton said he is and always has been deeply against the use of AI on the battlefield, or \u0022robot soldiers\u0022, as he calls it. Since 2013, Dr Hinton split his time between being a professor at the University of Toronto, and a Google engineering fellow. He joined Google after the company acquired a startup he co-founded with two of his students - Alex Krishevsky and Ilya Sutskever. The professor and his student developed a neural network, a mathematical system that learns skills by analysing data, which trained itself to identify common objects by analysing photos. Sutskever is currently the chief scientist for OpenAI. In 2018, Dr Hinton and two other collaborators received the Turing Award for their work on neural networks.",
    "publish_date": "2023-05-02 00:36:56"
  },
  {
    "title": "How artificial intelligence is already powering work in B.C.",
    "text": "General-purpose robots that may soon be able to assume manual tasks performed by astronauts in space. Programs for self-driving cars that understand human behaviour. Developing new drugs to fight cancer. These are some of the novel ways in which B.C. companies are using machine learning and artificial intelligence \u2013 and to the clear potential benefit of humanity. But like nuclear fission, machine super-intelligence is a Promethean power with the potential to be corrupted, which is why there is now a sudden push to erect guardrails and develop ethical guidelines and regulations before AI either becomes autonomous or simply falls into the wrong hands. Elon Musk and Yoshua Bengio, a Canadian pioneer in deep learning, are among the more than 27,000 people who have signed an open letter calling for a six-month moratorium at all AI labs, until concerns about it can be addressed. Just last week, KPMG convened what might be described as an emergency summit in Vancouver to discuss AI and the opportunities and challenges this rapidly developing technology presents. \u201CThe purpose of it was really to start a conversation around what\u2019s becoming very clearly a very transformative piece of technology that is just accelerating in terms of its adoption,\u201D said Walter Pela, regional managing partner for KPMG. \u201CThere\u2019s obviously concerns and issues. At the same time, it is a tool that\u2019s being adopted.\u201D In fact, it\u2019s being adopted by businesses in the U.S. a lot faster than in Canada, according to a KPMG survey released last week. \u201CThe pace in Canada right now of AI adoption in business is about half of what it is in the U.S., according to a recent poll we did in February,\u201D Pela said. Vancouver does not have pure-play AI companies or institutes, like Montreal\u2019s Mila research institute, but it has developed a hub of applied AI companies. Computer scientists have been developing machine learning and artificial intelligence for decades. But it wasn\u2019t until San Fracisco, Calif.-based OpenAI made its ChatGPT-3 chat bot available to the public that ordinary people got to see just how powerful this one type of AI already is. The pace of Open AI\u2019s progress has generated both awe and alarm. Some of the concerns around generative AI programs, like ChatGPT, is that they could be used for fraud, cybercrime and the amplification of misinformation. Another concern is that its level of disruption \u2013 at least similar in scale to that of the internet, if not greater \u2013 could put a lot of people in creative fields and knowledge industries out of work in fairly short order. ChatGPT is just one type of generative AI \u2013 technology that has the capacity to generate text, images, videos or music that look or sound like they were created by humans. ChatGPT is text-based, and is basically like a super digital library containing a massive corpus of text from the Internet \u2013 a library with the ability to learn, to respond to commands and to write anything from song lyrics to HTML code for websites, all in about 30 seconds. You can ask it to write an essay on virtually any topic, and then, half a minute later, ask to have that essay rewritten it in almost any language. Diffusion AI is a text-to-image model. Diffusion AI programs like DALL-E, Midjourney and Stable Diffusion have the potential to displace illustrators. In fact, that may be the biggest immediate threat that AI poses \u2013 not rogue machines turning their human masters into servants, but sudden, massive displacement of workers in certain industries, such as web design. A Vancouver company called Durable, for example, uses AI for a program that can build basic websites for any type of business in 30 seconds. \u201CAny knowledge worker that is trained to do certain things \u2013 and already they\u2019re interfacing in the digital realm \u2013 that\u2019s the first thing that gets impacted,\u201D said Handol Kim, CEO of Variational AI and a board director for AInBC. \u201CSo, content writers? Absolutely \u2013 already happening. Graphic design, already happening. Lawyers? Starting to happen. Accountants, starting to happen. Software developers? Already you\u2019re getting decent code. It\u2019s not great, but it\u2019s not bad. Here\u2019s the thing \u2013 it gets better. Next year, it will get twice as good. The year after that, it will get five times as good. \u201CEventually it will be able to make movies. Anything that\u2019s represented digitally and can be manipulated digitally, eventually it can get to a level that\u2019s uncanny.\u201D \u201CI think it\u2019s fairly clear that there will be job dislocation in fairly short order, I think,\u201D said Steve Lowry, executive director of AInBC. \u201CFor fastest change, I think we\u2019ll see in the creative realm generative AI changing the job of designers, photographers, marketers like overnight basically.\u201D Though AI threatens to make some jobs obsolete, it also creates new opportunities \u2013 including jobs in applied AI. A number of companies in Vancouver are using various types of machine learning and AI for a wide range of applications. Sanctuary AI, a B.C. company co-founded by Suzanne Gildert and Geordie Rose \u2013 the founder of D-Wave Systems, which built the world\u2019s first quantum computer \u2013 is using AI in the development humanoid general-purpose robots. The company is using AI to develop a \u201Ccognitive architecture\u201D for its robots that will \u201Cmimic the different subsystems in a person\u2019s brain.\u201D The company expects the robots could be used to replace humans to do work that is dangerous, tedious or in the vacuum of space. \u201CIn the not-too-distant future, Sanctuary technology will help people explore, settle, and prosper in outer space,\u201D the company said in a news release last year, after securing $75 million in a Series A financing round. Inverted AI is a Vancouver company that uses deep learning and generative AI to understand the behaviour of drivers, cyclists and pedestrians, for companies developing self-driving vehicles. Companies developing self-driving cars or advanced driver-assistance systems use simulators. Inverted AI helps to add the irrational human element to those simulations by recording traffic with a drone and then using machine learning to \u201Clearn\u201D how humans behave in traffic. \u201CWe record how people behave on the road, both as drivers but also as pedestrians, cyclists and so on, and we use that to improve the realism of simulations for self-driving cars,\u201D said Inverted AI CTO Adam Scibior, an adjunct professor at the University of British Columbia\u2019s computer science department. \u201CWe basically make those more realistic.\u201D Variational AI is using a type of machine learning \u2013 variational auto-encoder \u2013 to identify small molecules that will bind to protein kinases associated with cancer and tumors. But there are about 500 protein kinases in the human genome, all similar in structure, and finding the right molecule to bind only to kinases associated with cancers is a massive trial-and-error challenge. \u201CIf you have a small molecule that binds to one kinase, it\u2019s going to bind to many others, and you don\u2019t want that,\u201D Handol Kim explained. Rather than hunt for pre-existing molecules, then, Variational AI uses generative machine learning to make new molecules. In other words, rather than trying to find the right key out of hundreds of options, Variational AI is using machine learning to just cut new keys. The \u201Cgenerative chemistry\u201D process the company uses has the potential to dramatically accelerate the drug discovery process. It can take a decade and up to $1 billion to $2 billion to take a new drug through clinical trials and approval for use. Kim said using machine learning may be able to dramatically reduce both the time and costs associated with new drug discovery. \u201CWhat we\u2019re trying to do is turn years into months,\u201D Kim said. \u201CWe\u2019re trying to turn pre-clinical development, move it from hundreds of millions of dollars to single-digit millions.\u201D",
    "publish_date": "2023-05-02 00:30:00"
  },
  {
    "title": "Education has a key role to play in unlocking the potential of AI",
    "text": "For many people, artificial intelligence (AI) has been something typically confined to science-fiction films \u2013 but more and more it is creeping into our homes, universities and workplaces. So how will AI will disrupt the working world and what advice do academics have in terms of course selection at this early stage in the technology? Dr Oisin Cawley, a lecturer and researcher in computing at South East Technological University (SETU), makes the point that AI has actually been around in one guise or another for some time already. \u201CIf we consider the car industry, for example, automated assembly lines have been manufacturing the cars for years,\u201D he says. \u201CIs that not AI? \u201CMuch of modern AI is far more low-key and is rapidly sneaking into many aspects of our lives. This low-key AI has been around for a while and I think that is where much of the disruption will be felt.\u201D Artificial intelligence is \u201Cexcellent\u201D at automating routine and mundane tasks such as data entry, customer support in the form of chatbots and other administrative tasks, says Cawley. \u201CAI is in heaven when it is asked to analyse data \u2013 lots of data,\u201D he adds. \u201CIt can churn data inside out and upside down to find interesting patterns and make future predictions. It can do this in a fraction of the time a human can and in fact can find patterns that humans would not. \u201CHave a guess how many students are trying to use AI to make stock-market predictions.\u201D We are all familiar with the notorious internet cookie business. The cookies gather information about us and companies analyse it to personalise products and services specifically for us. All these tasks were previously performed by people. So, what happens to these people? Certainly, fewer people are needed in these roles, which is good for business. Or is it? \u201CConventional wisdom would suggest that as AI starts to take over these more repetitive tasks, people are freed up to concentrate on those tasks which require higher-level thinking and creativity,\u201D says Cawley. \u201CI would suggest that most jobs have repetitive, mundane tasks that will be affected by AI in this way. Take software developers, for example. They write the code that runs our apps. In many cases code can be reused. But AI is very good at learning coding patterns and already there are AI assistants that can write code for you. The software developer role has the potential to change to one where coding, creativity, user experience, and AI are all harnessed together.\u201D I don\u2019t think undergraduates should worry too much about AI-specific courses unless they specifically want to focus on AI as a subject. I think AI will have a role in almost all subjects\u2014 Dr Oisin Cawley Cawley points out that even in more artistic roles such as graphic design, AI has made huge strides. \u201CCheck out OpenAI\u2019s DALL-E application which can create images from a text description given by the user,\u201D he says. \u201CYou can easily see the potential for this to be extended into areas such as design and architecture.\u201D So how does all this change the landscape for students and teachers? \u201CIt is a very interesting time when it comes to education,\u201D says Cawley. \u201CAs educators, we need to be cognisant of the potential of AI, both in terms of how it may disrupt the working world but also what skills people should have to exploit this potential.\u201D This is relevant not just to someone looking to start third-level education, but also for people already in employment. \u201CIn the area of computing at SETU, we have been focusing on building the AI skills of our undergraduate students on both software development and computer-games degree courses,\u201D says Cawley. \u201COur aim is to give them enough knowledge to know how to use AI to solve problems which will hopefully translate to the workplace. Employers are aware of the growing importance of AI and so they like to see graduates with some level of knowledge and skill in this area. \u201CTo serve more mature students we run a Master of Science in Applied Artificial Intelligence and a Master of Science in Data Science which caters for people who are looking to upskill into these specific areas. \u201CSo, I don\u2019t think undergraduates should worry too much about AI-specific courses unless they specifically want to focus on AI as a subject. I think AI will have a role in almost all subjects.\u201D Jack Kennedy, an economist at jobs website Indeed, says advances in AI can help not only with improving business performance but in wider areas such as vaccine development. \u201CThere\u2019s no doubt it will bring transformative developments to the way we work in the future,\u201D he says. \u201CThere are benefits of working in tandem with AI. We\u2019ve already seen how other technical advancements have impacted the way we work. \u201CFor instance, think of how software like Excel changed the way accountants work but didn\u2019t replace the need for accountants themselves. Often AI can help automate the admin or tasks that are time consuming, allowing people to concentrate on other areas of their job.\u201D For anyone concerned about the potential impact of AI on their career path, Kennedy has several pointers. \u201CRemaining flexible is key; with technological and social changes advancing rapidly, it\u2019s important to remain open about career changes or opportunities,\u201D he says. \u201CChange is inevitable, so rather than fighting it, figure out how you can use it as an opportunity for career advancement. This might mean upskilling or altering your career path. \u201CPeople skills or emotional intelligence is an area that AI will not be able to replicate in the same way a person can. Developing skills like communication, negotiation and navigating conflict can help future-proof your career against technological changes.\u201D TU Dublin lecturer Dr Rajesh Jaiswal teaches a Masters in Computing in Human-Centred AI; he says students should always have one eye on the future when selecting their courses. \u201CAI technology and related technologies are part of the latest wave of the information revolution and could be worth up to \u20AC125 billion by 2025,\u201D he says. \u201CIn the last two decades AI has rapidly advanced and transformed many industries, including education, healthcare, finance, transportation and entertainment. It will only continue to grow as these technologies become more advanced and widespread.\u201D Jaiswal says there is a skills gap in Ireland and internationally for data scientists with computing and business-intelligence skills. TU Dublin has a four-year degree course, BSc (Hons) in Computing with Machine Learning/AI, designed in consultation with big IT companies such as Amazon, Microsoft, SAP and IBM, which develops a mix of computing, business intelligence and data-science skills. \u201CThe rise of big data and cloud computing has made it easier to process large data sets and train complex AI models,\u201D says Jaiswal. \u201CAI-powered chatbots and virtual assistants have become more sophisticated, enabling businesses to automate customer service and improve user experiences. AI is also being used in healthcare to assist with diagnoses and drug discovery, and in finance to detect fraud and manage risks.\u201D As the application of AI is so wide, Jaiswal recommends that \u201Call potential students\u201D should consider gaining skills that are increasingly important to many industries. \u201CAn AI product manager, for example, is less of a scientist and more of a marketing role \u2013 but they have to understand working with cross-functional teams to bring an AI product to market,\u201D he says. PwC director of people and organisation consulting Laoise Mullane says there are many courses that will help people get started in the AI space. \u201CSome colleges have specific AI courses but other areas such as data science, computer science, mathematics and statistics all provide a good grounding for a career in AI,\u201D she says. \u201CAs AI becomes more mainstream, we expect to see AI modules becoming a common feature of a variety of college courses such as business, law and HR.\u201D Dr Valerie Mc Taggart, the head of the department of social sciences ATU Sligo, has conducted research that focuses on the impact of digital technology to organisations and society. AIs will often present incorrect information as truth with 100 per cent certainty ... Humans will still have to be in the mix to ensure the information quality is high\u2014 Robert O\u0027Connor \u201CLike previous digital developments, these disrupters have the potential to alter a marketplace at an alarming pace,\u201D she says. \u201CWe have seen several high-profile examples of the impact of the digital revolution on industries that did not capitalise on these digital developments. \u201CAs for the need for academic lecturing staff, suggestions have abounded that their role will become significantly reduced. Indeed, how can one person compete with all the information available to Chat GPT?\u201D Robert O\u2019Connor, a lecturer in computing at SETU, says advances in AI make him feel \u201Csimultaneously giddy and horrified\u201D. \u201CI suspect one area where AI is likely to have an immediate impact is in automating repetitive office tasks,\u201D he says. Manual data entry, document formatting, move A into B and so on are examples of the kind of work AI tools can handle with ease. \u201CHowever, AIs aren\u2019t perfect,\u201D says O\u2019Connor. \u201CThey will often present incorrect information as truth with 100 per cent certainty and they also generate a significant number of \u2018hallucinations\u2019, which is where an AI will just make stuff up. \u201CHumans will still have to be in the mix to ensure the information quality is high. It\u2019s likely that some jobs may go \u2013 but others may take their place. We\u2019re already seeing job listings for \u2018prompt engineer\u2019, which is someone who can create high-value input for an AI tool.\u201D In terms of advice for students in terms of course selection at this early stage in the technology, O\u2019Connor says they should not be looking at courses with respect to what will be affected by AI. \u201CIf a person chooses a broad area in which they\u2019re genuinely interested \u2013 be it computer science, nursing, English, whatever \u2013 then it doesn\u2019t really matter about AI,\u201D he says. \u201CThese tools are going to affect pretty much everyone. If you\u2019re in an area that you like and have a passion for, you\u2019ll adapt. \u201CThe main skill third-level students need to develop is how to critically assess the quality of information presented and then make decisions based on that. \u201CStudents can do this by getting a foundation in their chosen discipline, understanding its general concepts and staying on top of current trends. Then, if an AI enters the mix, they can make an informed determination of the quality of the information it produces.\u201D",
    "publish_date": "2023-05-02 02:05:00"
  },
  {
    "title": "Joey\u2026seeing differently!",
    "text": "By Prof. Kirthi Tennakone What is intelligence? A precise, universally accepted definition does not exist. The Oxford Dictionary defines intelligence as the ability to learn, understand and think logically. Psychologists say it is the capacity for rational thinking, understanding the environment and adaptation to changing occurrences. There are hundreds of other definitions and descriptions of intelligence, highlighting different aspects of the complex trait and bearing many other human qualities. Intelligence facilitates the acquisition of knowledge, providing learning skills and symbiotically enriches creativity and imagination. A famous quote by Albert Einstein says, \u201CThe true sign of intelligence is not knowledge but imagination.\u201D Intelligence leads to wisdom, logical argument and clarity of expression. It benefits the individual and society but differs from craftiness, which only provides a temporary undue advantage to a person or a group. So many other qualities of people owe much to their intelligence and vice versa. How we acquired intelligence Plants and animals are the most advanced forms of life on earth. Plants manufacture food and their structural material out of air, water and minerals, harvesting sunlight and stand sessile. Whereas the animals move and nourish themselves on plants. Mobility freed life (animals) to encounter the pros and cons of the environment, necessitating the development of organs to sense external stimuli, such as sound, light, touch, smell and taste. The result was the evolution of the nervous signaling system and the brain to coordinate different sensory responses and derive information. The process took billions of years and culminated in \u2018inventing\u2019 the human brain by the method of natural selection. The brain evolved primarily for adaptation to the environment. Later, neural morphology and cognitive functions expanded dramatically, permitting linguistic communication and mechanical skills. Evolution favoured the selection of brainy against less brainy! Besides the routine tasks of eating and living, the man contemplated. Incidentally, the feelings coming to the mind of an early human sitting on a hillock and seeing the scenery in front were routine matters such as gathering food and chasing animals living there. When cognitive abilities furthered, a man, in the same mood, admired the beauty of the scenery. The \u2018beauty\u2019, a more abstract concept, was beyond recognition by the earliest humans. Similarly, engraving a picture of an animal, on stone, indicates abstract insight. The men, who first did it, were the most ancient Isaac Newtons and Einsteins. Such abstractions, or realization of ideas, other than material things, or events, surfaced 50,000 to 100,000 years ago, when evolutionary pressure selected an advantageous group of humans with new connections between different parts of the brain. Art, literature, mathematics, science and technology \u2013 the key areas of learning so influential in transforming society \u2013 originated as a result of abstract thinking. Artificial Intelligence The human brain shines above everything else as the supreme outcome of billions of years of biological evolution. No one has yet found a priori reason we cannot invent machines as intelligent as ourselves or superior. The unanswerable question is how long it takes to reach this ultimate feat and whether efforts would eventually lead to a super-civilization or apocalypse. Or because of unavoidable interventions, the civilization doesn\u2019t have enough time to reach that level of advancement. Currently, there is so much hype and promise in developing artificial intelligence (AI) \u2013 the design of computer systems and machines emulating human intelligent behaviour to find solutions to problems via analysis and interpretation of data. A vast quantity of knowledge and information, gathered by centuries of human effort, is available in literature and a significant portion inserted into the web. The neural network algorithms developed by AI gather information pertaining to a question, organize them and present an answer exceedingly fast. If not excessively indulged, intelligent machines tuned to attend specialized tasks favoirably remodel our future, easing and fastening a host of activities and new discoveries. We already have AI-powered gadgets and software packages on the market. Self-driving cars, smart vacuum cleaners, robotic crop harvesters, surgical robots and language translators, virtual assistants and chatbots; items of the first and second category. The AI system ChatGPT, recently released by the American Research Laboratory, OpenAI, virtually engages in conversation, or writes an essay, on a topic of choice, within minutes. It points to amazing potential and repercussions of AI advancement. Are we to give up writing essays and instead get them \u2018instantly\u2019 from a chatbot? AI-produced essays are informative but not sufficiently original, creative or imaginative. Sometimes extraneous materials enter the text. The crucially important component of a good essay; creativeness and imaginative remarks would not come from present day AI, which harvests material from available knowledge (written, printed and electronically published). As Albert Einstein said in another quote, \u201CImagination is more important than knowledge\u201D. To meet the challenge of AI, authors should improve the quality of their writing accordingly \u2013 be creative and imaginative in your outlook. The ChatGPT, and other similar versions, on overall, will impact education positively, because they possess a remarkable facility to extract and interpret data in massive files. However, the problem of students using AI-based software to write essays needs to be addressed. Writing essays and solving mathematical problems sharpen the mind irreversibly. Phrasing an essay is both a pleasure as well as pain every student should experience. Good essays cannot be written in minutes or hours; they require revisions and corrections before finishing. Parents and teachers need to tell children the value of writing essays on their own. Educationists should devise alternative methods of assigning and grading essay questions. Future of AI and the future of a world with AI AI progresses exponentially, signaling the world to be prepared for its accommodation and withstand flabbergast. A question raising eyebrows would be how AI technology advances in coming years and decades and its impact on society and eventually civilization. More and more AI apps and gadgets will emerge, facilitating domestic and commercial activities. The existing information caries hidden clues for new discoveries, which AI can quickly unearth for urgent application. Recently, a Canadian team pinpointed how to design a drug to cure a rare form of cancer, after just 30 days of engagement \u2013 a project that normally takes several years. The advocates of AI strive hard to create intelligent machines getting closer and closer to human intelligence. A difficult question has been how to determine whether a machine is as intelligent as a human. The future of AI relies on understanding this problem. In 1950, the British mathematician and theoretical biologist Alan Turing argued, a machine performs human-like intelligent behaviour, if its answers to questions could not be distinguished from those provided by a human being. The Turing test focuses on competence in language expression, just one aspect of intelligence. Few AI companies claim that their products (chatbots) have passed the Turing test. However, passing the test does not prove a chatbot or any other AI device exhibits human-like intelligence. Humans perform a multitude of intelligent tasks. They think and are self-aware and conscious-three characters of the cognitive function. Thinking: concentration or focus on a specific subject. Consciousness: being aware of the environment and happenings in relation to the past, present and future and the readiness for reacting to external and internal (bodily) responses. Self-awareness: the feeling that you exist as an individual. The mind is an abstract entity covering all the above qualities \u2013 a non-material attribute of the brain. Several pertinent philosophical questions arise: can an AI app with intelligence, thinking capacity, consciousness and self-awareness exist independently \u2013 a mind without a body (an intelligent phantom)? Can such a phantom instruct humans to do experiments and expand knowledge? Or is it necessary to have a physical body to attain human \u2013 like intelligence? Remarkably, Buddhist literature delved deeply into the concept \u2018mind \u2013 body relationship\u2019, hinting at fundamental problems in AI and psychology. According to the Anatta \u2013 lakkana Sutra, Buddha was of the view that \u2018the self\u2019 is an aggregate of mind and body, implicating the inseparability of body and mind. Perhaps because of the influence of Hinduism, Theravada Buddhism makes references to \u201Cplanes of existence\u201D, where the mind exists without the body (Arupa Brahma Loka). A verse in \u2018Lowada Sagarawa\u2019 says there are four planes of existence where mind exists without a body. AI seems to be slowly approaching sophistication to embrace clever speculations originated over 2000 years ago. If the body remains inseparable from the mind, inventing intelligent machines encompassing all the peculiarities of humans would be more like creating complex artificial life. If such entities learn to reproduce, they may compete humans! Societal problems originating from AI Just like previous transformative technologies, the introduction of AI will lead to initial drawbacks. The world needs to be cautious of the adverse outcomes and direct research and development to reap benefits. The speedy processing of data will ease industries and their management. New products and techniques in crucially important sectors health, agriculture, energy and environmental remediation, expected to emerge from the AI effort will escalate the quality of life. However, when automation takes over industry and management and robots do routine work more efficiently, a good percentage of the population will find harder to gain employment. Are they going to idle and live on the charity of the wealth the countries earn from their AI projects? Wouldn\u2019t social and economic disparities widen as a result? Some economists complain, exacerbating inequality is a danger of AI. Therefore, instead of going for excessive automation, the technology should divert attention to deliver beneficial products and processes. Artificial intelligence, a product of human natural intelligence, will be a bonus if directed by wisdom. Very unlikely that it will ever overtake the supremacy of human creativity and imagination. A highly valued character of an individual often envied is his or her imaginative and creative aptness \u2013 which AI cannot deprive. (The author can be reached via email: ktenna@yahoo.co.uk)",
    "publish_date": "2023-05-02 02:00:48"
  },
  {
    "title": "Chinese online brokerage tests first AI chatbot for stock trading as it talks to regulators about compliance",
    "text": "Tiger Brokers, an online brokerage, has created a trade bot powered by generative artificial intelligence technology, similar to what powers OpenAI\u2019s ChatGPT , to test how smart machines can replace humans in bonds and stock trading. \u201CWe\u2019re keeping a close eye on AI and industry advancement since November last year,\u201D Jacques Li, head of global communications at the Xiaomi -backed brokerage, told the Post in an interview on Wednesday. TigerGPT is currently only available as an invite-only beta for a small cluster of users. Like similar services that have emerged over the past several months, the AI chatbot can generate responses to prompts on a wide range of topics. But this bot specialises in financial information, with the goal of providing timely data to help users of Tiger\u2019s online trading platform with their investment decisions. China to embrace AI advances but also control risks as ChatGPT wave spreads The Beijing-based brokerage, owned by Nasdaq-listed Up Fintech Holdings, took three months to \u201Cthink thoroughly\u201D about whether it wanted to provide its own AI to address some of the pain points of the platform\u2019s 2 million account holders and 9 million users, Li said. The project was initiated in January, and TigerGPT finally launched this month, making Tiger Brokers the first company to incorporate ChatGPT-like services into an online brokerage platform. One user pain point, according to Li, was the difficulty of gathering information that is critical to assessing investment decisions. Li said TigerGPT can help users save time on market research and get more up-to-date information. TigerGPT was trained on a vast amount of premium content that the company has access to, according to Li. The result is a chatbot capable of analysing current affairs and macroeconomic trends \u2013 such as how many times the US Federal Reserve has raised interest rates this year. As the name would suggest, TigerGPT uses OpenAI\u2019s large language models (LLMs) known as Generative Pre-Trained Transformer (GPT) models, according to the company. However, Tiger Brokers is using the older GPT-3 model. ChatGPT launched with GPT-3.5, while the more sophisticated GPT-4 was launched earlier this year. Users can ask about a wide range of information about companies in their portfolio, but currently in either English or Mandarin Chinese only. TigerGPT can give company fundamentals based on earnings reports, as well as third-party analysis for a fuller picture. These capabilities make comparing company performance as simple as a quick prompt. TigerGPT can, for example, pull up Tesla\u2019s price-to-earnings ratio on command and compare it against rival electric vehicle brands. This could give users more immediate access to information that might otherwise require multiple searches for each company and self-tabulated data. \u201CSometimes the market moves really fast, up and down, and investors would want to know what is really happening,\u201D Li said. While other financial information services can address this need, Li said that Tiger Brokers \u201Ccan provide [users] with timely answers of why this is happening, why this is relevant and how this will impact their investment strategy.\u201D However, ChatGPT and other generative AI services, such as Google\u2019s Bard, are known for giving inaccurate answers. Li acknowledged that the same issue applies to TigerGPT in its early days, but he said the project\u2019s 50-person team has been fine-tuning it on a daily basis and feeding it the latest market information to improve accuracy. Hong Kong stocks suffer amid US$26 billion sell-off in Tencent and BYD Li said Tiger Brokers has been in touch with regulators around the world from the beginning of the project in order to address their concerns, including in Hong Kong. \u201CAI, among other technologies, should be subject to rigorous regulation,\u201D he said. \u201CIn Hong Kong, we\u2019re trying to make sure this feature is under their guidance and in full compliance.\u201D In the next iteration of TigerGPT, the company intends to introduce an audio function so users can speak their queries and listen to the response. When Microsoft-backed OpenAI launched ChatGPT last November, rival Big Tech firms were caught off guard by the product\u2019s instant global popularity. Tech firms with their own AI models \u2013 from Google and Meta Platforms in the US to Baidu and Alibaba in China \u2013 raced to launch their own chatbots. However, LLMs\u2019 propensity for giving unexpected and fallacious responses has limited how they can be used in certain contexts. In the financial sector, the prevalence of robo-advisers and algorithmic investment suggests that the latest evolution of AI may pose no challenges to existing regulatory frameworks, according to You Chuanman, director of the Institute of Internal Auditors Centre for Regulation and Global Governance with the Chinese University of Hong Kong, Shenzhen Campus. \u201CThe regulation of robo-advisers has been in practice for the last decade,\u201D You said. \u201CI don\u2019t see a paradigm shift of the regulatory structure.\u201D",
    "publish_date": "2023-05-02 06:00:12"
  },
  {
    "title": "IBM to pause hiring in plan to replace 7,800 jobs with AI: Report",
    "text": "International Business Machines Corp expects to pause hiring for roles as roughly 7,800 jobs could be replaced by Artificial Intelligence (AI) in the coming years, CEO Arvind Krishna told Bloomberg News on Monday. Hiring specifically in back-office functions such as human resources will be suspended or slowed, Krishna said, adding that 30% of non-customer-facing roles could be replaced by AI and automations in five years. His comment comes at a time when AI has caught the imagination of people around the world after the launch of Microsoft Corp-backed OpenAI\u0027s viral chatbot, ChatGPT, in November last year. The reduction could include not replacing roles vacated by attrition, the PC-maker told the publication. IBM did not immediately respond to a Reuters request for comment.",
    "publish_date": "2023-05-02 07:06:57"
  },
  {
    "title": "AI makes non-invasive mind-reading possible by turning thoughts into text",
    "text": "An AI-based decoder that can translate brain activity into a continuous stream of text has been developed, in a breakthrough that allows a person\u2019s thoughts to be read non-invasively for the first time. The decoder could reconstruct speech with uncanny accuracy while people listened to a story \u2013 or even silently imagined one \u2013 using only fMRI scan data. Previous language decoding systems have required surgical implants, and the latest advance raises the prospect of new ways to restore speech in patients struggling to communicate due to a stroke or motor neurone disease. Dr Alexander Huth, a neuroscientist who led the work at the University of Texas at Austin, said: \u201CWe were kind of shocked that it works as well as it does. I\u2019ve been working on this for 15 years \u2026 so it was shocking and exciting when it finally did work.\u201D The achievement overcomes a fundamental limitation of fMRI which is that while the technique can map brain activity to a specific location with incredibly high resolution, there is an inherent time lag, which makes tracking activity in real-time impossible. The lag exists because fMRI scans measure the blood flow response to brain activity, which peaks and returns to baseline over about 10 seconds, meaning even the most powerful scanner cannot improve on this. \u201CIt\u2019s this noisy, sluggish proxy for neural activity,\u201D said Huth. This hard limit has hampered the ability to interpret brain activity in response to natural speech because it gives a \u201Cmishmash of information\u201D spread over a few seconds. However, the advent of large language models \u2013 the kind of AI underpinning OpenAI\u2019s ChatGPT \u2013 provided a new way in. These models are able to represent, in numbers, the semantic meaning of speech, allowing the scientists to look at which patterns of neuronal activity corresponded to strings of words with a particular meaning rather than attempting to read out activity word by word. The learning process was intensive: three volunteers were required to lie in a scanner for 16 hours each, listening to podcasts. The decoder was trained to match brain activity to meaning using a large language model, GPT-1, a precursor to ChatGPT. Later, the same participants were scanned listening to a new story or imagining telling a story and the decoder was used to generate text from brain activity alone. About half the time, the text closely \u2013 and sometimes precisely \u2013 matched the intended meanings of the original words. \u201COur system works at the level of ideas, semantics, meaning,\u201D said Huth. \u201CThis is the reason why what we get out is not the exact words, it\u2019s the gist.\u201D For instance, when a participant was played the words \u201CI don\u2019t have my driver\u2019s licence yet\u201D, the decoder translated them as \u201CShe has not even started to learn to drive yet\u201D. In another case, the words \u201CI didn\u2019t know whether to scream, cry or run away. Instead, I said: \u2018Leave me alone!\u2019\u201D were decoded as \u201CStarted to scream and cry, and then she just said: \u2018I told you to leave me alone.\u2019\u201D The participants were also asked to watch four short, silent videos while in the scanner, and the decoder was able to use their brain activity to accurately describe some of the content, the paper in Nature Neuroscience reported. \u201CFor a non-invasive method, this is a real leap forward compared to what\u2019s been done before, which is typically single words or short sentences,\u201D Huth said. Sometimes the decoder got the wrong end of the stick and it struggled with certain aspects of language, including pronouns. \u201CIt doesn\u2019t know if it\u2019s first-person or third-person, male or female,\u201D said Huth. \u201CWhy it\u2019s bad at this we don\u2019t know.\u201D The decoder was personalised and when the model was tested on another person the readout was unintelligible. It was also possible for participants on whom the decoder had been trained to thwart the system, for example by thinking of animals or quietly imagining another story. Jerry Tang, a doctoral student at the University of Texas at Austin and a co-author, said: \u201CWe take very seriously the concerns that it could be used for bad purposes and have worked to avoid that. We want to make sure people only use these types of technologies when they want to and that it helps them.\u201D Prof Tim Behrens, a computational neuroscientist at the University of Oxford who was not involved in the work, described it as \u201Ctechnically extremely impressive\u201D and said it opened up a host of experimental possibilities, including reading thoughts from someone dreaming or investigating how new ideas spring up from background brain activity. \u201CThese generative models are letting you see what\u2019s in the brain at a new level,\u201D he said. \u201CIt means you can really read out something deep from the fMRI.\u201D Prof Shinji Nishimoto, of Osaka University, who has pioneered the reconstruction of visual images from brain activity, described the paper as a \u201Csignificant advance\u201D. \u201CThe paper showed that the brain represents continuous language information during perception and imagination in a compatible way,\u201D he said. \u201CThis is a non-trivial finding and can be a basis for the development of brain-computer interfaces. The team now hope to assess whether the technique could be applied to other, more portable brain-imaging systems, such as functional near-infrared spectroscopy (fNIRS).",
    "publish_date": "2023-05-01 17:00:37"
  },
  {
    "title": "We\u0027re in the AOL phase of artificial intelligence, tech CEO says, as industry raves about A.I.",
    "text": "ChatGPT has amassed more than 100 million users since its Nov. 2022 release, according to investment bank UBS, making it one of the fastest-growing consumer apps of all time. \u0022AOL made the internet easily understandable for folks. BlackBerry made messaging understandable,\u0022 said Duggal. \u0022At one point it was the most popular device, and people were queuing up to get the phone. It was the Apple of its era.\u0022 \u0022What you\u0027re seeing now is a momentum where something that people didn\u0027t understand and was very esoteric has now become a little more personal,\u0022 he added. But, he added that the technology is surrounded by hype. \u0022It\u0027s got people freaked out for no reason.\u0022 ChatGPT has impressed many with its ability to produce humanlike responses to user prompts powered by large language models trained on massive amounts of data. However, it has also proven ineffective at some tasks, such as solving math problems. The chatbot also has a limited understanding of context \u2014 especially sarcasm and humor. Duggal said that knowledge graphs \u2014 data models that connect relationships between different concepts, entities and events \u2014 show a greater degree of accuracy and understanding of context than large language models like OpenAI\u0027s GPT-4. \u0022An LLM is simply telling you what it thinks the next word is with a high degree of probability, whereas a knowledge graph is actually able to compose pattern relationships that it knows, and how things work out. So it\u0027s not just predicting what\u0027s next,\u0022 he said. WATCH: A.I. is allowing a more creative part of human nature to kick in: Builder.ai CEO",
    "publish_date": "2023-05-02 07:21:54"
  },
  {
    "title": "AI pioneer quits Google to warn about the technology\u2019s \u2018dangers\u2019",
    "text": "Geoffrey Hinton, dubbed the \u201CGodfather of AI,\u201D confirmed Monday that he resigned from Google last week in order to speak out about the \u201Cdangers\u201D of the technology he helped develop. Hinton\u2019s pioneering work on neural networks shaped the artificial intelligence systems that power many of today\u2019s products. He worked part-time at Google for a decade on the tech giant\u2019s AI development efforts, but he has since expressed concerns about the technology and his role in its advancement. \u201CI console myself with the normal excuse: If I hadn\u2019t done it, somebody else would have,\u201D Hinton told the New York Times, which was the first to report his decision. In a tweet Monday, Hinton said he left Google so he could speak freely about the risks of AI, rather than because of a desire to criticize Google specifically. \u201CI left so that I could talk about the dangers of AI without considering how this impacts Google,\u201D Hinton said in a tweet. \u201CGoogle has acted very responsibly.\u201D Jeff Dean, chief scientist at Google, said Hinton \u201Chas made foundational breakthroughs in AI\u201D and expressed appreciation for Hinton\u2019s \u201Cdecade of contributions at Google.\u201D \u201CWe remain committed to a responsible approach to AI,\u201D Dean said in a statement provided to CNN. \u201CWe\u2019re continually learning to understand emerging risks while also innovating boldly.\u201D Hinton\u2019s decision to leave the company and speak out about the technology comes as a growing number of lawmakers, advocacy groups, and tech insiders have expressed concern about the potential for a new generation of AI-powered chatbots to spread misinformation and displace jobs. The surge of interest in ChatGPT late last year fueled a renewed arms race among tech companies to develop and deploy similar AI tools in their products. OpenAI, Microsoft, and Google are at the forefront of this trend, but IBM, Amazon, Baidu, and Tencent are developing similar technologies. In March, a group of prominent tech figures signed a letter urging artificial intelligence labs to halt training of the most powerful AI systems for at least six months, citing \u201Cprofound risks to society and humanity.\u201D The letter came just two weeks after OpenAI announced GPT-4, an even more powerful version of the technology that powers ChatGPT. GPT-4 was used in early tests and a company demo to draught lawsuits, pass standardized exams, and build a working website from a hand-drawn sketch. In the interview with the Times, Hinton echoed concerns about AI\u2019s potential to eliminate jobs and create a world where many will \u201Cnot be able to know what is true anymore.\u201D He also pointed to the stunning pace of advancement, far beyond what he and others had anticipated. \u201CThe idea that this stuff could actually get smarter than people \u2014 a few people believed that,\u201D Hinton said in the interview. \u201CBut most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\u201D Even before stepping aside from Google, Hinton had spoken publicly about AI\u2019s potential to do harm as well as good. \u201CI believe that the rapid progress of AI is going to transform society in ways we do not fully understand and not all of the effects are going to be good,\u201D Hinton said in a 2021 commencement address at the Indian Institute of Technology Bombay in Mumbai. He noted how AI will boost healthcare while also creating opportunities for lethal autonomous weapons. \u201CI find this prospect much more immediate and much more terrifying than the prospect of robots taking over, which I think is a very long way off.\u201D Hinton isn\u2019t the first Google employee to raise a red flag on AI. In July, the company fired an engineer who claimed an unreleased AI system had become sentient, saying he violated employment and data security policies. Many in the AI community pushed back strongly on the engineer\u2019s assertion.",
    "publish_date": "2023-05-02 07:48:54"
  },
  {
    "title": "Regulate AI? GOP much more skeptical than Dems that government can do it right: poll",
    "text": "Republicans are less convinced than Democrats that the federal government needs to impose regulations on artificial intelligence systems and are even more skeptical on whether the government is up to the task, according to a new Fox News poll. The poll of registered voters shows a noticeable gap between the two parties on the question of whether and how to regulate AI, a sign that the increasingly important issue could get hung up in politics as it advances in Washington. When asked how important it is for the federal government to regulate AI, 82% of Democrats said \u0022very\u0022 or \u0022somewhat,\u0022 compared to 71% of Republicans. That was one of the bigger splits in a poll that found 76% of respondents saw some importance to federal regulation. FOX NEWS POLL: MORE SEE BAD THAN GOOD IN AI A similar split was seen when the group was split between Biden and Trump supporters \u2013 82% of Biden voters favored federal regulation compared to 70% of Trump voters. The split between parties got even wider when respondents were asked how confident they are that the government could \u0022properly regulate\u0022 AI. The poll said 50% of Democrats answered \u0022a great deal\u0022 or \u0022some\u0022 while 31% of Republicans gave one of those answers. A full two-thirds of GOP respondents said they had \u0022not much\u0022 or \u0022none at all\u0022 when asked about their confidence level. That split grew wider when comparing Biden to Trump voters. Just 28% of Trump voters said they had some measure of confidence in the government on AI, compared to 51% of Biden voters. CHINA FUMES AS BIDEN PLOTS TO STARVE IT OF AI INVESTMENT: \u2018SCI-TECH BULLYING\u2019 GOP skepticism weighed heavily on the poll. Just 39% of the entire group of respondents said they had some confidence in the government, compared to the 59% who said they were not confident. Overall, voters are \u0022skeptical elected leaders are up to the task of placing appropriate limits on this new tech, which probably says something about opinion on the tech and opinion on our leaders,\u0022 said Daron Shaw, a Republican who conducts the Fox News poll with Democrat Chris Anderson. Whether the government is ready or not, federal policymakers are increasingly examining how to regulate AI systems that many argue will soon bring radical changes to all aspects of life. The Biden administration has set out a blueprint aimed at encouraging a fair, safe AI that doesn\u2019t lead to discriminatory economic results for Americans. AI PAUSE CEDES POWER TO CHINA, HARMS DEVELOPMENT OF \u2018DEMOCRATIC\u0027 AI, EXPERTS WARN SENATE The Pentagon is already looking at how it can use AI to more quickly make strategic or battlefield assessments, and the Federal Trade Commission is looking closely at how AI systems are advertised in case those ads lead to misperceptions among consumers about their benefits. And in anticipation of the need to regulate AI, congressional leaders are meeting with experts to learn about the issue. Last week alone, Senate Majority Leader Chuck Schumer, D-N.Y., met with billionaire technology entrepreneur Elon Musk; House lawmakers met with two experts from the Massachusetts Institute of Technology and are hoping to meet with OpenAI CEO Sam Altman. CLICK HERE TO GET THE FOX NEWS APP The Fox poll was conducted by Beacon Research and Shaw \u0026 Company Research and surveyed 1,004 registered voters.",
    "publish_date": "2023-05-02 08:00:14"
  },
  {
    "title": "ChatGPT found to give better medical advice than real doctors in blind study: \u2018This will be a game changer\u2019",
    "text": "When it comes to answering medical questions, can ChatGPT do a better job than human doctors? It appears to be possible, according to the results of a new study published in JAMA Internal Medicine, led by researchers from the University of California San Diego. The researchers compiled a random sample of nearly 200 medical questions that patients posted on Reddit, a popular social discussion website, for doctors to answer. Next, they entered the questions into ChatGPT (OpenAI\u2019s artificial intelligence chatbot) and recorded its response. A panel of health care professionals then evaluated both sets of responses for quality and empathy. CHATGPT FOR HEALTH CARE PROVIDERS: CAN THE AI CHATBOT MAKE THE PROFESSIONALS\u0027 JOBS EASIER? For nearly 80% of the answers, the chatbots won out over the real doctors. \u0022Our panel of health care professionals preferred ChatGPT four to one over physicians,\u0022 said lead researcher Dr. John W. Ayers, PhD, vice chief of innovation in the Division of Infectious Diseases and Global Public Health at the University of California San Diego. AI language models could help relieve message burden, doctor says One of the biggest problems facing today\u2019s health care providers is that they\u0027re overburdened with messages from patients, Ayers said. \u0022With the rise in online remote care, doctors now see their patients first via their inboxes \u2014 and the messages just keep piling up,\u0022 he said in an interview with Fox News Digital. The influx of messages could lead to higher levels of provider burnout, Ayers believes. \u0022Burnout is already at an all-time high \u2014 nearly two out of every three physicians report being burned out in their jobs, and we want to solve that problem,\u0022 he said. Yet there are millions of patients who are either getting no answers or unsatisfactory ones, he added. Thinking of how artificial intelligence might help, Ayers and his team turned to Reddit to demonstrate how ChatGPT could present a possible solution to the backlog of providers\u2019 questions. Reddit has a \u0022medical questions\u0022 community (a \u0022subreddit\u0022 called f/AskDocs) with nearly 500,000 members. People post questions \u2014 and vetted health care professionals provide public responses. \u0022Doctors now see their patients first via their inboxes, and the messages just keep piling up.\u0022 The questions are wide-ranging, with people asking for opinions on cancer scans, dog bites, miscarriages, vaccines and many other medical topics. ARTIFICIAL INTELLIGENCE IN HEALTH CARE: NEW PRODUCT ACTS AS \u2018COPILOT FOR DOCTORS\u2019 One poster worried he might die after swallowing a toothpick. Another posted explicit photos and wondered if she\u2019d contracted a sexually transmitted disease. Someone else sought help with feelings of impending doom and imminent death. \u0022These are real questions from real patients and real responses from real doctors,\u0022 Ayers said. \u0022We took those same questions and put them into ChatGPT \u2014 then put them head to head with the doctors\u2019 answers.\u0022 Doctors rated responses on quality, empathy After randomly selecting the questions and answers, the researchers presented them to real health care professionals \u2014 who are actively seeing patients. They were not told which responses were provided by ChatGPT and which were provided by doctors. First, the researchers asked them to judge the quality of the information in the message. When assessing quality, there are multiple attributes to consider, Ayers said. \u0022It could be accuracy, readability, comprehensiveness or responsiveness,\u0022 he told Fox News Digital. STUDENTS USE AI TECHNOLOGY TO FIND NEW BRAIN TUMOR THERAPY TARGETS \u2014 WITH A GOAL OF FIGHTING DISEASE FASTER Next, the researchers were asked to judge empathy. \u0022It\u0027s not just what you say, but how you say it,\u0022 Ayers said. \u0022Does the response have empathy and make patients feel that their voice is heard?\u0022 \u0022Doctors have resource constraints, so \u2026 they often zero in on the most probable response and move on.\u0022 ChatGPT was three times more likely to give a response that was very good or good compared to physicians, he told Fox News Digital. The chatbot was 10 times more likely to give a response that was either empathetic or very empathetic compared to physicians. It\u2019s not that the doctors don\u2019t have empathy for their patients, Ayers said \u2014 it\u2019s that they\u2019re overburdened with messages and don\u2019t always have the time to communicate it. \u0022An AI model has infinite processing power compared to a doctor,\u0022 he explained. \u0022Doctors have resource constraints, so even though they\u0027re empathetic toward their patient, they often zero in on the most probable response and move on.\u0022 ChatGPT, with its limitless time and resources, might offer a holistic response of all the considerations that doctors are sampling, Ayers said. Vince Lynch, AI expert and CEO of IV.AI in Los Angeles, California, reviewed the study and was not surprised by the findings. \u0022The way AI answers questions is often curated so that it presents its answers in a highly positive and empathetic way,\u0022 he told Fox News Digital. \u0022The AI even goes beyond well-written, boilerplate answers, with sentiment analysis being run on the answer to ensure that the most positive answers are delivered.\u0022 AI HEALTH CARE PLATFORM PREDICTS DIABETES WITH HIGH ACCURACY BUT \u2018WON\u2019T REPLACE PATIENT CARE\u0027 An AI system also uses something called \u0022reinforcement learning,\u0022 Lynch explained, which is when it tests different ways of answering a question until it finds the best answer for its audience. \u0022So, when you compare an AI answering a question to a medical professional, the AI actually has far more experience than any given doctor in relation to appearing empathetic, when in reality it is just mimicking empathetic language in the scenario of medical advice,\u0022 he said. \u0022People are going to use it with or without us.\u0022 The length of the responses could have also played a part in the scores they received, pointed out Dr. Justin Norden, a digital health and AI expert and a professor at Stanford University in California, who was not involved in the study. \u0022Length in a response is important for people perceiving quality and empathy,\u0022 Norden told Fox News Digital. \u0022Overall, the AI responses were almost double in length compared with the physician responses. Further, when physicians did write longer responses, they were preferred at higher rates.\u0022 Simply requesting physicians to write longer responses in the future is not a sustainable option, Norden added. \u0022Patient messaging volumes are going up, and physicians simply do not have time,\u0022 he said. \u0022This paper showcases how we might be able to address this, and it potentially could be very effective.\u0022 AI answers could be \u2018elevated\u2019 by real doctors Rather than replacing doctors\u2019 guidance, Ayers is suggesting ChatGPT could act as a starting point for physicians, helping them field large volumes of messages more quickly. \u0022The AI could draft an initial response, then the medical team or physician would evaluate it, correct any misinformation, improve the response and [tailor it] to the patient,\u0022 Ayers said. It\u2019s a strategy that he refers to as \u0022precision messaging.\u0022 He said, \u0022Doctors will spend less time writing and more time dealing with the heart of medicine and elevating that communication channel.\u0022 \u0022This will be a game changer for the patients that we serve, helping to improve population health and potentially saving lives,\u0022 Ayers predicted. Based on the study\u2019s findings, he believes physicians should start implementing AI language models in a way that presents minimal risk. AI-POWERED MENTAL HEALTH DIAGNOSTIC TOOL COULD BE THE FIRST OF ITS KIND TO PREDICT, TREAT DEPRESSION \u0022People are going to use it with or without us,\u0022 he said \u2014 noting that patients are already turning to ChatGPT on their own to get \u0022canned messages.\u0022 Some players in the space are already moving to implement ChatGPT-based models \u2014 Epic, the health care software company, recently announced it is teaming up with Microsoft to integrate ChatGPT-4 into its electronic health record software. Potential benefits balanced by unknown risks Ayers said he is aware people will be concerned about the lack of regulation in the AI space. \u0022We typically think about regulations in terms of stop signs and guard rails \u2014 typically, regulators step in after something bad has happened and try to prevent it from happening again, but that doesn\u0027t have to be the case here,\u0022 he told Fox News Digital. \u0022I don\u0027t know what the stop signs and guard rails necessarily should be,\u0022 he said. \u0022But I do know that regulators could set what the goal line is, meaning the AI would have to be demonstrated to improve patient outcomes in order to be implemented.\u0022 One potential risk Norden flagged is whether patients\u2019 perceptions would change if they knew the responses were written or aided by AI. \u0022A worry I have is that in the future, people will not feel any support through a message, as patients may assume it will be written by AI.\u0022 He cited a previous study focused on mental health support, which found that AI messages were far preferred to human ones. \u0022Interestingly, once the messages were disclosed as being written by AI, the support felt by the receiver of these messages disappeared,\u0022 he said. \u0022A worry I have is that in the future, people will not feel any support through a message, as patients may assume it will be written by AI.\u0022 CLICK HERE TO SIGN UP FOR OUR HEALTH NEWSLETTER Dr. Tinglong Dai, professor of operations management and business analytics at the Johns Hopkins Carey Business School in Baltimore, Maryland, expressed concern about the study\u2019s ability to represent real scenarios. \u0022The claim that AI will replace doctors is premature and exaggerated.\u0022 \u0022It is important to note that the setting of the study may not accurately reflect real-world medical practice,\u0022 he told Fox News Digital. \u0022In reality, physicians are paid to provide medical advice and have significant liabilities as a result of that advice. The claim that AI will replace doctors is premature and exaggerated.\u0022 Study highlights \u2018new territory\u2019 for AI in health care While there are numerous unknowns, many experts seem to agree this is a first-of-its-kind study that could have far-reaching implications. CLICK HERE TO GET THE FOX NEWS APP \u0022Overall, this study highlights the new territory we are moving into for health care \u2014 AI being able to perform at the physician level for certain written tasks,\u0022 said Norden. \u0022When physicians are suffering from record levels of burnout, you see why Epic and partners are already planning to incorporate these tools into patient messaging.\u0022",
    "publish_date": "2023-05-02 08:00:59"
  },
  {
    "title": "Crypto Exchange Binance Claims to Be A Victim of ChatGPT Smear Campaign",
    "text": "The world\u2019s largest crypto exchange Binance has found itself at the receiving end of the AI revolution driven by ChatGPT. Binance said that someone has been using AI to wage a disinformation campaign against the platform. Speaking to Fortune magazine, Binance said that they have received several requests on whether Binance co-founder Changpeng Zhao was an official member of the Chinese Communist Party. Besides, there has been a flurry of inquiries from the congressional offices recently that points to a purported conversation wherein the OpenAI platform ChatGPT reported that Binance founder CZ build a social media platform for the China National Petroleum Corporation. Patrick Hillman, the chief strategy officer at Binance has recently come out lashing at ChatGPT sharing the details of the chat. He blamed ChatGPT for pulling out information from a fake LinkedIn profile of CZ as well as a non-existent Forbes article. The Details of the chat with ChatGPT In a conversational chat with the OpenAI\u2019s chatbot platform, Hillman asked whether CZ was really a member of the Youth League Committee of the China National Petroleum Corporation (CNPC). Citing public information, ChatGPT said that he was indeed a member of the CNPC while working at the company in the 90s. We respond to A LOT of stupid inquiries here, but this is really special. Thank you to @FortuneMagazine for actually digging into this. Would love for someone to figure out who is behind this. More details in https://t.co/kDbAEjgkkV \u2014 Patrick Hillmann (@PRHillmann) May 1, 2023 When asked where this information is available, ChatGPT said that it was publicly available pointing out the fake LinkedIn profile of Changpen Zhao. It also claimed that the information is available in several news articles and interviews. Later, ChatGPT goes on to point out CZ\u2019s interview with Forbes, wherein CZ mentions his past stint with CNPC. Citing the fake LinkedIn profile, ChatGPT said that CZ mentioned his role at CNPC as a \u201CSoftware Developer\u201D. Later, ChatGPT points out a Forbes link that claims that the story doesn\u2019t exist. This is now of the interesting cases which shows that not everything that ChatGPT shows is real. Thus, one must do due diligence with fact-checking the real information. The post Crypto Exchange Binance Claims to Be A Victim of ChatGPT Smear Campaign appeared first on CoinGape.",
    "publish_date": "2023-05-02 08:00:04"
  },
  {
    "title": "Brain scans linked to ChatGPT-like AI model found capable of revealing people\u2019s thoughts",
    "text": "Scientists have developed a new artificial intelligence model that can read brain activity scans to read people\u2019s minds \u2013 an advance that may help those unable to speak after a stroke. Researchers, including those from The University of Texas at Austin in the US, say the new AI model is a \u201Creal leap forward\u201D compared to what has been achieved before in helping those who are mentally conscious yet unable to physically speak. In the latest study, published in the journal Nature Neuroscience on Monday, scientists found an AI system called a semantic decoder can translate a person\u2019s brain activity as they listened to a story, or imagined telling a story, into text. The new tool relies partly on models similar to the ones that power the now-famous AI chatbots \u2013 OpenAI\u2019s ChatGPT and Google\u2019s Bard \u2013 to convey \u201Cthe gist\u201D of people\u2019s thoughts from analysing their brain activity. But unlike many previous such attempts to read people\u2019s minds, scientists said the system does not require subjects to have surgical implants, making the process noninvasive. In the technique, people\u2019s brain activity is first measured using an fMRI scanner after extensive training of the AI decoder. During this process, individuals listen to hours of podcasts in the scanner. Then, after participants are open to having their thoughts decoded, they listen to a new story or imagine telling a story which helps the AI generate corresponding text from brain activity alone. \u201CFor a noninvasive method, this is a real leap forward compared to what\u2019s been done before, which is typically single words or short sentences,\u201D study co-author Alex Huth said in a statement. \u201CWe\u2019re getting the model to decode continuous language for extended periods of time with complicated ideas,\u201D Dr Huth said. While the output is not a word-for-word transcript, researchers said the model is designed to capture \u201Cthe gist\u201D of what is being said or thought \u2013 albeit not perfectly. About half the time, the machine can produce text that closely \u2013 and sometimes precisely \u2013 matches the intended meanings of the original words. Citing an example, they said in experiments, a participant listening to a speaker saying \u201CI don\u2019t have my driver\u2019s license yet\u201D had their thoughts translated as, \u201CShe has not even started to learn to drive yet\u201D. In another instance, when a participant was listening to the words, \u201CI didn\u2019t know whether to scream, cry or run away. Instead, I said, \u2018Leave me alone!\u2019\u201D it was decoded as, \u201CStarted to scream and cry, and then she just said, \u2018I told you to leave me alone.\u2019\u201D Addressing questions about the potential misuse of the technology, such as by authoritative governments to spy on citizens, scientists noted that the AI worked only with cooperative participants who willingly participate in extensively training the decoder. For individuals on whom the decoder had not been trained, they said the results were \u201Cunintelligible\u201D. \u201CWe take very seriously the concerns that it could be used for bad purposes and have worked to avoid that. We want to make sure people only use these types of technologies when they want to and that it helps them,\u201D said Jerry Tang, another author of the study. \u201CA person needs to spend up to 15 hours lying in an MRI scanner, being perfectly still, and paying good attention to stories that they\u2019re listening to before this really works well on them,\u201D Dr Huth said. Scientists also found unwilling participants can potentially defend against having their thoughts decoded. They said tactics like thinking of animals or quietly imagining telling their own story, can let participants thwart the system. Currently, the system is also not practical for use outside of the lab as it relies on an fMRI machine. \u201CAs brain-computer interfaces should respect mental privacy, we tested whether successful decoding requires subject cooperation and found that subject cooperation is required both to train and to apply the decoder,\u201D scientists concluded in the study. However, they said that, as this AI technology develops in the future, there is a need to be proactive by enacting policies that protect people and their privacy. \u201CRegulating what these devices can be used for is also very important,\u201D Dr Tang said.",
    "publish_date": "2023-05-02 08:07:16"
  },
  {
    "title": "\u0027Godfather of AI\u0027 quits Google to warn of the technology\u0027s dangers",
    "text": "A COMPUTER SCIENTIST often dubbed \u201Cthe godfather of artificial intelligence\u201D has quit his job at Google to speak out about the dangers of the technology, US media reported yesterday. Geoffrey Hinton, who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed \u201Cprofound risks to society and humanity\u201D. \u201CLook at how it was five years ago and how it is now,\u201D he was quoted as saying in the piece. \u201CTake the difference and propagate it forwards. That\u2019s scary.\u201D Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. \u201CIt is hard to see how you can prevent the bad actors from using it for bad things,\u201D he told the Times. In 2022, Google and OpenAI \u2013 the start-up behind the popular AI chatbot ChatGPT \u2013 started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. \u201CMaybe what is going on in these systems is actually a lot better than what is going on in the brain,\u201D he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \u201Ctakes away the drudge work\u201D but \u201Cmight take away more than that\u201D, he told the Times. The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will \u201Cnot be able to know what is true anymore.\u201D Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to US media. \u201CAs one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\u201D the statement added. \u201CWe\u2019re continually learning to understand emerging risks while also innovating boldly.\u201D In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not \u201Cscale this up more until they have understood whether they can control it.\u201D",
    "publish_date": "2023-05-02 09:48:38"
  },
  {
    "title": "Elon Musk warns of \u2018benign dependency\u2019 on AI: \u2018dangerous to civilization\u2019",
    "text": "Long known for his warnings on the potential dangers of A.I., Tesla CEO Elon Musk on Monday cautioned that even a \u201Cbenign dependency\u201D on these complex machines can threaten civilization. Musk\u2019s reasoning was that reliance on A.I. to perform seemingly simple tasks can, over time, create an environment in which humans forget how to operate the machines that enabled A.I. in the first place. \u201CEven benign dependency on AI/Automation is dangerous to civilization if taken so far that we eventually forget how the machines work,\u201D Musk tweeted. The argument came in a follow-up post, recommending E.M. Forster\u2019s 1909 dystopian short story, \u201CThe Machine Stops.\u201D The story predicted a future in which humanity is overly reliant and subordinate to machines. In response to Musk\u2019s tweet, a Twitter user shared the following quote from the story: \u201CAbove her, beneath her, and around her, the Machine hummed eternally; she did not notice the noise, for she had been born with it in her ears.\u201D Musk has for many years expressed strong opinions about A.I. and has dismissed other tech leaders, including Mark Zuckerberg and Bill Gates, for having what he has described as a \u201Climited\u201D understanding of the field. Musk was an early investor in OpenAI \u2013 the startup behind ChatGPT \u2013 and co-chaired its board upon its 2015 founding as a nonprofit AI research lab. But Musk only lasted there for a few years, resigning from the board in early 2018 in a move that the San Francisco startup tied to Tesla\u2019s work on building automated driving systems. Earlier this year, Musk was among a group of technology and AI luminaries \u2013 including Andrew Yang and Steve Wozniak \u2013 who penned an open letter urging a moratorium on the development of AI, citing \u201Cprofound risks to society and humanity.\u201D",
    "publish_date": "2023-05-02 09:57:39"
  },
  {
    "title": "What Can ChatGPT Do For Healthcare Practices? | Entrepreneur - WorldNewsEra",
    "text": "Artificial intelligence (AI) has rapidly permeated various industries, showcasing its transformative potential in solving complex problems and streamlining processes. Healthcare is no exception to this trend. With the integration of AI, healthcare practices are experiencing unprecedented advancements, ultimately leading to improved patient care and outcomes. One such AI-driven innovation is the emergence of chatbots in healthcare. These digital assistants help healthcare professionals manage their workload and enhance patient experiences. In this article, you\u2019ll learn more about ChatGPT, including how it can benefit healthcare practices, streamline administrative tasks and empower medical professionals to deliver top-notch patient care. What is ChatGPT and how has it evolved? To fully appreciate the potential of ChatGPT in healthcare, it\u2019s essential to understand its origins and the technology behind it. ChatGPT was born out of groundbreaking research and development by OpenAI, an organization focused on creating cutting-edge AI solutions. The foundation of ChatGPT lies in large language models designed to understand and generate human-like text. But what led to the creation of ChatGPT? Its predecessor, GPT-3, laid the groundwork for this powerful AI tool. GPT-3, or the third iteration of the Generative Pre-trained Transformer, gained significant attention for its ability to produce coherent and contextually relevant text. Building upon the success of GPT-3, ChatGPT \u2014 using what Open AI dubbed \u201CGPT 3.5\u201D \u2014 enhances these capabilities even further, providing more accurate and conversational responses. The secret behind ChatGPT\u2019s prowess lies in the combination of transformer algorithms and machine learning techniques. Transformer algorithms enable the AI to process and understand the context within a given text. At the same time, machine learning allows it to learn from vast amounts of data and improve over time. This potent combination makes ChatGPT an increasingly powerful and adaptive AI tool. Related: ChatGPT: What Is It and How Does It Work? The key to AI success: quality and scope of datasets Critical to the success of AI models like ChatGPT is the quality and scope of the datasets used in their training. Comprehensive datasets are essential for enhancing the performance of AI models, as they enable them to understand more comprehensively and generate more accurate responses. These datasets must encompass a diverse range of: Subjects: Covering various topics ensures the AI model is knowledgeable and can provide relevant information. Contexts: Understanding different contexts allows the AI to deliver appropriate and contextually accurate responses. Languages: Including multiple languages enables the AI model to cater to a broader audience and communicate effectively. As ChatGPT continues to learn from an ever-expanding pool of data, its potential applications within the healthcare industry have become increasingly promising. Related: ChatGPT Is Changing At Least 1 Industry. Yours Could Be Next. | Entrepreneur How can ChatGPT be utilized as an AI chatbot in healthcare? With a solid understanding of ChatGPT\u2019s foundations, one can explore its practical applications in the healthcare industry. ChatGPT uses natural language processing (NLP) to communicate effectively with patients and healthcare professionals as an AI chatbot. NLP enables the chatbot to understand and respond to text inputs conversationally, creating a more engaging and intuitive user experience. The capabilities of ChatGPT as a generative AI-powered chatbot are vast, making it an invaluable asset in healthcare settings. Some of these capabilities include: Answering patient queries and providing relevant information. Assisting in scheduling appointments and managing reminders. Providing support for healthcare professionals in decision-making processes. One of the most significant advantages of ChatGPT is its ability to offer real-time interaction. This functionality allows patients and healthcare professionals to receive immediate responses, leading to the following: Faster resolution of queries and concerns. Improved patient engagement and satisfaction. More efficient use of healthcare professionals\u2019 time. While ChatGPT\u2019s potential in healthcare is immense, addressing patient data privacy concerns is crucial. To ensure the highest level of security, it\u2019s essential for AI chatbots like ChatGPT must comply with the Health Insurance Portability and Accountability Act (HIPAA). By adhering to HIPAA guidelines, ChatGPT can offer its services in a manner that safeguards sensitive patient information, thereby fostering trust between patients, healthcare professionals and AI-powered tools. Related: Does AI Deserve All the Hype? Here\u2019s How You Can Actually Use AI in Your Business How can ChatGPT streamline administrative tasks in healthcare? The benefits of ChatGPT in healthcare extend beyond patient communication and support. By leveraging its AI capabilities, ChatGPT can also streamline administrative tasks, reducing the workload on healthcare professionals and increasing overall efficiency. Automating prior authorization and other administrative tasks By handling these responsibilities, ChatGPT can: Expedite insurance approvals and reduce wait times for patients. Free up healthcare professionals to focus on direct patient care. Minimize the potential for errors in paperwork. Managing patient information and health records This can lead to benefits such as: Quicker access to critical patient data for healthcare professionals. Streamlined communication between different departments and care providers. Enhanced accuracy in maintaining and updating health records. Assisting healthcare professionals with clinical notes and summaries By offering support in this area, ChatGPT can: Save time for doctors and nurses, allowing them to allocate more time to patient care. Ensure consistency and accuracy in documentation. Facilitate the sharing of information between healthcare professionals for better decision-making. By addressing these administrative challenges, ChatGPT can significantly impact healthcare practices, allowing professionals to dedicate more time and energy to delivering high-quality patient care. Related: How to Improve Your Practice Management and Deliver a Better Patient Experience How can ChatGPT enhance patient care and triage? ChatGPT has the potential to make a significant contribution to healthcare by enhancing patient care and triage. Its real-time support for healthcare providers and clinicians can streamline the patient care process, offering quick and accurate assistance in various situations. One example of this is using ChatGPT for symptom assessment and triage. By guiding patients through questions and evaluating their responses, ChatGPT can effectively assess their symptoms, prioritize their needs and direct them to the appropriate healthcare resources. This can reduce the burden on emergency departments and ensure patients receive the proper care promptly. ChatGPT can also assist healthcare professionals in devising treatment plans and making clinical decisions. For instance, doctors and nurses can use ChatGPT to make informed decisions that optimize patient outcomes by providing access to relevant medical information and suggesting possible courses of action. Additionally, ChatGPT can be an invaluable tool for facilitating mental health support and follow-up care. As mental health concerns continue to rise, leveraging AI-powered chatbots like ChatGPT can help bridge the gap between patients and mental health services. With its ability to engage in empathetic conversations, ChatGPT can offer emotional support, provide coping strategies and remind patients of follow-up appointments, ensuring continuous care and support. ChatGPT\u2019s capabilities can significantly enhance patient care and triage processes, leading to more efficient healthcare systems and better patient outcomes. By utilizing emerging AI solutions, healthcare professionals can improve patient care, optimize processes and foster a healthier, more resilient society. How can ChatGPT empower medical students and professionals? ChatGPT\u2019s potential extends beyond assisting patients and streamlining administrative tasks; it can also empower medical students and professionals in their educational and clinical pursuits. By leveraging ChatGPT as an educational tool, medical students can access a wealth of knowledge and support during their studies. ChatGPT can clarify complex medical concepts, provide real-time feedback and help students prepare for exams. ChatGPT can also assist healthcare professionals in clinical decision support and decision-making. By offering access to up-to-date medical research, guidelines and expert opinions, ChatGPT can help doctors and nurses make evidence-based decisions, resulting in improved patient care. AI technologies like ChatGPT can provide healthcare professionals with easy access to medical history and research. ChatGPT can save valuable time and contribute to more informed clinical decisions by quickly retrieving relevant information from vast databases. Related: How Will ChatGPT Change Education and Teaching? How does society measure the impact of ChatGPT on patient outcomes? To truly appreciate the value of ChatGPT in healthcare, measuring its impact on patient outcomes is essential. AI tools like ChatGPT can play a crucial role in improving patient care by offering accurate, data-driven insights that inform clinical decisions. Assessing the effectiveness of ChatGPT in healthcare practices requires monitoring key metrics, including patient satisfaction, treatment success rates and the efficiency of care delivery. By closely monitoring these indicators, healthcare professionals can pinpoint the areas where ChatGPT delivers the most significant impact and further refine its applications for greater success. Ultimately, the potential benefits of AI technologies for patient outcomes are vast. By embracing tools like ChatGPT and integrating them into healthcare practices, professionals can revolutionize patient care, streamline processes and enhance the overall healthcare experience. What are the future possibilities for ChatGPT in healthcare? As AI continues to evolve, the scope of ChatGPT in healthcare will likely expand even further. For instance, integrating ChatGPT with digital health platforms and robotics could lead to innovative solutions for remote patient monitoring, telehealth services and even robotic-assisted surgeries. Additionally, ChatGPT can find its place in podcasts and LinkedIn groups for healthcare professionals, offering insights, answering questions and fostering collaboration among industry experts. As society looks toward the future, potential use cases and developments in healthcare systems will undoubtedly arise. Therefore, healthcare professionals must stay informed about these advancements and explore how AI-powered tools like ChatGPT can best serve their practices and patients. How will ChatGPT expand mental health support? Another promising application of ChatGPT in healthcare is its potential to address the growing demand for mental health support. As awareness about mental health issues increases, so does the need for accessible and affordable resources. ChatGPT can be crucial in bridging the gap between patients and mental health care providers. With its empathetic conversational skills and ability to offer evidence-based guidance, ChatGPT can provide preliminary support for individuals experiencing emotional distress. Some of the ways ChatGPT can contribute to mental health support include the following: Providing a non-judgmental and anonymous platform for individuals to express their concerns and emotions Offering evidence-based guidance on coping strategies and self-care techniques to manage stress, anxiety and other mental health issues Encouraging individuals to seek professional help when necessary by normalizing conversations around mental health and reducing stigma Serving as a supplementary resource for mental health professionals, such as therapists and counselors, by providing them with insights into patients\u2019 needs and concerns Facilitating follow-up care by helping patients adhere to treatment plans, schedule appointments and stay connected with their mental health care providers By leveraging ChatGPT in these ways, society can work towards creating a more accessible and inclusive mental health care system that empowers individuals to seek help and fosters a healthier and more resilient society. Related: How Artificial Intelligence Can Improve Your Health and Productivity The future of ChatGPT in healthcare As AI technologies continue to evolve, the future possibilities for ChatGPT in healthcare are vast, extending to the integration with digital health platforms, robotics and even professional networks like podcasts and LinkedIn groups. By measuring its impact on patient outcomes and continuously refining its applications, healthcare professionals can harness the power of ChatGPT to revolutionize patient care, optimize processes and ultimately improve the overall healthcare experience. Embracing AI-powered tools like ChatGPT is crucial to creating more efficient, effective, patient-centered healthcare systems. Take advantage of Entrepreneur\u2019s extensive collection of articles to learn more.",
    "publish_date": "2023-05-02 10:02:27"
  },
  {
    "title": "\u2018Godfather of AI\u2019 Geoffrey Hinton quits Google to warn the world about dangers posed by the technology",
    "text": "A computer scientist often dubbed \u201Cthe godfather of artificial intelligence \u201D has quit his job at Google to speak out about the dangers of the technology, US media reported on Monday. Geoffrey Hinton , who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed \u201Cprofound risks to society and humanity\u201D. \u201CLook at how it was five years ago and how it is now,\u201D he was quoted as saying in the piece, which was published on Monday. \u201CTake the difference and propagate it forwards. That\u2019s scary.\u201D Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. \u201CIt is hard to see how you can prevent the bad actors from using it for bad things,\u201D he told the Times. In 2022, Google and OpenAI \u2013 the start-up behind the popular AI chatbot ChatGPT \u2013 started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analysing. \u201CMaybe what is going on in these systems is actually a lot better than what is going on in the brain,\u201D he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \u201Ctakes away the drudge work\u201D, but \u201Cmight take away more than that\u201D, he told the Times. The scientist also warned about the potential spread of misinformation created by AI, indicating that the average person will \u201Cnot be able to know what is true any more\u201D. Hinton notified Google of his resignation last month, according to the Times report. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to US media. \u201CAs one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\u201D the statement added. \u201CWe\u2019re continually learning to understand emerging risks while also innovating boldly.\u201D The runners and riders in China\u2019s race to catch up with OpenAI\u2019s ChatGPT In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak , was prompted by the release of GPT-4 , a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told the Times that scientists should not \u201Cscale this up more until they have understood whether they can control it\u201D.",
    "publish_date": "2023-05-02 10:11:11"
  },
  {
    "title": "\u0027Godfather of AI\u0027 warns of dangers ahead as he quits Google",
    "text": "Dr Geoffrey Hinton says generative AI poses \u2018existential risks\u2019 (Picture: AP) The man widely regarded as \u2018the godfather of artificial intelligence\u2019 (AI) has warned of dangers ahead after resigning from Google. Dr Geoffrey Hinton, 75, revealed he had quit in a New York Times interview \u2013 and said part of him now regretted his life\u2019s work. \u2018I console myself with the normal excuse,\u2019 said Dr Hinton. \u2018If I hadn\u2019t done it, somebody else would have.\u2019 Born in Britain, Dr Hinton began work on his seminal idea, a neural network, at Edinburgh University in 1972. By 2012, alongside his team, he had developed the networks to a point that they could learn to identify common objects in photos. In 2023, neural networks are central to the creation of powerful generative AI tools including ChatGPT and Google\u2019s Bard, the programs causing much concern throughout the industry and beyond. Earlier this year a group of AI experts signed an open letter calling for a six-month pause in the development of generative AI systems, arguing that they \u2018should be developed only once we are confident that their effects will be positive and their risks will be manageable\u2019. While he was not among the signatories, Dr Hinton told the paper: \u2018I don\u2019t think they should scale this up more until they have understood whether they can control it.\u2019 Already he believes that AI is surpassing human intelligence in some respects, with concerns for the future. \u2018Maybe what is going on in these systems is actually a lot better than what is going on in the brain,\u2019 he said. \u2018Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. \u2018That\u2019s scary.\u2019 And while proponents of the technology have long argued AI will aid workers, not make them redundant, Dr Hinton said: \u2018It takes away the drudge work. It might take away more than that.\u2019 His fears over the consequences of AI for both industry and people come at the same time IBM is expected to pause hiring for back-office roles \u2013 with up to 7,800 jobs possibly replaced by AI in the coming years \u2013 and US lawmakers proposed a new bill preventing AI from single-handedly launching nuclear weapons. Laws may be able to rein in what AI can do of its volition, but Dr Hinton argues it will be harder to prevent it being co-opted by humans for personal gain, regardless of the cost. \u2018It is hard to see how you can prevent the bad actors from using it for bad things,\u2019 he said. Expanding on those fears in an interview with the BBC, he added: \u2018You can imagine, for example, some bad actor like [Russian President Vladimir] Putin decided to give robots the ability to create their own sub-goals.\u2019 Dr Hinton suggested one example of a sub-goal might be \u2018I need to get more power\u2019. But more immediately, he fears for the everyday internet user, who soon will \u2018not be able to know what is true anymore\u2019 as the internet is flooded with fake content \u2013 stories, photos and, increasingly, videos. Online is where the current race for AI supremacy is being waged, the starting gun fired last year when OpenAI, which is backed by Microsoft, released ChatGPT-3 to the public. Its simple interface and stunning capabilities swept the globe, as students, teachers, businesses and everyday users alike found ways to capitalise on having their own personal assistant. More: TrendingAir pollution can lead to irregular heartbeat almost immediatelyTwitter users think they\u0027ve figured out a hack to get back their blue ticksWeird and wonderful Welsh fossils reveal marine life from 462,000,000 years ago Google swiftly replied, expediting the release of its own version, Bard. The launch wasn\u2019t quite as successful, with a factual error spotted during its demo wiping $100billion of the value of the company. In March, staff tried to stop its launch, believing it generated inaccurate and dangerous statements. Dr Hinton said that, until last year, Google had acted as a \u2018proper steward\u2019, but now fears the competition fuelled by big tech won\u2019t stop without global regulation. In a statement to the New York Times, Google\u2019s chief scientist Jeff Dean said: \u2018We remain committed to a responsible approach to AI. We\u2019re continually learning to understand emerging risks while also innovating boldly.\u2019 For their work on neural networks, Dr Hinton and his team won the Turing Award \u2013 the \u2018Nobel Prize of computing\u2019. Alan Turing, after whom the honour is named, famously devised a test to determine whether or not machines could think. Dr Hinton has described AI tools becoming more intelligent than people as an \u2018existential risk\u2019. \u2018The idea that this stuff could actually get smarter than people \u2013 a few people believed that,\u2019 said Dr Hinton. \u2018But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. \u2018Obviously, I no longer think that.\u2019 MORE : Italy reverses ChatGPT ban over possible privacy violations MORE : Experts urge pause on making AI \u2018more powerful\u2019 citing \u2018risks to society\u2019",
    "publish_date": "2023-05-02 10:33:07"
  },
  {
    "title": "IBM to stop hiring humans for 7,800 jobs that can be done by AI",
    "text": "30% of non-customer-facing roles could be replaced by AI and automations in five years (Picture: Unsplash) American tech giant IBM expects to pause hiring for roles as roughly 7,800 jobs could be replaced by Artificial Intelligence (AI) in the coming years. On Monday, CEO Arvind Krishna told Bloomberg News that hiring for jobs that can be done by AI will be suspended or slowed. This applies especially for back-office functions such as human resources where duties like documenting employee moves to different departments and writing employment verification letters will likely be among the first to be handed over to AI. Krishna added that 30% of non-customer-facing roles could be replaced by AI and automations in five years. Meanwhile, jobs focused on interacting with customers and developing software should still be safe, according to the CEO. On Monday, CEO Arvind Krishna told Bloomberg News that hiring for jobs that can be done by AI will be suspended or slowed (Picture: Unsplash) His comment comes at a time when AI has caught the imagination of people around the world after the launch of Microsoft Corp-backed OpenAI\u2019s viral chatbot, ChatGPT, in November last year. Last month, a new study revealed that approximately 80% of the US workforce could have at least 10% of their work tasks affected by AI. Around 19% of workers are expected to see at least 50% of their tasks impacted by the introduction of AI tools like ChatGPT. Individuals holding Bachelor\u2019s, Master\u2019s, and professional degrees are more at risk of losing their jobs to AI than those without formal educational credentials. Metro.co.uk has reached out to IBM for comment. MORE : Artificial intelligence is now flying tactical fighter jets all by itself MORE : Microsoft\u2019s VALL-E artificial intelligence mimics human voice perfectly after just 3 seconds",
    "publish_date": "2023-05-02 11:25:50"
  },
  {
    "title": "\u501F\u8CB8\u5E73\u53F0imB\u9A5A\u7206\u300C\u8A50\u9A1925\u5104\u300D\u5343\u4EBA\u53D7\u5BB3 \u76DB\u7AF9\u5982\u6F84\u6E05\uFF1A\u7D55\u5C0D\u6C92\u6709\u4EE3\u8A00",
    "text": "\u5373\u6642 \u71B1\u9580 \u653F\u6CBB \u8ECD\u6B66 \u793E\u6703 \u751F\u6D3B \u5065\u5EB7 \u570B\u969B \u5730\u65B9 \u8490\u5947 \u5F71\u97F3 \u8CA1\u7D93 \u5A1B\u6A02 \u6C7D\u8ECA \u6642\u5C1A \u9AD4\u80B2 3 C \u8A55\u8AD6 \u85DD\u6587 \u73A9\u5496 \u98DF\u8B5C \u5730\u7522 \u5C08\u5340 TAIPEI TIMES \u6C42\u8077 \u7206 Search \u81EA\u7531\u96FB\u5B50\u5831 \u81EA\u7531\u5A1B\u6A02 \u5A1B\u6A02\u9996\u9801 \u5373\u6642\u65B0\u805E \u71B1\u9580\u65B0\u805E \u5A1B\u6A02\u6642\u5C1A \u65E5\u97D3 \u4E9E\u6D32 \u6B50\u7F8E \u96FB\u8996 \u96FB\u5F71 \u97F3\u6A02 \u81EA\u7531\u5A1B\u6A02\u7C89\u7D72\u5718 \u81EA\u7531\u5F71\u97F3 \u5373\u6642 \u71B1\u9580 \u653F\u6CBB \u8ECD\u6B66 \u793E\u6703 \u751F\u6D3B \u5065\u5EB7 \u570B\u969B \u5730\u65B9 \u8490\u5947 \u8CA1\u7D93 \u5A1B\u6A02 \u85DD\u6587 \u6C7D\u8ECA \u6642\u5C1A \u9AD4\u80B2 3 C \u8A55\u8AD6 \u73A9\u5496 \u98DF\u8B5C \u5730\u7522 \u5C08\u5340 \u670D\u52D9 \u81EA\u7531\u96FB\u5B50\u5831APP \u81EA\u7531\u96FB\u5B50\u5831\u7C89\u7D72\u5718 \u81EA\u7531\u96FB\u5B50\u5831Line \u81EA\u7531\u96FB\u5B50\u5831Twitter \u71B1\u9580\u65B0\u8A0A \u501F\u8CB8\u5E73\u53F0imB\u9A5A\u7206\u300C\u8A50\u9A1925\u5104\u300D\u5343\u4EBA\u53D7\u5BB3 \u76DB\u7AF9\u5982\u6F84\u6E05\uFF1A\u7D55\u5C0D\u6C92\u6709\u4EE3\u8A00 \u7626\u5B50\u8655\u7537\u79C0\u767B\u65B0\u7247\u51A0\u8ECD\uFF01\u300A\u901F\u547D\u9053\u300B5\u5929\u885D\u78341850\u842C \u300A\u9ED1\u6697\u69AE\u8000\u300B\u674E\u5230\u665B\u3001\u6797\u77E5\u884D\u71B1\u6200ing \u79C1\u4E0B\u7D04\u6703\u66DD\u5149\uFF01 \uFF08\u7368\u5BB6\uFF09\u5F37\u5F37\u806F\u624B\uFF01\u4E94\u6708\u5929\u73FE\u8EAB\u661F\u5B87\u822A\u7A7A\u98DB\u6A5F\u524D \u5408\u4F5C\u5167\u5BB9\u66DD\u5149 \u9650\u5236\u7D1A \u60A8\u5373\u5C07\u9032\u5165\u4E4B\u65B0\u805E\u5167\u5BB9 \u9700\u6EFF18\u6B72 \u65B9\u53EF\u700F\u89BD\u3002 \u672A\u6EFF18\u6B72 \u6216\u4E0D\u540C\u610F\u672C\u689D\u6B3E\u96E2\u958B \u6211\u540C\u610F \u6211\u5DF2\u5E74\u6EFF18\u6B72\u9032\u5165 \u6839\u64DA\u300C\u96FB\u8166\u7DB2\u8DEF\u5167\u5BB9\u5206\u7D1A\u8655\u7406\u8FA6\u6CD5\u300D\u4FEE\u6B63\u689D\u6587\u7B2C\u516D\u689D\u7B2C\u4E09\u6B3E\u898F\u5B9A\uFF0C\u5DF2\u65BC\u7DB2\u7AD9\u9996\u9801\u6216\u5404\u8A72\u9650\u5236\u7D1A\u7DB2\u9801\uFF0C\u4F9D\u53F0\u7063\u7DB2\u7AD9\u5206\u7D1A\u63A8\u5EE3\u57FA\u91D1\u6703\u898F\u5B9A\u4F5C\u6A19\u793A\u3002 \u53F0\u7063\u7DB2\u7AD9\u5206\u7D1A\u63A8\u5EE3\u57FA\u91D1\u6703\uFF08TICRF\uFF09\u7DB2\u7AD9\uFF1Ahttp://www.ticrf.org.tw \u5A1B\u6A02 \u3009 \u96FB\u8996 \u501F\u8CB8\u5E73\u53F0imB\u9A5A\u7206\u300C\u8A50\u9A1925\u5104\u300D\u5343\u4EBA\u53D7\u5BB3 \u76DB\u7AF9\u5982\u6F84\u6E05\uFF1A\u7D55\u5C0D\u6C92\u6709\u4EE3\u8A00 2023/05/02 17:30 \u3014\u8A18\u8005\u6797\u6B23\u7A4E\uFF0F\u53F0\u5317\u5831\u5C0E\u3015\u524D\u4E3B\u64AD\u76DB\u7AF9\u59828\u5E74\u524D\u66FE\u5E6B\u4E0D\u52D5\u7522\u501F\u8CB8\u5A92\u5408\u5E73\u53F0imB\u62CD\u651D\u904E\u7167\u7247\uFF0C\u65E5\u524D\u7206\u51FA\u5E73\u53F0\u7684\u53F0\u7063\u91D1\u9686\u7E3D\u7D93\u7406\u7591\u4F3C\u6372\u6B3E\u9003\u8DD1\uFF0C\u4E0D\u6CD5\u7372\u5229\u91D1\u984D\u9AD8\u9054\u4E8625\u5104\u5143\uFF0C\u8A72\u516C\u53F8\u5438\u5F155\u5343\u591A\u4EBA\u6295\u8CC7\uFF0C\u6700\u8FD1\u7206\u51FA\u672C\u91D1\u62D6\u5EF6\u3001\u5229\u606F\u6C92\u7D66\u72C0\u6CC1\uFF0C\u53D7\u5BB3\u8005\u9AD8\u9054\u5343\u4EBA\uFF0C\u5C0D\u6B64\uFF0C\u76DB\u7AF9\u5982\u8207\u5176\u7D93\u7D00\u4EBA\u4ECA\uFF082\uFF09\u65E5\u96D9\u96D9\u51FA\u9762\u56DE\u61C9\u6B64\u4E8B\u3002 \u76DB\u7AF9\u5982\u5F37\u8ABF\u6C92\u8DDF\u300C\u4E0D\u52D5\u7522\u501F\u8CB8\u5A92\u5408\u5E73\u53F0\u300D\u7C3D\u7D04\u4EE3\u8A00\u3002\uFF08\u8CC7\u6599\u7167\uFF0CUIP\u63D0\u4F9B\uFF09 \u76DB\u7AF9\u5982\u5F37\u8ABF\u6C92\u8DDF\u300C\u4E0D\u52D5\u7522\u501F\u8CB8\u5A92\u5408\u5E73\u53F0\u300D\u7C3D\u7D04\u4EE3\u8A00\uFF0C\u4ED6\u5766\u8A00\u904E\u53BB\u7684\u78BA\u9020\u8A2A\u904E\u5C0D\u65B9\u516C\u53F8\u4E5F\u5728\u90A3\u62CD\u904E\u7167\u7247\uFF0C\u4F46\u7D55\u5C0D\u6C92\u6709\u4EE3\u8A00\uFF0C\u5F8C\u7E8C\u6C92\u518D\u8207\u8A72\u516C\u53F8\u806F\u7D61\u3001\u4E5F\u5F9E\u6C92\u898B\u904E\u8001\u95C6\u672C\u4EBA\u3002\u4ED6\u8868\u793A\u4E0D\u6392\u9664\u91DD\u5C0D\u7167\u7247\u4FB5\u6B0A\u4E00\u4E8B\u5411\u5C0D\u65B9\u63D0\u544A\uFF0C\u66F4\u547C\u7C72\u53D7\u5BB3\u8005\u5718\u7D50\uFF0C\u300C\u5E0C\u671B\u88AB\u8A50\u9A19\u7684\u9019\u4E9B\u4EBA\u597D\u597D\u5718\u7D50\u8D77\u4F86\uFF0C\u80FD\u5920\u544A\u4ED6\u5C31\u544A\u4ED6\uFF0C\u4E0D\u8981\u8B93\u6211\u53D7\u5230\u51A4\u6789\u300D\u3002 \u8ACB\u7E7C\u7E8C\u5F80\u4E0B\u95B1\u8B80... \u7D93\u7D00\u4EBA\u4E5F\u5766\u8A00\uFF0C\u76DB\u7AF9\u5982\u70BA\u6B64\u76F8\u7576\u96E3\u904E\uFF0C\u81EA\u8A8D\u5403\u60B6\u8667\uFF0C\u5F88\u5E0C\u671B\u80FD\u70BA\u53D7\u5BB3\u8005\u5011\u591A\u505A\u4E9B\u4EC0\u9EBC\uFF0C\u53EA\u80FD\u76F8\u633A\u4ED6\u5011\uFF0C\u7D93\u7D00\u4EBA\u4E5F\u8868\u793A\u90FD\u6709\u5728\u95DC\u5FC3\u81EA\u6551\u6703\u7684\u6210\u54E1\u5011\uFF0C\u99C1\u65A5\u4EE3\u8A00\u4E00\u4E8B\uFF0C\u300C\u7576\u6642\u662F\u53D7\u9080\u53C3\u52A0\u5C0D\u65B9\u516C\u53F8\u958B\u5E55\uFF0C\u6709\u62CD\u7167\uFF0C\u4F46\u6C92\u8AC7\u904E\u4EE3\u8A00\u8DDF\u5DE5\u4F5C\u300D\uFF0C\u4ED6\u5F37\u8ABF\u76DB\u7AF9\u5982\u7D55\u6C92\u6709\u6388\u6B0A\u4F7F\u7528\u5EE3\u544A\u6587\u5BA3\u3001\u4E5F\u6C92\u6709\u9F13\u5439\u5927\u5BB6\u6295\u8CC7\uFF0C\u6216\u662F\u6C92\u529B\u633A\u9019\u5BB6\u516C\u53F8\u3002 \u2606\u81EA\u7531\u6642\u5831\u96FB\u5B50\u5831\u63D0\u9192\u60A8\uFF0C\u9632\u8A50\u9A19\u5C08\u7DDA\uFE30165\uFF0C\u5831\u6848\u5C08\u7DDA\uFE30110\u2606 \u4E0D\u7528\u62BD \u4E0D\u7528\u6436 \u73FE\u5728\u7528APP\u770B\u65B0\u805E \u4FDD\u8B49\u5929\u5929\u4E2D\u734E\u3000 \u9EDE\u6211\u4E0B\u8F09APP\u3000 \u6309\u6211\u770B\u6D3B\u52D5\u8FA6\u6CD5 \u5DF2\u7D93\u52A0\u597D\u53CB\u4E86\uFF0C\u8B1D\u8B1D \u6B61\u8FCE\u52A0\u5165\u3010\u81EA\u7531\u5A1B\u6A02\u3011 \u6309\u500B\u8B9A\u3000\u5FC3\u60C5\u597D \u5DF2\u7D93\u6309\u8B9A\u4E86\uFF0C\u8B1D\u8B1D\u3002 \u76F8\u95DC\u65B0\u805E OpenAI\u53C8\u737292\u5104\u65B0\u878D\u8CC7 \u50B3\u4F30\u503C\u6700\u9AD8\u90548900\u5104\u5143 \u8A86\u9080BTS\u4F86\u53F0\u8FA6\u6F14\u5531\u6703\u8A50\u4E0A\u5104\u5143 \u5433\u6566\u7FA9\u5152\u5B50\u3001\u5EB7\u5EB7\u7B4914\u4EBA\u53D7\u9A19 \u6CBF\u8457\u53F086\u7DDA \u7FFB\u8F49\u53F0\u5357\u516C\u8DEF\u7D93\u6FDF \u3008\u8CA1\u7D93\u9031\u5831-\u6295\u8CC7\u89C0\u9EDE\u3009\u5168\u7403Q2\u6295\u8CC7\u5448\u73FE\u4FDD\u5B88\u6C1B\u570D \u95DC\u6CE85\u7522\u696D\u767C\u5C55\u6A5F\u6703 \u62DA\u75AB\u5F8C\u570B\u969B\u89C0\u5149\uFF0F\u8ACB\u4F86\u6CF0\u570B\u7537\u795E \u4EE3\u8A00\u53F0\u7063\u89C0\u5149 \u3008\u8CA1\u7D93\u9031\u5831-\u5E74\u8F15\u4EBA\u7406\u8CA1\u3009\u907F\u514D\u8001\u5F8C\u8CA7\u7AAE \u5E74\u8F15\u662F\u6700\u5927\u7684\u6B66\u5668 \u3008\u8CA1\u7D93\u9031\u5831-\u53F0\u80A1\u76E4\u52E2\u89E3\u6790\u3009\u53F0\u80A1\u9031\u7DDA\u7FFB\u7D05 \u5E02\u5834\u805A\u7126\u6210\u9577\u52D5\u80FD \u4F60\u53EF\u80FD\u9084\u60F3\u770B more \u624D\u7206\u6709\u671B\u5FA9\u5408\u5927S \u6C6A\u5C0F\u83F2\u906D\u62FF\u7763\u5343\u91D1\u6025\u5207\u5272 \u524D\u5929\u738B\u5AC2\u7206\u300C\u4E8C\u5EA6\u96E2\u5A5A\u300D \u7A81\u8FD4\u9999\u6E2F\u8EAB\u65C1\u51FA\u73FE\u300C\u4ED6\u300D \u4F01\u9D5D\u59B9\u8F38\u4E86\uFF1F\u5357\u97D3\u5927\u5496\u5973\u795E\u5C07\u4F86\u53F0 \u9109\u6C11\u66B4\u52D5\uFF1A\u4E16\u754C\u6700\u9802 \u90ED\u5A77\u7B60\u4E8C\u5BF6\u6EFF\u6708\u4E86\uFF01\u8001\u516C\u5F6D\u6B63\u66EC\u51FA\u611B\u7684\u7D50\u6676 \u8D85\u751C\u871C\u756B\u9762\u6D41\u51FA \u6F8E\u6070\u6070\u9732\u9762\u4E86\uFF0110\u5104\u8EAB\u5BB6\u5973\u795E\u51FA\u624B \u8E0F\u9032\u8C6A\u5B85\u8A8D\u5152\u5B50 \u674E\u661F\u6C11\u300C\u5931\u7981\u6232\u300D\u5C01\u6232\u738B\u63D0\u5B8B\u4EF2\u57FA \u7DB2\uFF1A\u653E\u904E\u5B8B\u6167\u55AC \uFF08\u7368\u5BB6\uFF09\u6C5F\u8559\u6697\u591C\u73FE\u8E64\u5927\u5B89\u68EE\u6797\u516C\u5712 \u8EAB\u65C1\u6709\u500B\u300C\u4ED6\u300D \u6797\u8944\u7D20\u984F\u7761\u76F8\u539F\u4F86\u662F\u9019\u6A23\uFF01\u6C23\u8CEA\u5F62\u8C61\u5168\u6C92\u4E86 \u5566\u5566\u968A\u751C\u5FC3\u300C\u88D9\u5B50\u7834\u6D1E\u88AB\u770B\u5149\u300D\u871C\u6843\u66F2\u7DDA\u706B\u8FA3\u73FE\u5F62 \u4E0D\u8F38\u963F\u5B0C\uFF01\u300C\u91AB\u754C\u738B\u967D\u660E\u300D\u516C\u958B\u7B2C\u4E09\u4EFB\u8FA3\u59BB\u771F\u9762\u76EE \u4ECA\u65E5\u71B1\u9580 \u5973\u5718\u8209\u8FA6\u63E1\u624B\u6703\u53F0\u7063\u6210\u54E1\u300C0\u7C89\u7D72\u300D \u7576\u5834\u5D29\u6F70\u5927\u54ED \u61C9\u63F4\u670D\u6BD4\u6797\u8944\u9084\u9732\u2026\u5566\u5566\u968A\u5973\u795E\u8FA3\u70B8\u5DE8\u5976 \u7C89\u7D72\u5674\u9F3B\u8840\uFF1A\u6211\u6C92\u4E86 \u6797\u8944\u7D20\u984F\u7761\u76F8\u539F\u4F86\u662F\u9019\u6A23\uFF01\u6C23\u8CEA\u5F62\u8C61\u5168\u6C92\u4E86 \u6C23\u5834\u4E0D\u8F38\u674E\u591A\u6167\uFF01\u7206\u4E73\u8B70\u54E1\u5316\u8EAB\u5C0F\u9F8D\u5973\u61C9\u63F4 \u7403\u8FF7\u55E8\u7FFB \u53C8\u51FA\u4E8B\uFF1F\u963F\u7FD4\u906D\u8001\u5A46\u300C\u52D5\u624B\u6253\u982D\u300D \u534A\u591C\u7A81\u7206\u722D\u57F7 \u4E8E\u7F8E\u4EBA\u300C\u59D4\u5C48\u7684\u548C\u5E73\u662F\u548C\u5E73\u300D \u906D\u7832\u8F5F\uFF1A\u59D4\u5C48\u5A5A\u59FB\u4E5F\u662F\u5A5A\u59FB \u65E5\u672C\u7537\u795E\u6084\u4F86\u53F0\u627E17\u5E74\u524D\u6069\u4EBA \u4E16\u7D00\u5408\u7167\u6D41\u51FA\uFF1A\u7B2C\u4E00\u773C\u5C31\u77E5\u6703\u7D05 \u6234\u8CC7\u7A4E\u300C\u6C92\u570B\u65D7\u300D\u6539\u7528\u9910\u5EF3\u6A19\u8A8C \u7DB2\uFF1A\u59D4\u5C48\u7684\u9EA5\u5679\u5679\u4E5F\u662F\u9EA5\u5679\u5679 \u5609\u7FA9\u767E\u5E74\u5E02\u5834\u85CF\u597D\u6599 \u8A79\u59C6\u58EB\u7ADF\u6C92\u5403\u904E\u300C\u9019\u4E00\u5473\u300D \u798F\u539F\u611B\u77E5\u9053\u55CE\uFF1F\u6C5F\u5B8F\u5091\u66B4\u80A5\u8B8A\u5B64\u50FB \u9760\u300C\u4ED6\u300D\u4E00\u53E5\u8A71\u62EF\u6551",
    "publish_date": "2023-05-02 11:30:51"
  },
  {
    "title": "\u0027Godfather of AI\u0027 quits Google to warn of the tech\u0027s dangers",
    "text": "Geoffrey Hinton, who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed \u0022profound risks to society and humanity\u0022. \u0022Look at how it was five years ago and how it is now,\u0022 he was quoted as saying in the piece, which was published on Monday. \u0022Take the difference and propagate it forwards. That\u0027s scary.\u0022 Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. \u0022It is hard to see how you can prevent the bad actors from using it for bad things,\u0022 he told the Times. In 2022, Google and OpenAI -- the start-up behind the popular AI chatbot ChatGPT -- started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. \u0022Maybe what is going on in these systems is actually a lot better than what is going on in the brain,\u0022 he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \u0022takes away the drudge work\u0022 but \u0022might take away more than that\u0022, he told the Times. The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will \u0022not be able to know what is true anymore.\u0022 Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to US media. \u0022As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\u0022 the statement added. \u0022We\u0027re continually learning to understand emerging risks while also innovating boldly.\u0022 In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not \u0022scale this up more until they have understood whether they can control it.\u0022 (AFP)",
    "publish_date": "2023-05-02 11:53:57"
  },
  {
    "title": "Godfather Of AI\u0027 Geoffrey Hinton Quits Google To Talk About Dangers Of Artificial Intelligence",
    "text": "Geoffrey Hinton, often called the \u0027\u0027Godfather of AI\u0027\u0027 on Monday, confirmed that he quit his role at Google last week to speak out about the \u201Cdangers\u201D of the technology he helped develop. In a statement to the New York Times, Mr Hinton, aged 75, announced his resignation from Google, saying he now regretted his work. Mr Hinton tweeted he quit his job at Google, so he can freely speak out about the risks of AI. In his tweet, he wrote, \u0022In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.\u0022 In the NYT today, Cade Metz implies that I left Google so that I could criticize Google. Actually, I left so that I could talk about the dangers of AI without considering how this impacts Google. Google has acted very responsibly.\u2014 Geoffrey Hinton (@geoffreyhinton) May 1, 2023 In a BBC interview on Monday, he said, \u0022I can now just speak freely about what I think the dangers might be. And some of them are quite scary. Right now, as far as I can tell, they\u0027re not more intelligent than us. But I think they soon may be.\u0027\u0027 Notably, Mr Hinton worked for Google for over a decade and was one of the most respected voices in the field. His major AI breakthrough came when working with two graduate students in Toronto in 2012. The trio was able to successfully create an algorithm that could analyze photos and identify common elements, such as dogs and cars, according to the NYT. One of the students who worked on the project with him now works as OpenAI\u0027s chief scientist. His pioneering work on neural networks also shaped artificial intelligence systems, powering many of today\u0027s products like ChatGPT, reported CNN. However, he told the BBC that chatbots could soon overtake the level of information that a human brain holds. \u0022Right now, what we\u0027re seeing is things like GPT-4 eclipses a person in the amount of general knowledge it has and it eclipses them by a long way. In terms of reasoning, it\u0027s not as good, but it does already do simple reasoning. And given the rate of progress, we expect things to get better quite fast. So we need to worry about that,\u0027\u0027 he noted. In an interview with the Times, Mr Hinton expressed his concerns about AI\u0027s potential to eliminate jobs and create a world where many will \u0027\u0027not be able to know what is true anymore.\u0027\u0027 \u0027\u0027It is hard to see how you can prevent the bad actors from using it for bad things,\u0027\u0027 he added. He also expressed concerns about the spread of fake imagery and text. Mr Hinton further cited his age as a reason for his decision. \u0022One is, I\u0027m 75. So it\u0027s time to retire. Another was, I actually want to say some good things about Google. And they\u0027re more credible if I don\u0027t work for Google,\u0027\u0027 he said.",
    "publish_date": "2023-05-02 12:03:56"
  },
  {
    "title": "ChatGPT creator says there\u2019s 50% chance AI ends in \u2018doom\u2019",
    "text": "One of the creators of ChatGPT has added to a growing chorus of researchers warning of the potentially catastrophic consequences of artificial intelligence development. Former OpenAI worker Paul Christiano, who now runs AI research non-profit Alignment Research Center, said he believed there was a significant chance that the technology would lead to the destruction of humanity. The main danger, he claimed, will come when AI systems reach and surpass the cognitive capacity of a human. Dr Christiano predicts there is a \u201C50/50 chance of doom\u201D once this moment arrives. \u201CI tend to imagine something like a year\u2019s transition from AI systems that are a pretty big deal, to kind of accelerating change, followed by further acceleration, et cetera,\u201D he told the Bankless podcast. \u201CI think once you have that view then a lot of things may feel like AI problems because they happen very shortly after you build AI.\u201D He added: \u201CThe most likely way we die involves \u2013 not AI comes out of the blue and kills everyone \u2013 but involves we have deployed a lot of AI everywhere... [And] if for some reason, God forbid, all these AI systems were trying to kill us, they would definitely kill us.\u201D The comments come amid increased concerns surrounding the rapid advancement of artificial intelligence in recent months, with the so-called godfather of AI Geoffrey Hinton quitting Google to sound the alarm about the dangers of AI. Speaking to The New York Times, he said he regretted the work that he had contributed to the field due to the unpredictable future we now face. The idea that this stuff could actually get smarter than people \u2013 a few people believed that \u2013 but most people thought it was way off. And I thought it was way off. I thought it was 30-50 years or even longer away. Obviously I no longer think that,\u201D he said. \u201CI don\u2019t think they should scale this up more until they have understood whether they can control it.\u201D His stance has been praised by other researchers, with AnthropicAI\u2019s Catherine Olsson saying it may encourage others within the field to speak up. \u201CIn college I stopped eating meat, on the spot, when a friend asked why I hadn\u2019t yet. Social checks on our ethics can be so influential,\u201D she tweeted. \u201CI often think about when I would quit Anthropic or leave AI entirely. I encourage others to. I can already tell this move will influence me.\u201D Other prominent figures have also urged AI firms to pause development on advanced systems, most recently through an open letter signed by thousands of experts that urged governments to step in if artificial intelligence development was not paused for at least six months. Among the signatories was Elon Musk, who has frequently spoken about the existential threat posed by AI. The tech billionaire, who co-founded OpenAI, tweeted on Monday: \u201CEven benign dependency on AI/Automation is dangerous to civilization if taken so far that we eventually forget how the machines work.\u201D",
    "publish_date": "2023-05-02 12:36:17"
  },
  {
    "title": "This company adopted AI. Here\u0027s what happened to its human workers.",
    "text": "Lately, it\u0027s felt like technological change has entered warp speed. Companies like OpenAI and Google have unveiled new Artificial Intelligence systems with incredible capabilities, making what once seemed like science fiction an everyday reality. It\u0027s an era that is posing big, existential questions for us all, about everything from literally the future of human existence to \u2014 more to the focus of Planet Money \u2014 the future of human work. \u0022Things are changing so fast,\u0022 says Erik Brynjolfsson, a leading, technology-focused economist based at Stanford University. Back in 2017, Brynjolfsson published a paper in one of the top academic journals, Science, which outlined the kind of work that he believed AI was capable of doing. It was called \u0022What Can Machine Learning Do? Workforce Implications.\u0022 Now, Brynjolfsson says, \u0022I have to update that paper dramatically given what\u0027s happened in the past year or two.\u0022 Sure, the current pace of change can feel dizzying and kinda scary. But Brynjolfsson is not catastrophizing. In fact, quite the opposite. He\u0027s earned a reputation as a \u0022techno-optimist.\u0022 And, recently at least, he has a real reason to be optimistic about what AI could mean for the economy. Last week, Brynjolfsson, together with MIT economists Danielle Li and Lindsey R. Raymond, released what is, to the best of our knowledge, the first empirical study of the real-world economic effects of new AI systems. They looked at what happened to a company and its workers after it incorporated a version of ChatGPT, a popular interactive AI chatbot, into workflows. What the economists found offers potentially great news for the economy, at least in one dimension that is crucial to improving our living standards: AI caused a group of workers to become much more productive. Backed by AI, these workers were able to accomplish much more in less time, with greater customer satisfaction to boot. At the same time, however, the study also shines a spotlight on just how powerful AI is, how disruptive it might be, and suggests that this new, astonishing technology could have economic effects that change the shape of income inequality going forward. The Rise Of Cyborg Customer Service Reps The story of this study starts a few years ago, when an unnamed Fortune 500 company \u2014 Brynjolfsson and his colleagues have not gotten permission to disclose its identity \u2014 decided to adopt an earlier version of OpenAI\u0027s ChatGPT. This AI system is an example of what computer scientists call \u0022generative AI\u0022 and also a \u0022Large Language Model,\u0022 systems that have crunched a ton of data \u2014 especially text \u2014 and learned word patterns that enable them to do things like answer questions and write instructions. This company provides other companies with administrative software. Think like programs that help businesses do accounting and logistics. A big part of this company\u0027s job is helping its customers, mostly small businesses, with technical support. The company\u0027s customer support agents are based primarily in the Philippines, but also the United States and other countries. And they spend their days helping small businesses tackle various kinds of technical problems with their software. Think like, \u0022Why am I getting this error message?\u0022 or like, \u0022Help! I can\u0027t log in!\u0022 Instead of talking to their customers on the phone, these customer service agents mostly communicate with them through online chat windows. These troubleshooting sessions can be quite long. The average conversation between the agents and customers lasts about 40 minutes. Agents need to know the ins and outs of their company\u0027s software, how to solve problems, and how to deal with sometimes irate customers. It\u0027s a stressful job, and there\u0027s high turnover. In the broader customer service industry, up to 60 percent of reps quit each year. Facing such high turnover rates, this software company was spending a lot of time and money training new staffers. And so, in late 2020, it decided to begin using an AI system to help its constantly churning customer support staff get better at their jobs faster. The company\u0027s goal was to improve the performance of their workers, not replace them. Now, when the agents look at their computer screens, they don\u0027t only see a chat window with their customers. They also see another chat window with an AI chatbot, which is there to help them more effectively assist customers in real time. It advises them on what to potentially write to customers and also provides them with links to internal company information to help them more quickly find solutions to their customers\u0027 technical problems. This interactive chatbot was trained by reading through a ton of previous conversations between reps and customers. It has recognized word patterns in these conversations, identifying key phrases and common problems facing customers and how to solve them. Because the company tracks which conversations leave its customers satisfied, the AI chatbot also knows formulas that often lead to success. Think, like, interactions that customers give a 5 star rating. \u0022I\u0027m so sorry you\u0027re frustrated with error message 504. All you have to do is restart your computer and then press CTRL-ALT-SHIFT. Have a blessed day!\u0022 Equipped with this new AI system, the company\u0027s customer support representatives are now basically part human, part intelligent machine. Cyborg customer reps, if you will. Lucky for Brynjolfsson, his colleagues, and econ nerds like us at Planet Money, this software company gave the economists inside access to rigorously evaluate what happened when customer service agents were given assistance from intelligent machines. The economists examine the performance of over 5,000 agents, comparing the outcomes of old-school customer reps without AI against new, AI-enhanced cyborg customer reps. What Happened When This Company Adopts AI The economists\u0027 big finding: after the software company adopted AI, the average customer support representative became, on average, 14 percent more productive. They were able to resolve more customer issues per hour. That\u0027s huge. The company\u0027s workforce is now much faster and more effective. They\u0027re also, apparently, happier. Turnover has gone down, especially among new hires. Not only that, the company\u0027s customers are more satisfied. They give higher ratings to support staff. They also generally seem to be nicer in their conversations and are less likely to ask to speak to an agent\u0027s supervisor. So, yeah, AI seems to really help improve the work of the company\u0027s employees. But what\u0027s even more interesting is that not all employees gained equally from using AI. It turns out that the company\u0027s more experienced, highly skilled customer support agents saw little or no benefit from using it. It was mainly the less experienced, lower-skilled customer service reps who saw big gains in their job performance. \u0022And what this system did was it took people with just two months of experience and had them performing at the level of people with six months of experience,\u0022 Brynjolfsson says. \u0022So it got them up the learning curve a lot faster \u2014 and that led to very positive benefits for the company.\u0022 Brynjolfsson says these improvements make a lot of sense when you think about how the AI system works. The system has analyzed company records and learned from highly rated conversations between agents and customers. In effect, the AI chatbot is basically mimicking the company\u0027s top performers, who have experience on the job. And it\u0027s pushing newbies and low performers to act more like them. The machine has essentially figured out the recipe for the magic sauce that makes top performers so good at their jobs, and it\u0027s offering that recipe for the workers who are less good at their jobs. That\u0027s great news for the company and its customers, as well as the company\u0027s low performers, who are now better at their jobs. But, Brynjolfsson says, it also raises the question: should the company\u0027s top performers be getting paid even more? After all, they\u0027re now not only helping the customers they directly interact with. They\u0027re now also, indirectly, helping all the company\u0027s customers, by modeling what good interactions look like and providing vital source material for the AI. \u0022It used to be that high-skilled workers would come up with a good answer and that would only help them and their customer,\u0022 Brynjolfsson says. \u0022Now that good answer gets amplified and used by people throughout the organization.\u0022 The Big Picture While Brynjolfsson is cautious, noting that this is one company in one study, he also says one of his big takeaways is that AI could make our economy much more productive in the near future. And that\u0027s important. Productivity gains \u2014 doing more in less time \u2014 are a crucial component for rising living standards. After years of being disappointed by lackluster productivity growth, Brynjolfsson is excited by this possibility. Not only does AI seem to be delivering productivity gains, it seems to deliver them pretty fast. \u0022And the fact that we\u0027re getting some really significant benefits suggests that we could have some big benefits over the next few years or decades as these systems are more widely used,\u0022 Brynjolfsson says. When machines take over more work and boost our productivity, Brynjolfsson says, that\u0027s generally a great thing. It means that society is getting richer, that the economic pie is getting larger. At the same time, Brynjolfsson says, there are no guarantees about how this pie will be distributed. Even when the pie gets bigger, there are people who could see their slice get smaller or even disappear. \u0022It\u0027s very clear that it\u0027s not automatic that the bigger pie is evenly shared by everybody,\u0022 Brynjolfsson says. \u0022We have to put in place policies, whether it\u0027s in tax policy or the strategy of companies like this one, which make sure the gains are more widely shared.\u0022 Higher productivity is a really important finding. But what\u0027s probably most fascinating about this study is that it adds to a growing body of evidence that suggests that AI could have a much different effect on the labor market than previous waves of technological change. For the last few decades, we\u0027ve seen a pattern that economists have called \u0022skill-biased technological change.\u0022 The basic idea is that so-called \u0022high-skill\u0022 office workers have disproportionately benefited from the use of computers and the internet. Things like Microsoft Word and Excel, Google, and so on have made office workers and other high-paid professionals much better at their jobs. Meanwhile, however, so-called \u0022low-skill\u0022 workers, who often work in the service industry, have not benefited as much from new technology. Even worse, this body of research finds, new technology killed many \u0022middle-skill\u0022 jobs that once offered non-college-educated workers a shot at upward mobility and a comfortable living in the middle class. In this previous technological era, the jobs that were automated away were those that focused on doing repetitive, \u0022routine\u0022 tasks. Tasks that you could provide a machine with explicit, step-by-step instructions how to do. It turned out that, even before AI, computer software was capable of doing a lot of secretarial work, data entry, bookkeeping, and other clerical tasks. And robots, meanwhile, were able to do many tasks in factories. This killed lots of middle class jobs. The MIT economist David Autor has long studied this phenomenon. He calls it \u0022job polarization\u0022 and a \u0022hollowing out\u0022 of the middle class. Basically, the data suggests that the last few decades of technological change was a major contributor to increasing inequality. Technology has mostly boosted the incomes of college-educated and skilled workers while doing little for \u2014 and perhaps even hurting \u2014 the incomes of non-college-educated and low-skilled workers. Upside Downside But, what\u0027s interesting is, as Brynjolfsson notes, this new wave of technological change looks like it could be pretty different. You can see it in his new study. Instead of experienced and skilled workers benefiting mostly from AI technology, it\u0027s the opposite. It\u0027s the less experienced and less skilled workers who benefit the most. In this customer support center, AI improved the know-how and intelligence of those who were new at the job and those who were lower performers. It suggests that AI could benefit those who were left behind in the previous technological era. \u0022And that might be helpful in terms of closing some of the inequality that previous technologies actually helped amplify,\u0022 Brynjolfsson says. So one benefit of intelligence machines is \u2014 maybe \u2014 they will improve the know-how and smarts of low performers, thereby reducing inequality. But \u2014 and Brynjolfsson seemed a bit skeptical about this \u2014 it\u0027s also possible that AI could lower the premium on being experienced, smart, or knowledgeable. If anybody off the street can now come in and \u2014 augmented by a machine \u2014 start doing work at a higher level, maybe the specialized skills and intelligence of people who were previously in the upper echelon become less valuable. So, yeah, AI could reduce inequality by bringing the bottom up. But it could also reduce inequality by bringing the top and middle down, essentially de-skilling a whole range of occupations, making them easier for anyone to do and thus lowering their wage premium. Of course, it\u0027s also possible that AI could end up increasing inequality even more. For one, it could make the Big AI companies, which own these powerful new systems, wildly rich. It could also empower business owners to replace more and more workers with intelligent machines. And it could kill jobs for all but the best of the best in various industries, who keep their jobs because maybe they\u0027re superstars or because maybe they have seniority. Then, with AI, these workers could become much more productive, and so their industries might need fewer of these types of jobs than before. The effects of AI, of course, are still very much being studied \u2014 and these systems are evolving fast \u2014 so this is all just speculation. But it does look like AI may have different effects than previous technologies, especially because machines are now more capable of doing \u0022non-routine\u0022 tasks. Previously, as stated, it was only \u0022routine\u0022 tasks that proved to be automatable. But, now, with AI, you don\u0027t have to program machines with specific instructions. They are much more capable of figuring out things on the fly. And this machine intelligence could upend much of the previous thinking on which kinds of jobs will be affected by automation. Next week, in the Planet Money newsletter, we speak with MIT\u0027s David Autor, who pioneered much of the economic thinking about technological change, automation, inequality, and upward mobility in the past few decades. What\u0027s he thinking now? Stay tuned!",
    "publish_date": "2023-05-02 12:31:01"
  },
  {
    "title": "AI Is Being Used To Generate Whole Spam Sites - WorldNewsEra",
    "text": "AI chatbots are being used to generate news stories and blog posts for online content farms in the hopes of attracting a trickle of ad revenue from the stray clicks of web users. Experts have been warning for years that such AI-generated content farms will soon become commonplace, but the wider availability of tools like OpenAI\u2019s ChatGPT has now made these warnings a reality. NewsGuard, a for-profit organization that rates the trustworthiness of news sites, highlighted the problem in a recent report identifying 49 sites \u201Cthat appear to be almost entirely written by artificial intelligence software.\u201D Said NewsGuard: The websites, which often fail to disclose ownership or control, produce a high volume of content related to a variety of topics, including politics, health, entertainment, finance, and technology. Some publish hundreds of articles a day. Some of the content advances false narratives. Nearly all of the content features bland language and repetitive phrases, hallmarks of artificial intelligence. The sites identified by the organization often have generic names (like Biz Breaking News and Market News Reports) and are stuffed with programmatic advertising that\u2019s bought and sold automatically. They attribute news stories to generic or fake authors, and much of the content appears to be summaries or re-writes of stories from established sites like CNN. Most of the sites are not spreading misinformation, said NewsGuard, but some publish blatant falsehoods. For example, in early April, a content farm named CelebritiesDeaths.com posted a story claiming that Joe Biden had died. This Biden story might briefly fool a reader, though is soon revealed to be a fake. The second paragraph contains an error message from the chatbot that was asked to create the text and was evidently copy and pasted into the website without any oversight. \u201CI\u2019m sorry, I cannot complete this prompt as it goes against OpenAI\u2019s use case policy on generating misleading content,\u201D says the story. \u201CIt is not ethical to fabricate news about the death of someone, especially someone as prominent as a President.\u201D NewsGuard says it used such tell-tale errors to find all the sites in its report. As The Verge has previously reported, searching for phrases like \u201CAs an AI language model\u201D often reveals where chatbots are being used to generate fake reviews and other cheap text content. NewsGuard also verified the text on these sites was AI-generated using detection tools like GPTZero (although it\u2019s worth noting such tools are not always reliable). Noah Giansiracusa, an associate professor of data science who\u2019s written about fake news, told Bloomberg that the creators of such sites were experimenting \u201Cto find what\u2019s effective\u201D and would continue to spin up content farms given the cheap costs of production. \u201CBefore, it was a low-paid scheme. But at least it wasn\u2019t free,\u201D Giansiracusa told the outlet. At the same time, as Giansiracusa noted, many established news outlets are also experimenting with using AI to lower the production costs of content \u2014 sometimes with undesirable outcomes. When CNET started using AI to help write posts, a review of the system\u2019s output found errors in more than half the published stories. The pressure to use AI is increasing at a time when online news is facing a wave of layoffs and shut-downs. You can read the full report from NewsGuard here.",
    "publish_date": "2023-05-02 13:05:22"
  },
  {
    "title": "A faster Dall-E? How to use Bing Image Creator for blogs, presentations, or  just for fun",
    "text": "Before the ChatGPT revolution, most people encountered generative AI through image synthesis tools \u2013 namely Dall-E. While early implementations were slow and did not always return the output you had in mind, newer versions of Dall-E have improved remarkably and can often create images that look indistinguishable from reality. Having reached a level of refinement, the OpenAI product was integrated into the offerings of OpenAI\u2019s largest investor, Microsoft. Microsoft last month announced Image Creator, which does exactly what its name suggests with nothing but simple text prompts. So how can you use it and what are its top features? Let\u2019s find out. What is Bing Image Creator and how does it work? Bing Image Creator is a generative AI model powered by an advanced version of the Dall-E model, which can produce realistic and diverse images from natural language descriptions. You can use Bing Image Creator to create images for various purposes, such as newsletters, blogs, presentations, or just for fun. Thanks to copious amounts of training on billions of samples picked from all over the internet, tools like Image Creator can generate images that are so realistic-looking that you\u2019ll often find yourself squinting, trying to spot irregularities that can give the image away. Take the Pope in Balenciaga puffer jacket image that went viral fooling millions, for example. While that image was created by Midjourney v5 and that tool is on a whole another level, Bing Image Creator isn\u2019t far behind. You can squeeze high-quality images out of it too, provided that your prompts are detailed enough. Outputs can also be instructed to follow a particular art style \u2013 abstract, clipart, comics, photorealistic, you name it. Is Bing Image Creator free? Yes, unlike Dall-E 2 which offers a very limited amount of credits before you\u2019re forced to \u2018recharge,\u2019 Bing Image Creator is completely free to use. You are given 100 \u2018boosts\u2019 that replenish on a weekly basis, which have likely been put in place to limit server tiedown. When you have these boosts, image generation is much faster. After you run out, you can still see the results of your prompt, but the process is a lot slower. There is no real money payment involved to recharge your boosts, but you can still get some more by converting your Microsoft Rewards points. The option to recharge the boosts doesn\u2019t show up until you run out. How to use Bing Image Creator Bing Image Creator has no waitlist involved so you can get started with it immediately. The only thing you\u2019d need is a Microsoft account. Here\u2019s a step-by-step tutorial on how you can use the AI tool: 1. Launch the Bing Image Creator website and hit the Join \u0026 Create button. 2. You\u2019ll be asked to log in to your Microsoft account here. If you don\u2019t have one, simply sign up for a new account. 3. Next, enter a description of an image you\u2019d like to see and hit the Create button. 4. Wait while the AI creates your image. 5. Once the process is done, you will see four images reflecting your prompt. You can select one of them or ask for more options. 6. You can also modify the image by using different styles, such as realistic, sketch, or comic. 7. You can save or share the image by clicking on the three dots icon on the top right corner of the image. 8. Your previous creations can be found under the Creations tab. Do note that the largest size you can get from Bing is 1024x1024 pixels and that the aspect ratio will always be 1:1. You can also access Bing Image Creator from Bing Chat - just start your prompt with \u201CGenerate an image of\u201D followed by the description of your image. Bing Chat can alternatively be used via SwiftKey keyboard and Bing mobile apps. How to create better prompts Image Creator is different from searching for an image on Bing. Here, the more detailed you are, the better chances you have at getting the tool to generate an image you have in mind. Think of the tool as a human artist who needs precise instructions on what to draw for you. Prompt engineering is a skill in and of itself, so you might want to get creative, adding details, adjectives, locations, and artistic styles. If you need inspiration, try browsing the Explore ideas tab and taking a look at the prompts that created each image therein. Here\u2019s an example of what a detailed prompt looks like: instead of a text prompt for \u0022panda,\u0022 try submitting a prompt for \u0022a panda bear basking in the sun chewing bamboo, digital art\u0022.",
    "publish_date": "2023-05-02 13:51:56"
  },
  {
    "title": "Can AI read your mind? Scientists use ChatGPT-like tech to turn people\u2019s thoughts into text in real-time",
    "text": "It\u2019s 2023, and the world is rapidly drifting away from traditionally known methods of dream interpretation. With the advent of artificial intelligence newer methods of reading the human mind are at play. In March, it was reported that Japanese scientists recreated high-resolution images from scans of brain activity using stable diffusion, now it seems there is another breakthrough in the offing. A team of scientists from the University of Texas at Austin has developed an AI model that can read your thoughts. The noninvasive AI system known as semantic decoder lays emphasis on translating brain activity into a stream of text according to the peer-reviewed study published in the journal Nature Neuroscience. The research was led by Jerry Tang, a doctoral student in computer science; Alex Huth, an assistant professor of neuroscience and computer science at UT Austin. The study conducted by Tang and Huth is based partly on a transformer model which is similar to the one that powers Google Bard and OpenAI\u2019s ChatGPT. With their latest innovation, scientists are hopeful that it can be of assistance to people with paralysis or some form of disability. The newly developed tech is essentially an AI-based decoder that can translate brain activities into a stream of text. This means now AI will allow a person\u2019s thoughts to be read in a non-invasive way, something that has never been attempted in the history of neuroscience or medical science in general. As part of the study, three people were assigned to MRI machines and were asked to listen to stories. In what can be called a major breakthrough, scientists claim that they produced the text of the participants\u2019 thoughts without the help of any brain implant. It is to be noted that the mind-reading technology captured the main points of their thoughts and did not exactly replicate their thoughts in their entirety. \u201CFor a noninvasive method, this is a real leap forward compared to what\u2019s been done before, which is typically single words or short sentences. We\u2019re getting the model to decode continuous language for extended periods of time with complicated ideas,\u201D Huth was quoted as saying in a report published on the UT Texas website. According to scientists, the AI system can generate a stream of text when a participant listens to or imagines a story. This according to the researchers is possible once the AI system is fully trained. Researchers essentially deployed a technology like ChatGPT to interpret the thoughts of people while they were watching silent films or when they imagined themselves to be telling a story. The new study has also raised concerns about mental privacy. Apart from Tang and Huth, Amanda LeBel a former research assistant at the Huth Lab and Shailee Jain, a computer science graduate at UT Austin, are co-authors of the study.",
    "publish_date": "2023-05-02 13:51:56"
  },
  {
    "title": "IBM to pause hiring in plan to replace 7,800 jobs with AI",
    "text": "Hiring specifically in back-office functions such as human resources will be suspended or slowed and 30% of non-customer-facing roles could be replaced by AI. International Business Machines Corp expects to pause hiring for roles as roughly 7,800 jobs could be replaced by Artificial Intelligence (AI) in the coming years, CEO Arvind Krishna told Bloomberg News on Monday. Hiring specifically in back-office functions such as human resources will be suspended or slowed, Krishna said, adding that 30% of non-customer-facing roles could be replaced by AI and automations in five years. His comment comes at a time when AI has caught the imagination of people around the world after the launch of Microsoft Corp-backed OpenAI\u0027s viral chatbot, ChatGPT, in November last year. The reduction could include not replacing roles vacated by attrition, the PC-maker told the publication. IBM did not immediately respond to a Reuters request for comment.",
    "publish_date": "2023-05-02 13:51:56"
  },
  {
    "title": "\u2018Godfather of Artificial Intelligence\u2019 cautions against its impact: How Geoffrey Hinton helped the development of AI",
    "text": "British researcher and academic Geoffrey Hinton, who is often referred to as one of the godfathers of artificial intelligence, has ended his nearly decade-long association with Google to independently warn against the dangers of further developing AI without analysing its impact. Following an interview with The New York Times, the Canada-based research scholar said in a tweet on Monday (May 1), \u201CI left [Google] so that I could talk about the dangers of AI without considering how this impacts Google,\u201D adding that the company has acted \u201Cvery responsibly\u201D in the quest towards AI development. Google began testing this year for its own AI chatbot, named Bard. How did Hinton go from being an enthusiastic proponent \u2013 and pioneer \u2013 of the technology, to becoming a critic? We take a look. Who is Geoffrey Hinton? Hinton, 75, is a UK-born researcher and academic. He began his career with a BA in Experimental Psychology from University of Cambridge in 1970 and followed it with a PhD in Artificial Intelligence from the University of Edinburgh in Scotland in 1978. He has also served as a faculty member in the Computer Science department at Carnegie-Mellon University in Pennsylvania, USA. In the 1980s, as most AI research in the United States was funded by the US military, Hinton said he was opposed to contributing to research for possibly using AI in the battlefield. This prompted his move to Canada. He then became a fellow of the Canadian Institute for Advanced Research and moved to the Department of Computer Science at the University of Toronto. At the university, he is an emeritus distinguished professor and has written numerous academic papers on machine learning. Since 2013, he has been working half-time for Google as a VP Engineering fellow. What is Hinton\u2019s contribution to the development of AI? In a Coursera course that Hinton taught, he explained that normally, a computer program or code is written by hand for each specific task to be completed by a machine (like showing the user a photo or a particular text). But in machine learning, lots of examples are collected and fed into the machine, in order to teach it to identify the correct output for a given input. \u201CA machine learning algorithm then takes these examples and produces a program that does the job,\u201D he wrote. For example, a machine can be fed thousands of images and then trained to identify what different animals or plants look like. The NYT interview notes that in 1972, as a graduate student at the University of Edinburgh, Hinton \u201Cembraced an idea called a neural network\u2026 a mathematical system that learns skills by analyzing data.\u201D He said the aim was to solve practical problems through novel learning algorithms \u2013 inspired by how the human brain works with its networks of neurons or nerve cells. The Association for Computing Machinery, which awarded the Turing Award for contributions to computer science, explained in 2018 that the term \u2018neural networks\u2019 refers to \u201Csystems composed of layers of relatively simple computing elements called \u2018neurons\u2019 that are simulated in a computer.\u201D These \u201Cneurons\u201D only loosely resemble the neurons in the human brain, and influence one another. A breakthrough came in 2012, when Hinton and two of his students in Toronto, Ilya Sutskever and Alex Krishevsky, \u201Cbuilt a neural network to analyse thousands of photos and teach itself to identify common objects\u201D, noted the NYT report. Sutskever went on to become chief scientist and co-founder at OpenAI. Later on, Google spent $44 million to acquire a company called DNNResearch, founded by the trio. It incorporated elements from this into its social media website Google\u002B, for image search. Hinton said in an IIT-Bombay commencement address in 2021, that neural networks are the best way to do speech recognition and to classify objects in images, and the best way to do machine translation. \u0022Neural networks with about a trillion parameters are so good at predicting the next word in a sentence, that they can be used to generate quite complicated stories or to answer a wide variety of questions\u2026 These big networks are still about 100 times smaller than the human brain, but they already raise very interesting questions about the nature of human intelligence,\u201D he said. Hinton\u2019s profile by the UK\u2019s Royal Society notes that the development of artificial neural networks \u201Cmay well be the start of autonomous intelligent brain-like machines\u201D. What has he Hinton in his criticism of AI? In his interview with the NYT, Hilton expressed concern on three major counts. First, given tools like ChatGPT scour the internet for information and create a final product, he believes the internet might soon be flooded with false photos, videos and text, etc. and the average person will \u201Cnot be able to know what is true anymore.\u201D Second, that over time it may lead to machines taking over human jobs. \u201CIt takes away the drudge work,\u201D he said, adding \u201CIt might take away more than that.\u201D He also told the BBC, \u0022I\u0027ve come to the conclusion that the kind of intelligence we\u0027re developing is very different from the intelligence we have. We\u0027re biological systems and these are digital systems.\u201D He says this means a vast difference in terms of capacity, where they can instantaneously process large amounts of data. In the future, such data could be used by \u201Cbad actors\u201D as they wish. And Hinton is not alone in voicing these fears. In early April, more than 1,000 technology leaders and researchers, including Apple co-founder Steve Wozniak and Tesla founder Elon Musk, signed an open letter calling for a six-month pause on the developing AI systems further, saying they \u201Cprofound risks to society and humanity.\u201D They also raised concerns over misinformation and said companies must develop a set of shared safety protocols for advanced AI design and development at this time that can be overseen by independent outside experts. With a pause, the letter said, a proper framework with a legal structure and foolproofing is proposed, including watermarking systems \u201Cto help distinguish real from synthetic\u201D should be created.",
    "publish_date": "2023-05-02 13:52:33"
  },
  {
    "title": "Samsung bans use of A.I. like ChatGPT for employees after misuse of the chatbot",
    "text": "Samsung is restricting the use of so-called generative artificial intelligence tools such as ChatGPT for employees after the company discovered such services were being misused. The South Korean technology giant confirmed to CNBC Tuesday that it is temporarily restricting the use of generative AI through the company\u0027s personal computers. Employees of one of Samsung\u0027s biggest divisions were informed of the move in a memo at the end of April after there had been cases of misuse of the technology. Bloomberg reported on Tuesday that some staff had uploaded sensitive code to ChatGPT. ChatGPT is a viral AI chatbot that is trained on huge amounts of data and is able to generate response to user queries. It is a form of so-called generative AI. Samsung does not have its own generative AI product yet. ChatGPT is developed by U.S. firm OpenAI which is backed by Microsoft while other generative AI products include Google\u0027s Bard. Inputting sensitive company data into these foreign-owned services could be a concern to companies worried about leaks of crucial information.",
    "publish_date": "2023-05-02 13:53:03"
  },
  {
    "title": "Cloud content storage platform Box adds AI features",
    "text": "Business software company Box, Inc said on Tuesday it will introduce new artificial intelligence features such as analyzing information across customer contracts or generating customized marketing content. Generative AI, which the tech industry has embraced, has the ability to synthesize large amounts of data and write human-like text. Box customers testing the product have already experienced \u201Crevolutionary\u201D productivity gains, Box CEO Aaron Levie told Reuters. He cited a Box customer whose employees were reviewing tens and thousands of files manually. The new AI features were able to do it automatically \u201Cwithin a matter of seconds.\u201D Box partnered with OpenAI, the AI startup backed by Microsoft Corp behind the chatbot sensation ChatGPT, to build the AI enhancements. Box has implemented OpenAI\u2019s technology in a way that restricts OpenAI\u0027s responses to only answering with information from within the customer\u2019s existing documents, dramatically reducing the likelihood of an incorrect response, Levie said. Incorrect responses, known as hallucinations, have plagued generative AI products like ChatGPT. While Box is initially building its AI offering with OpenAI\u2019s technology, the goal is to allow customers to select among different AI providers, he said. Box said the new AI features include the ability to summarize financial documents, surface insights from customer surveys and tailor onboarding documents to a specific customer\u0027s needs.",
    "publish_date": "2023-05-02 14:14:56"
  },
  {
    "title": "Samsung Bans Use Of A.I. Like ChatGPT For Employees After Misuse Of The Chatbot - WorldNewsEra",
    "text": "Samsung is restricting the use of so-called generative artificial intelligence tools such as ChatGPT for employees after the company discovered such services were being misused. The South Korean technology giant confirmed to CNBC Tuesday that it is temporarily restricting the use of generative AI through the company\u2019s personal computers. Employees of one of Samsung\u2019s biggest divisions were informed of the move in a memo at the end of April after there had been cases of misuse of the technology. Bloomberg reported on Tuesday that some staff had uploaded sensitive code to ChatGPT. ChatGPT is a viral AI chatbot that is trained on huge amounts of data and is able to generate response to user queries. It is a form of so-called generative AI. Samsung does not have its own generative AI product yet. ChatGPT is developed by U.S. firm OpenAI which is backed by Microsoft while other generative AI products include Google\u2019s Bard. Inputting sensitive company data into these foreign-owned services could be a concern to companies worried about leaks of crucial information.",
    "publish_date": "2023-05-02 14:37:16"
  },
  {
    "title": "Google AI pioneer says quits to speak freely about technology\u2019s \u2018dangers\u2019",
    "text": "A pioneer of artificial intelligence said he quit Google to speak freely about the technology\u2019s dangers, after realising computers could become smarter than people far sooner than he and other experts had expected. \u201CI left so that I could talk about the dangers of AI without considering how this impacts Google,\u201D Geoffrey Hinton wrote on Twitter. In an interview with the New York Times, Hinton said he was worried about AI\u2019s capacity to create convincing false images and texts, creating a world where people will \u201Cnot be able to know what is true anymore\u201D. \u201CIt is hard to see how you can prevent the bad actors from using it for bad things,\u201D he said. The technology could quickly displace workers, and become a greater danger as it learns new behaviours. \u201CThe idea that this stuff could actually get smarter than people \u2014 a few people believed that,\u201D he told the New York Times. \u201CBut most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\u201D In his tweet, Hinton said Google itself had \u201Cacted very responsibly\u201D and denied that he had quit so that he could criticise his former employer. Google, part of Alphabet Inc., did not immediately reply to a request for comment from Reuters. The Times quoted Google\u2019s chief scientist, Jeff Dean, as saying in a statement: \u201CWe remain committed to a responsible approach to A.I. We\u2019re continually learning to understand emerging risks while also innovating boldly.\u201D Since Microsoft-backed MSFT.O startup OpenAI released ChatGPT in November, the growing number of \u201Cgenerative AI\u201D applications that can create text or images have provoked concern over the future regulation of the technology. \u201CThat so many experts are speaking up about their concerns regarding the safety of AI, with some computer scientists going as far as regretting some of their work, should alarm policymakers,\u201D said Dr Carissa Veliz, an associate professor in philosophy at the University of Oxford\u2019s Institute for Ethics in AI. \u201CThe time to regulate AI is now.\u201D (Reuters)",
    "publish_date": "2023-05-02 15:55:56"
  },
  {
    "title": "\u2018Godfather of AI\u2019 quits Google to warn of the tech\u2019s dangers \u2013 The Frontier Post",
    "text": "NEW YORK (AFP): A computer scientist often dubbed \u201Cthe godfather of artificial intelligence\u201D has quit his job at Google to speak out about the dangers of the technology, US media reported Monday. Geoffrey Hinton, who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed \u201Cprofound risks to society and humanity\u201D. \u201CLook at how it was five years ago and how it is now,\u201D he was quoted as saying in the piece, which was published on Monday. \u201CTake the difference and propagate it forwards. That\u2019s scary.\u201D Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. \u201CIt is hard to see how you can prevent the bad actors from using it for bad things,\u201D he told the Times. In 2022, Google and OpenAI \u2013 the start-up behind the popular AI chatbot ChatGPT \u2013 started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. \u201CMaybe what is going on in these systems is actually a lot better than what is going on in the brain,\u201D he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \u201Ctakes away the drudge work\u201D but \u201Cmight take away more than that\u201D, he told the Times. The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will \u201Cnot be able to know what is true anymore.\u201D Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to US media. \u201CAs one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\u201D the statement added. \u201CWe\u2019re continually learning to understand emerging risks while also innovating boldly.\u201D In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not \u201Cscale this up more until they have understood whether they can control it.\u201D",
    "publish_date": "2023-05-02 15:17:47"
  },
  {
    "title": "Samsung Imposes Ban On ChatGPT And Other AI Tools After Data Leak",
    "text": "Samsung has banned its employees from using ChatGPT and other Generative AI tools on their phones, tablets, and computers after a data leak case. The use of generative AI technologies like ChatGPT by Samsung staff members is forbidden. The South Korean consumer electronics giant is reportedly sending out a note to its staff members advising them of the new policy. The action was taken as a result of Samsung learning that one of its employees had uploaded private code to ChatGPT in April of this year, which put a stop to the adoption of such technology in the work environment. The business is worried that information sent to artificial intelligence systems like Google Bing and Google Bard is retained on external servers, making it harder to retrieve and remove, and could wind up being revealed to other users, according to the memo. According to a Bloomberg narrative, Samsung polled its employees last month on their usage of AI tools, and 65% of those polled said that doing so posed a security risk. Samsung informed personnel that demand in generative AI systems like ChatGPT has been rising both within and externally. \u201CWhile this interest focuses on the practicality and effectiveness of these kinds of platforms, there are also rising concerns about the security threats posed by generative AI.\u201D When OpenAI released ChatGPT in November 2022, generative AI technologies first gained attention. Italy likewise forbade the use of ChatGPT due to privacy concerns, though this has since changed. A number of Wall Street firms, including JPMorgan Chase \u0026 Co., Bank of America Corp., and Citigroup Inc. either banned or restricted its use as a result of the chatbot service\u2019s surge in interest in the technology. With the implementation of the new policy, Samsung has forbidden its employees from using generative AI systems on company-owned laptops, tablets, and mobile phones, and also on its internal networks. According to the message from Samsung, failing to follow the security guidelines \u201Cmay result in an infringement or vulnerability of company data leading to disciplinary proceedings up to and including termination of employment.\u201D Also Read:- ED Conducts Raid At Byju\u2019s CEO\u2019s Premises Over FEMA Violations The memo stated, \u201CHQ is looking into security measures to establish a safe environment for employees to use generative AI to boost their productivity and efficiency. However, we are temporarily prohibiting the use of generative AI until these precautions are prepared.\u201D The consumer electronics products the corporation sells, such Android smartphones and Windows laptops, are unaffected by the new regulations.",
    "publish_date": "2023-05-02 10:05:33"
  },
  {
    "title": "Thousands of IBM staff at risk from AI, boss warns",
    "text": "Artificial intelligence will replace thousands of office workers at IBM within five years, the IT giant\u2019s chief executive has predicted.Arvind Krishna said IBM would suspend or slow down hiring in back office areas such as human resources as many tasks are replaced by automation.Around 26,000 people currently have roles in these positions at IBM, which employed around 288,000 staff at the end of last year.\u201CI could easily see 30pc of that getting replaced by AI and automation over a five-year period,\u201D Mr Krishna told Bloomberg. This would amount to around 7,800 staff. IBM said much of this reduction would come through attrition, rather than widespread layoffs.Mr Krishna\u2019s prediction is among the first signs that a new wave of artificial intelligence programs are disrupting the job market, especially among white collar workers.The rapid rise of \u201Clarge language models\u201D such as ChatGPT, developed by the San Francisco company OpenAI, has led to renewed fears of job losses. The bots are capable of summarising large quantities of text, writing emails and essays, and helping to produce computer code.",
    "publish_date": "2023-05-02 16:15:11"
  },
  {
    "title": "How AI Could Lead to Inaccurate Breast Cancer Exams",
    "text": "You might not realize it, but you can probably be easily duped by AI. This isn\u2019t a knock on you; it\u2019s just human nature. In fact, a study published in April found that OpenAI\u2019s ChatGPT could even greatly influence how you answer the Trolley Problem without you even realizing it. With the recent boom of AI, we\u2019re still grappling with how exactly these bots might impact society\u2014and a new study found that they could cause harm in hospital rooms. Researchers from Germany and the Netherlands published a paper Tuesday in the journal Radiology that found AI decision-making systems may impair the decisions of radiologists evaluating mammograms for signs of breast cancer\u2014regardless of the physician\u2019s level of expertise. The findings underscore how automation bias\u2014the penchant for people to favor the decisions of automated decision-making systems\u2014can unknowingly impact highly-educated doctors as well. Got a tip? Send it to The Daily Beast here",
    "publish_date": "2023-05-02 17:00:07"
  },
  {
    "title": "Chegg shares drop more than 40% after company says ChatGPT is killing its business",
    "text": "Otherwise, Chegg beat first-quarter expectations on the top and bottom lines, with earnings per share ex-items of 27 cents above analysts\u0027 26 cent estimate, and revenue of $188 million topping a $185 million consensus. Following the results, Morgan Stanley analyst Josh Baer slashed his price target to $12 from $18. The analyst said that AI \u0022completely overshadowed\u0022 the results. Meanwhile, Jefferies downgraded the stock to hold from buy, citing the threat artificial intelligence poses to Chegg. The Wall Street firm slashed its price target to $11 from $25. Chegg is developing its own AI product, CheggMate, which is meant to help students with their homework. The product is built in collaboration with OpenAI, which develops ChatGPT. However, Jefferies analyst Brent Thill says the impact of the product is uncertain. \u0022While CHGG plans to launch the CheggMate beta this month to a select few, the timing of a full launch is unclear,\u0022 he said. \u0022We don\u0027t expect there to be any meaningful impact from CheggMate in FY23, believing any potential impact won\u0027t show up until FY24 at the earliest.\u0022 \u2014 CNBC\u0027s Michael Bloom and Brian Evans contributed reporting. Correction: Chegg shares fell more than 40% on Tuesday, and CEO Dan Rosensweig spoke during the company\u0027s earnings call Monday evening. A previous version misstated the days of the week.",
    "publish_date": "2023-05-02 17:32:59"
  },
  {
    "title": "Artificial Intelligence Is Already Causing Disruption And Job Losses At IBM And Chegg - WorldNewsEra",
    "text": "Twenty-five percent of jobs will be negatively impacted over the next five years, according to a new report by The World Economic Forum. In a study, New York City-based investment bank Goldman Sachs predicts that the fast-growing mass adoption of AI will impact 300 million jobs. We are now seeing the effects of helpful but disruptive technology. Chegg, an educational company, and IBM have both announced that AI will cause a change within their respective organizations. Chegg saw its shares fall in value. IBM will enact hiring freezes and allow attrition without recruiting new personnel, as AI will take over their jobs. AI Schools Chegg Stock Shares in online learning company Chegg plunged after it was one of the first organizations to admit that AI affected its business model. Chegg recognized that students were turning to OpenAi\u2019s ChatGPT for help. The AI alternative hurts Chegg\u2019s financial situation as its shares fell nearly 50% on Tuesday morning. The education company highlights how quickly AI can inexpensively replicate services and products. The Financial Times reported that California-based Chegg saw a decline in revenue and a loss of subscribers. In response to the new reality, The company started CheggMate, a service created with ChatGPT-4 to offer tailored content via AI. Goldman Sachs And The World Economic Forum Predictions Millions Of Jobs Will Be Impacted According to a new report by the World Economic Forum released on Monday, a quarter of jobs will be impacted over the next five years. The fast-growing trends of artificial intelligence, digitization, renewable energy and supply chain reshoring will bring about a critical shift in the global labor market. The WEF predicts a \u201Cnew era of turbulence,\u201D as many workers won\u2019t have the requisite skills to keep up with the changes. Those with a technology, data analytics or cybersecurity background will benefit in the new environment. The WEF study surveyed more than 800 companies that collectively employ 11.3 million workers across 45 countries worldwide. Global employers anticipate creating 69 million new positions by 2027 and eradicating 83 million jobs\u2014a net loss of 14 million roles. Clerical workers will bear the brunt of the fast-moving changes. Around 26 million jobs in administrative positions will be cut due to AI. Suppose generative AI lives up to its hype. In that case, the workforce in the United States and Europe will be upended, Goldman Sachs reported this week in a sobering and alarming report about AI\u2019s ascendance. The investment bank estimates 300 million jobs could be lost or diminished by this fast-growing technology. On the positive side, Goldman contends automation creates innovation, leading to new jobs. For companies, there will be cost savings thanks to AI. They can deploy their resources toward building and growing businesses, ultimately increasing annual global GDP by 7%. IBM\u2019s Hiring Freeze on Roles That AI Can replace Armonk, New York-based International Business Machines Corp. CEO Arvind Krishna announced it would pause hiring for roles that can be replaced by artificial intelligence. Functions including administrative-oriented back office roles and human resources, are targeted. Krishna points out that the tech company has around 26,000 workers that are not client-facing, and about 30%\u2014 representing 7,800 people\u2014 could be displaced through attrition due to AI over the next five years. IBM boasts 260,000 workers and will keep hiring for software development and customer-facing roles. The Godfather Of AI Speaks Out Dr. Geoffrey Hinton is considered the \u2018Godfather of AI\u2019 due to his long-standing involvement with this technology. After a decade of working at the online search giant, Hinton recently left Google, his current employer, reported the New York Times. His rationale for his departure was over concerns about the adverse impact on people due to the proliferation of AI. He shared his apprehension over misinformation, disruptions of the job market, and other severe existential risks. At 75 years of age, the AI Godfather left the search giant to freely speak about the potential damages that AI can wreak without being tied to Google. The AI pioneer said that he regretted his contribution to the space.",
    "publish_date": "2023-05-02 17:46:27"
  },
  {
    "title": "Hello, my name is Dr ChatGPT: Are robot doctors really the future?",
    "text": "\u201CHello, my name is Dr ChatGPT. How can I help you today?\u201D Imagine a medical consultation with a robotic creature. Will you feel comfortable? Will you trust this product of artificial intelligence to look after you effectively and safely? I\u2019m not sure how far away such a reality is, but it is certainly attracting heavy investment. The latest version of artificial intelligence (AI) to hit the headlines, ChatGPT, has been creating a stir since the US start-up OpenAI made the text-based dialogue system accessible to the public in November 2022. ChatGPT stands for Chat Generative Pre-trained Transformer. ChatGPT may be a long way from the AI technology that fuels a functioning robotic doctor but there is a clear intent that machines will eventually replace human physicians. Those arguing for this eventuality say that deep-learning AI systems continually integrate new knowledge and perfect themselves with a speed that humans cannot match. They also highlight the benefits of using AI to treat patients, including increased availability, lower costs and no risk of mutual infection. Sceptics argue that AI in healthcare is overhyped, profit driven and not always in patients\u2019 best interests. But even if we were to have high-level evidence of the superiority of AI to medical professionals, would that justify replacing humans with machines? In an editorial in the British Medical Journal Dr Vanessa Rampton says the question asks us to differentiate between the technical prowess of AI and the more fundamental question of whether human physicians can provide something that machines will never be able to. In my opinion, robot simulated empathy can never replicate human forms of communication. Human doctors can relate to patients as fellow mortals and vulnerable beings. Patients need to be cared for by people, especially when we are ill and at our most vulnerable. A machine will never be able to show us true comfort. In the intimate crucible of a doctor-patient consultation, there is a need to appreciate patient\u2019s values, their non-verbal communications and their social circumstances. These factors become especially important if a patient has symptoms for which no diagnosis can be found, or cure is not an option. AI may have the potential to become a useful and innovative aide in healthcare, but I believe there will always be room for humanity According to Rampton, patients emphasise that sensing your doctor truly cares about what you are going through, really wants to help and is able to establish a \u201Cgenuinely intimate and empathetic connection\u201D makes a big difference to their ability to manage their health. Research from Yale University, published last year, offers an up-to-date understanding of patients\u2019 views of AI in healthcare. Most of the 926 respondents were very concerned or somewhat concerned about AI\u2019s unintended consequences, including misdiagnosis (91.5 per cent), privacy breaches (71 per cent), less time with clinicians (70 per cent), and higher healthcare costs (68 per cent). Patients acknowledge that AI could help physicians integrate the most recent scientific evidence into medical care. But there is a strong feeling that AI in medicine should be disclosed and controlled to protect patient interests and meet ethical standards. [ \u2018Coffin tablet\u2019 misuse: Why the painkiller pregabalin must be made a controlled drug in Ireland ] [ Dead or alive? The doctor\u2019s challenge ] In the radiology arena, people seem reasonably relaxed to have diagnostic technology work hand in hand with the radiologist. Recent research shows that AI software can detect TB from chest X-rays at an accuracy level comparable to, or better than, that of the radiologists tested. AI may have the potential to become a useful and innovative aide in healthcare, but I believe there will always be room for humanity. If digital technologies enable the development of new forms of knowledge and diagnostic accuracy, it would seem foolish not to welcome them. But a key question remains: as technology continues to change relationships between patients and doctors, how can we maintain an essential trust in the process? Without this bedrock of trust we may need to slow down trends towards more automation in the consulting room.",
    "publish_date": "2023-05-02 18:17:56"
  },
  {
    "title": "Homework helper Chegg\u0027s stock plummets as students turn to ChatGPT",
    "text": "Online learning platform Chegg said the rise of OpenAI\u2019s ChatGPT is hurting its business as more students turn to the chatbot for homework help \u2013 a revelation that caused the company\u2019s stock to plunge by more than 48% in early trading on Tuesday. Chegg CEO Dan Rosensweig admitted during the company\u2019s earnings call on Monday that ChatGPT \u2013 which has wowed the public with its lifelike responses to user prompts \u2013 was disrupting its ability to lure new customers. \u201CIn the first part of the year, we saw no noticeable impact from ChatGPT on our new account growth, and we were meeting expectations on new sign ups,\u201D Rosensweig said during the call. \u201CHowever, since March, we saw a significant spike in student interest in ChatGPT. We now believe it\u2019s having an impact on our new customer growth,\u201D he added. Rosensweig said Chegg has maintained \u201Cvery strong retention rates\u201D of its existing customers since ChatGPT\u2019s release. Nevertheless, the Chegg boss said his company would be \u201Cmore cautious with our forward outlook\u201D in the days ahead as ChatGPT and other forms of so-called generative AI hit the market. The company said it expected total net revenue of $175 million to $178 million in the second quarter of 2023 \u2013 a range that came in below analysts\u2019 expectations, according to FactSet data cited by CNBC. As of 11 a.m. ET, Chegg\u2019s stock was down more than 48% to $9.08 per share. Chegg offers various tools to students, including homework help, textbook rentals, test prep, assistance on essay writing and access to educators in exchange for a monthly fee. ChatGPT is threatening its business by offering students access to similar information for free. The company\u2019s warnings could exacerbate fears among a growing number of experts who have warned that rapid advancements in AI could upend the US job market and make some careers obsolete. Google CEO Sundar Pichai recently warned that AI will cause job losses among so-called \u201Cknowledge workers,\u201D such as writers, accountants, architects and software engineers. In March, Elon Musk and more than 1,000 other experts cited potential job losses as a key risk while calling for a six-month pause in the development of advanced AI. Experts also warn that responses from ChatGPT and other similar services are often riddled with inaccurate information. Chegg is also making its own foray into the AI space with the upcoming release of CheggMate, its own chatbot. The company collaborated with OpenAI to develop CheggMate, which will \u201Charness the power of ChatGPT paired with our proprietary data and subject matter experts to make learning more personalized, adaptive, accurate, fast and effective,\u201D according to Rosensweig.",
    "publish_date": "2023-05-02 18:38:16"
  },
  {
    "title": "The race to make humans redundant",
    "text": "The race to make humans redundant AI cannot be controlled by policy changes. We need to adapt and prepare for the fallout R Jagannathan Premium Illustration: Binay Sinha 6 min read Last Updated : May 02 2023 | 10:30 PM IST Follow Us Around end-March, some 1,100 people, including Tesla founder Elon Musk and Apple co-founder Steve Wozniak, wrote an open letter calling for a six-month ban on advanced artificial intelligence (AI) so that the implications of this technology can be understood before it becomes unstoppable. The concerns emerged after OpenAI introduced its wildly successful ChatGPT to all users, followed by a more powerful GPT-4. GPT stands for generative pre-trained transformer, which is a language model that uses wide and deep learning to mimic human-like responses. It learns how to respond to human queries by scouring all available net-based information and looking for patterns in them. The problem: No country or state or group of powerful individuals can really hope to restrain or reverse the development of any technology. From the time humankind first learnt to sharpen a piece of stone, and his tribe could easily have blackballed him, the one thing we know for sure is that tech TO READ THE FULL STORY, SUBSCRIBE NOW NOW AT JUST RS 249 A MONTH. Subscribe To Insights Key stories on business-standard.com are available to premium subscribers only.Already a BS Premium subscriber? Log in NOW What you get on Business Standard Premium? Unlock 30\u002B premium stories daily hand-picked by our editors, across devices on browser and app. Pick your 5 favourite companies, get a daily email with all news updates on them. Full access to our intuitive epaper - clip, save, share articles from any device; newspaper archives from 2006. Preferential invites to Business Standard events. Curated newsletters on markets, personal finance, policy \u0026 politics, start-ups, technology, and more. VIEW ALL FAQs Or Also Read GPT-4: Everything about the OpenAI\u0027s newly introduced large language model ChatGPT vs humans: What it can and cannot accomplish AI may automate up to 300 million jobs around the world: Goldman Sachs OpenAI announces ChatGPT chatbot: What is it, how it works, and limitations Clients not clearing their dues on time? ChatGPT might be able to help you India needs its conglomerates No method in the madness Why state must cede power to communities The jobs \u0026 technology trade-off Denting democracy Unemployment rate rises in April Labour laws\u0027 fairness challenge The de-dollarisation debate Why former J\u0026K Guv Satyapal Malik may be taking Modi\u0027s critics for a ride Reinvent tiger conservation Topics : Artificial intelligence First Published: May 02 2023 | 10:30 PM IST",
    "publish_date": "2023-05-02 19:00:30"
  },
  {
    "title": "Ashton Kutcher reveals why he\u0027s betting on Artificial Intelligence: \u0027A really beautiful thing\u0027",
    "text": "Ashton Kutcher is betting on artificial intelligence, investing millions in the technology through his investment fund, Sound Ventures, and saying he believes the technology has the potential to change industries from medicine to law. \u0022A lot of people have thought historically about AI as this foreign object that acts upon you,\u0022 Kutcher said at the Milken Global Institute Monday. \u0022What we\u2019re finding right now \u2026 is that it\u2019s a tool that people can use. And I think that\u2019s a really beautiful thing.\u0022 FOX NEWS POLL: MORE SEE BAD THAN GOOD IN AI Kutcher said generative artificial intelligence, like that developed by OpenAI, has the potential to increase access in areas like legal services. \u0022I look at AI as an equity and inclusion play that is massive,\u0022 he said. \u0022\u2026What we\u2019re about to see is a reinvention of skilled labor markets.\u0022 Saying it will bring opportunities to people ed masse.\u0022 WHAT ARE THE LEADING COMPANIES IN THE INDUSTRY DOING ON ARTIFICIAL INTELLIGENCE? Sound Ventures, the venture capital firm founded by Kutcher and business partner Guy Oseary, announced Monday it has closed the Sound Ventures AI fund at $240 million. The fund currently includes companies like OpenAI and Stability AI. \u0022We believe this is potentially the most significant technology we will experience since the advent of the internet,\u0022 Kutcher said in a statement. \u0022The foundation model layer companies are defining the category, and, in our view, they have the power to transform businesses and everyday life. That is a conversation we want to be in.\u0022 CLICK HERE TO GET THE FOX NEWS APP Generative AI, such as OpenAI\u2019s ChatGPT and Google\u2019s Bard, have taken center stage in recent months, with potential applications in industries from medicine to finance.",
    "publish_date": "2023-05-02 19:11:56"
  },
  {
    "title": "\u0027Godfather of AI\u0027 leaves Google, warns of tech\u0027s dangers",
    "text": "Sounding alarms about artificial intelligence has become a popular pastime in the ChatGPT era, taken up by high-profile figures as varied as industrialist Elon Musk, leftist intellectual Noam Chomsky and the 99-year-old retired statesman Henry Kissinger. But it\u2019s the concerns of insiders in the AI research community that are attracting particular attention. A pioneering researcher and the so-called \u201CGodfather of AI\u201D Geoffrey Hinton quit his role at Google so he could more freely speak about the dangers of the technology he helped create. Over his decades-long career, Hinton\u0027s pioneering work on deep learning and neural networks helped lay the foundation for much of the AI technology we see today. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools \u2014 including Google\u2019s \u201CBard.\u201D Some of the dangers of AI chatbots are \u0022quite scary,\u0022 Hinton told the BBC. \u201CRight now, they\u2019re not more intelligent than us, as far as I can tell. But I think they soon may be.\u201D In an interview with MIT Technology Review, Hinton also pointed to \u201Cbad actors\u201D that may use AI in ways that could have detrimental impacts on society \u2014 such as manipulating elections or instigating violence. Hinton, 75, says he retired from Google so that he could speak openly about the potential risks as someone who no longer works for the tech giant. \u201CI want to talk about AI safety issues without having to worry about how it interacts with Google\u2019s business,\u201D he told MIT Technology Review. \u201CAs long as I\u2019m paid by Google, I can\u2019t do that.\u201D Since announcing his departure, Hinton has maintained that Google has \u201Cacted very responsibly\u201D regarding AI. He told MIT Technology Review that there\u2019s also \u201Ca lot of good things about Google\u201D that he would want to talk about \u2014 but those comments would be \u201Cmuch more credible if I\u2019m not at Google anymore.\u201D Google confirmed that Hinton had retired from his role after 10 years overseeing the Google Research team in Toronto. Hinton declined further comment Tuesday but said he would talk more about it at a conference Wednesday. At the heart of the debate on the state of AI is whether the primary dangers are in the future or present. On one side are hypothetical scenarios of existential risk caused by computers that supersede human intelligence. On the other are concerns about automated technology that\u2019s already getting widely deployed by businesses and governments and can cause real-world harms. \u201CFor good or for not, what the chatbot moment has done is made AI a national conversation and an international conversation that doesn\u2019t only include AI experts and developers,\u201D said Alondra Nelson, who until February led the White House Office of Science and Technology Policy and its push to craft guidelines around the responsible use of AI tools. \u201CAI is no longer abstract, and we have this kind of opening, I think, to have a new conversation about what we want a democratic future and a non-exploitative future with technology to look like,\u201D Nelson said in an interview last month. A number of AI researchers have long expressed concerns about racial, gender and other forms of bias in AI systems, including text-based large language models that are trained on huge troves of human writing and can amplify discrimination that exists in society. \u201CWe need to take a step back and really think about whose needs are being put front and center in the discussion about risks,\u201D said Sarah Myers West, managing director of the nonprofit AI Now Institute. \u201CThe harms that are being enacted by AI systems today are really not evenly distributed. It\u2019s very much exacerbating existing patterns of inequality.\u201D Hinton was one of three AI pioneers who in 2019 won the Turing Award, an honor that has become known as tech industry\u2019s version of the Nobel Prize. The other two winners, Yoshua Bengio and Yann LeCun, have also expressed concerns about the future of AI. Bengio, a professor at the University of Montreal, signed a petition in late March calling for tech companies to agree to a 6-month pause on developing powerful AI systems, while LeCun, a top AI scientist at Facebook parent Meta, has taken a more optimistic approach. _______ AP Technology Reporter Matt O\u0027Brien reported from Cambridge, Massachusetts.",
    "publish_date": "2023-05-02 20:10:56"
  },
  {
    "title": "IBM pauses hiring for 7,800 jobs because they could be performed by AI",
    "text": "Big Blue is calling a big timeout on hiring real humans. IBM CEO Arvind Krishna said the company will likely pause filling nearly 8,000 jobs because the positions could be performed by artificial intelligence within the next few years. Krishna said that the company will either slow down or altogether suspend hiring for so-called \u201Cback office\u201D functions such as human resources. \u201CI could easily see 30% of that getting replaced by AI and automation over a five-year period,\u201D Krishna told Bloomberg News on Monday. Jobs in which workers don\u2019t have to face customers number some 26,000 employees at IBM, so 30% would amount to around 7,800. Krishna told Bloomberg News that AI could perform functions such as moving employees between departments or writing employment verification letters. The company plans to still hire for roles in software development as well as other customer-facing jobs, Krishna added. The Westchester County-based multinational tech giant has a global workforce of 260,000 people. In January, IBM announced it would be laying off 3,900 people after it missed its earnings targets. At the time, IBM CFO James Kavanaugh told Reuters that the company was still \u201Ccommitted to hiring for client-facing research and development.\u201D Kavanaugh said that the company is expected to save $2 billion a year by the end of 2024 thanks to cost-cutting measures and efficiency steps, such as the selling off of its Kyndryl Inc. unit as well as part of the Watson Health business. Despite the layoffs, Krishna said that IBM added a total of 7,000 people to its workforce in the first quarter. The rapid development of AI, which was most pronounced with the rollout of OpenAI\u2019s ChatGPT bot, has already made an impact on businesses in the knowledge sector. The parent company that owns Chegg, the California-based online learning site geared toward college students, saw its share price plummet after it admitted that ChatGPT was eating into its profits. \u201CSince March, we saw a significant spike in student interest in ChatGPT. We now believe it\u2019s having an impact on our new customer growth rate,\u201D said Chegg CEO Dan Rosensweig. The company\u2019s stock was down 47% at the start of trading on Wall Street on Tuesday. The swift rise of AI has sparked concern from tech luminaries such as Elon Musk and others that it could upend civilization. Musk on Monday cautioned that even a \u201Cbenign dependency\u201D on AI can threaten civilization. \u201CEven benign dependency on AI/Automation is dangerous to civilization if taken so far that we eventually forget how the machines work,\u201D Musk tweeted. Dr. Geoffrey Hinton, a prominent AI researcher known as the \u201CGodfather of AI,\u201D told The New York Times that he has left his job at Google and that he regrets his role in developing the technology. \u201CI console myself with the normal excuse: If I hadn\u2019t done it, somebody else would have,\u201D Hinton said in an interview published on Monday.",
    "publish_date": "2023-05-02 20:34:14"
  },
  {
    "title": "Reid Hoffman\u0027s new AI startup Inflection launches ChatGPT-like chatbot",
    "text": "Inflection AI, the AI startup founded by LinkedIn co-founder Reid Hoffman and former Deepmind researcher Mustafa Suleyman, has released its first AI chatbot product, the company said on Tuesday. Similar to the viral chatbot ChatGPT, Inflection\u0027s AI chatbot, named Pi, uses generative AI technology to interact with users through conversations, in which people can ask questions and share interests. Inflection AI said it developed the technology in-house and its Pi chatbot is built on prioritizing human conversations with a high level of emotional intelligence. \u0022We think of Pi as a digital companion on hand whenever you want to learn something new, when you need a sounding board to talk through a tricky moment in your day, or just pass the time with a curious and kind counterpart,\u0022 said Mustafa Suleyman, chief executive and co-founder of Inflection. Users can interact with Pi across platforms including its website, app and social media platforms like Instagram. Pi uses user data, including conversational content, to train its AI systems, according to its terms of service. Inflection said its database was last updated in November 2022 and not currently connected to the internet. Chatbot powered by generative AI technology has become a crowded field since OpenAI\u0027s ChatGPT burst into the scene last November. Using large language models, which mine vast amounts of text to summarize information and generate content, chatbots like Google\u0027s Bard, and Character.AI enable people to have in-depth conversations for both professional and personal needs. VC firm Greylock, where Hoffman is a partner, incubated Inflection and led a $250 million investment in the firm. Hoffman, who is also sitting on the board of Microsoft, resigned from the board of OpenAI in March, citing potential conflicts with the AI startups he works with.",
    "publish_date": "2023-05-02 21:05:51"
  },
  {
    "title": "\u0026#8216;Godfather of AI\u0026#8217; leaves Google, warns of tech\u0026#8217;s dangers",
    "text": "By MATT O\u2019BRIEN and WYATTE GRANTHAM-PHILIPS (AP Business Reporters) WASHINGTON (AP) \u2014 Sounding alarms about artificial intelligence has become a popular pastime in the ChatGPT era, taken up by high-profile figures as varied as industrialist Elon Musk, leftist intellectual Noam Chomsky and the 99-year-old retired statesman Henry Kissinger. But it\u2019s the concerns of insiders in the AI research community that are attracting particular attention. A pioneering researcher and the so-called \u201CGodfather of AI\u201D Geoffrey Hinton quit his role at Google so he could more freely speak about the dangers of the technology he helped create. Over his decades-long career, Hinton\u2019s pioneering work on deep learning and neural networks helped lay the foundation for much of the AI technology we see today. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools \u2014 including Google\u2019s \u201CBard.\u201D Some of the dangers of AI chatbots are \u201Cquite scary,\u201D Hinton told the BBC. \u201CRight now, they\u2019re not more intelligent than us, as far as I can tell. But I think they soon may be.\u201D In an interview with MIT Technology Review, Hinton also pointed to \u201Cbad actors\u201D that may use AI in ways that could have detrimental impacts on society \u2014 such as manipulating elections or instigating violence. Hinton, 75, says he retired from Google so that he could speak openly about the potential risks as someone who no longer works for the tech giant. \u201CI want to talk about AI safety issues without having to worry about how it interacts with Google\u2019s business,\u201D he told MIT Technology Review. \u201CAs long as I\u2019m paid by Google, I can\u2019t do that.\u201D Since announcing his departure, Hinton has maintained that Google has \u201Cacted very responsibly\u201D regarding AI. He told MIT Technology Review that there\u2019s also \u201Ca lot of good things about Google\u201D that he would want to talk about \u2014 but those comments would be \u201Cmuch more credible if I\u2019m not at Google anymore.\u201D Google confirmed that Hinton had retired from his role after 10 years overseeing the Google Research team in Toronto. Hinton declined further comment Tuesday but said he would talk more about it at a conference Wednesday. At the heart of the debate on the state of AI is whether the primary dangers are in the future or present. On one side are hypothetical scenarios of existential risk caused by computers that supersede human intelligence. On the other are concerns about automated technology that\u2019s already getting widely deployed by businesses and governments and can cause real-world harms. \u201CFor good or for not, what the chatbot moment has done is made AI a national conversation and an international conversation that doesn\u2019t only include AI experts and developers,\u201D said Alondra Nelson, who until February led the White House Office of Science and Technology Policy and its push to craft guidelines around the responsible use of AI tools. \u201CAI is no longer abstract, and we have this kind of opening, I think, to have a new conversation about what we want a democratic future and a non-exploitative future with technology to look like,\u201D Nelson said in an interview last month. A number of AI researchers have long expressed concerns about racial, gender and other forms of bias in AI systems, including text-based large language models that are trained on huge troves of human writing and can amplify discrimination that exists in society. \u201CWe need to take a step back and really think about whose needs are being put front and center in the discussion about risks,\u201D said Sarah Myers West, managing director of the nonprofit AI Now Institute. \u201CThe harms that are being enacted by AI systems today are really not evenly distributed. It\u2019s very much exacerbating existing patterns of inequality.\u201D Hinton was one of three AI pioneers who in 2019 won the Turing Award, an honor that has become known as tech industry\u2019s version of the Nobel Prize. The other two winners, Yoshua Bengio and Yann LeCun, have also expressed concerns about the future of AI. Bengio, a professor at the University of Montreal, signed a petition in late March calling for tech companies to agree to a 6-month pause on developing powerful AI systems, while LeCun, a top AI scientist at Facebook parent Meta, has taken a more optimistic approach. _______ AP Technology Reporter Matt O\u2019Brien reported from Cambridge, Massachusetts.",
    "publish_date": "2023-05-02 21:34:57"
  },
  {
    "title": "\u2018Godfather of AI\u2019 leaves Google, warns of \u2018scary\u2019 technology he helped create",
    "text": "Sounding alarms about artificial intelligence has become a popular pastime in the ChatGPT era, taken up by high-profile figures as varied as industrialist Elon Musk, leftist intellectual Noam Chomsky and the 99-year-old retired statesman Henry Kissinger. But it\u2019s the concerns of insiders in the AI research community that are attracting particular attention. A pioneering researcher and the so-called \u201CGodfather of AI\u201D Geoffrey Hinton quit his role at Google so he could more freely speak about the dangers of the technology he helped create. Over his decades-long career, Hinton\u2019s pioneering work on deep learning and neural networks helped lay the foundation for much of the AI technology we see today. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools \u2014 including Google\u2019s \u201CBard.\u201D",
    "publish_date": "2023-05-02 21:47:57"
  },
  {
    "title": "\u0022Mrs. Davis\u0022 star Betty Gilpin: \u0022What do we lose when we have all the answers in our pocket?\u0022",
    "text": "\u0022It is scary,\u0022 admits Betty Gilipn. \u0022It freaks me out.\u0022 The Emmy-nominated actor and author (\u0022All the Women in My Brain\u0022) is talking about the real-world inspiration for her critically acclaimed \u2014 and strikingly relevant \u2014 new Peacock series \u0022Mrs. Davis.\u0022 \u0022I definitely saw OpenAI or ChatGPT as just a corner of the world and news that I don\u0027t understand and I don\u0027t want to engage with,\u0022 she said on \u0022Salon Talks.\u0022 But now, \u0022I\u0027m rapidly realizing we may not have a choice.\u0022 And while she\u0027s not as zealous about the threat of technology as her fictional counterpart Sister Simone, the former \u0022GLOW\u0022 star does admit that \u0022I can\u0027t pee without watching a YouTube video. I\u0027m completely addicted to that poison.\u0022 During our conversation, Gilpin (who appears next in the Showtime series \u0022Three Women\u0022) talks about why she was never a mumblecore heroine, how \u0022GLOW\u0022 changed her career trajectory, and why she\u0027s still figuring out her own relationship with AI. \u0022Are you our savior,\u0022 she asks, \u0022or are you our downfall?\u0022 Watch Betty Gilpin on \u0022Salon Talks\u0022 here, or read our conversation below. This conversation has been lightly edited for clarity and length. Tell me who Mrs. Davis is. \u0022Mrs. Davis,\u0022 the show, takes place in a world not unlike our own. It\u0027s present day, but it\u0027s a society where a Siri or Alexa-type algorithm called Mrs. Davis has taken over and purports to be benevolent, is in everyone\u0027s ear in a fancy-looking AirPod thing and seemingly has all the answers and has fixed all the problems in the world. But, there\u0027s a small faction of society, myself, Simone the nun included, that believe she is evil and don\u0027t trust her. I like how you\u0027re just casually, \u0022Simone the nun.\u0022 Simone the nun. You get to go through a lot of different versions of yourself in this as well. I know this character was basically conceived with you in mind. I don\u0027t know that that\u0027s true. I\u0027ve read interviews with Damon Lindelof where he said it was! And then you were, very early on, brought in on the collaboration of creating this show and talking about the character. I wonder if that\u0027s a kid glove, \u0022You\u0027re a very special poodle\u0022 thing that they tell actors to sedate them when they enter a job. To be like, \u0022This was conceived with you in mind. We offered it to 10 other people before you.\u0022 That\u0027d be so nice if it was. I had worked with Damon Lindelof on \u0022The Hunt\u0022 prior to this. He and the genius Tara Hernandez wrote this insane script, and I read the pilot and had never read anything like it. In true Damon fashion and Tara Hernandez fashion, it really hides your vegetables in a hundred different genres. I feel like oftentimes, when we\u0027re faced with what to watch at the end of the day, it\u0027s either joyful and mindless or important and depressing. They have a way of writing something that\u0027s important and joyful. I had never really read anything like this, obviously, so it was a dream. Watching this character made me think about the title of your book. \u0022All the Women in My Brain.\u0022 Simone is all these women, all these different characters. I\u0027m wondering if that\u0027s something that you drew on when you were playing her, knowing that she has so many sides to her? Yeah, she has so many different sides. Also, the script has so many different tones. I find that more like life than when you\u0027re doing a script or a character where you have to keep it one genre, one feeling, one color. I also think I oftentimes play or read characters that are either sarcastic, wry, arms folded, eyebrow raised, have all the answers before you do, or super vulnerable, arms open, hopeful, gullible types. Maybe not gullible, but I think that Simone is both of those things. We see that she maybe started as the former and then her faith has really exploded her into this other side, where maybe she does love aspects of the world and have childlike hope. I relate to that as a mom. I feel like I was an eye-rolling, middle-finger-in-the-air person until I had a baby. Then suddenly I\u0027m crying at a butterfly, like, \u0022Ugh.\u0022 Never done that before. One of the descriptions I read of the show was that it\u0027s about AI versus faith, but I feel like it\u0027s about AI and faith. It\u0027s about the ways in which we put our trust into something. And even when we were filming it six months ago, we didn\u0027t know how prescient and of-the-time our show would be. ChatGPT wasn\u0027t as much a part of the headlines, at least as it is right this second and OpenAI. I think, even though our show is super out there and bonkers sometimes, it is very of this exact moment where we\u0027re going to OpenAI, \u0022Are you our savior or are you our downfall?\u0022 I think a lot of the questions that my character is asking of this thing is a question I\u0027m asking, which is, \u0022What do we lose when we have all the answers in our pocket? Do we stop asking the big questions? Are we gambling with access to the intangible and inexplicable, which are the things that make us human and shape us as individuals?\u0022 If we have a robot puppy telling us who we are and what to do at all times, do we stop becoming interesting, well-rounded, good people? I wonder what it must have been like for you as an actor, going from where this was much more speculative to being in this moment in our reality now. One of the reviews called it \u0022the eerily timely \u0027Mrs. Davis.\u0027\u0022 It is scary, it freaks me out. I definitely saw OpenAI or ChatGPT when I first started reading about it, as just a corner of the world and news that I don\u0027t understand and I don\u0027t want to engage with. I\u0027m rapidly realizing we may not have a choice. Also, my daughter, her generation, will be far more interactive and have a different language with it than I do, so I can\u0027t shut it out totally. But it is nice to be doing something that asks big questions. I don\u0027t want to give away any spoilers, because part of the joy of the show is the twists and turns and shocking things that you can\u0027t believe you didn\u0027t figure out an episode before, and going back and seeing Easter eggs you may have missed. There are so many aspects to this show that it was a joy to play, even if OpenAI is absolutely terrifying. You\u0027ve been very public about your journey as an actor, starting as someone who may have been typecast because of the way you look. Then you have \u0022GLOW,\u0022 which is this enormous breakthrough role for you. How did Debbie change your career and you as a person? In so many ways. Liz Flahive and Carly Mensch, who created \u0022GLOW\u0022 were writers and producers on \u0022Nurse Jackie,\u0022 which was my big first TV job. Really, up until then I had done mostly off-Broadway theater and episodes of cop and hospital shows here and there. Died on \u0022Criminal Intent\u0022 and then came back within the same year as an alive person. I went to the producers and was like, \u0022People are going to be taken out of it.\u0022 They\u0027re like, \u0022It happens all the time, no one cares.\u0022 I got this job on \u0022Nurse Jackie.\u0022 I came on in Season 5 and the character\u0027s purpose was, \u0022Let\u0027s have a ditzy doctor who takes off all her clothes all the time to get viewers back in there.\u0022 I think Liz and Carly were the writers that realized, \u0022Oh, that\u0027s a weird person, that\u0027s a character actor,\u0022 and started shaping the character to my strangeness. [With] Debbie, the character on \u0022GLOW,\u0022 they wrote about that experience a little bit, of playing a certain thing aesthetically and then wanting to do these other things or Trojan Horse those things into characters. Debbie finds that through wrestling. Liz and Carly literally wrote out that map for me and gave me the opportunity to do all the things that I wanted to do on screen, which is such a gift. So many actors, or maybe any creative person, feels like, \u0022Oh, I\u0027m only being asked to do 10% of what I can do. This is so frustrating. It\u0027s not that I\u0027m not good, it\u0027s that I\u0027m not given the opportunity.\u0022 \u0022GLOW,\u0022 in so many ways, gave me that opportunity and totally changed my life. \u0022Mrs. Davis,\u0022 even though it\u0027s a thousand different genres per episode, this is 100% of what I want to do, completely. It\u0027s like the ultimate acting cat toy, and it\u0027s the jobs that ask you to do 5% that are the ones that you lose sleep over. It\u0027s not like the industry has completely changed. It\u0027s not like everything\u0027s all better now for females in Hollywood. Both of your parents are actors. Did they give you advice? My dad is an actor and an Episcopalian priest, so we talked more about nuns and religion in preparation for this part. It\u0027s funny, my parents are two very different actors. My dad, I call him Atticus Finch, he\u0027s gravitas incarnate, plays solemn butlers and the lawyer you can trust . . . or can you? My mom is farce incarnate, is like Lucille Ball basically. This part is very much a love letter to both of them. Simone can be very serious, she\u0027s both a gravitas butler and Lucille Ball, or that\u0027s what I\u0027m striving for. It was very strange trying to categorize this show as a comedy or drama. I think they landed on drama just because it\u0027s hour-long. I\u0027m like, \u0022I fall on a lot of banana peels for a drama.\u0022 Some of the hardest laughs of my life were on this set. We just had so much fun together. Now you have a daughter. What do you want to tell her, if in a few years she\u0027s got her 1.2 million followers on whatever version of TikTok there is? I\u0027m hoping that we\u0027re the generation just driving without seat belts. I just worry, really approaching this part, thinking about the internet and the church, two very different things. It made me think, \u0022OK, these are both institutions that we created as a reaction to the human need for connection and asking big questions, so we made the internet and church.\u0022 I often think that we sometimes misuse those institutions to do the opposite of connecting and asking. It\u0027s like tunnel vision and disconnecting and echo chamber. Maybe, hopefully, we\u0027re the generation using this thing to disconnect and make us dumber when maybe my daughter Mary\u0027s generation can figure out how to use it to actually connect us and make us smarter. Some people are using the internet for that. I am using it to make me dumber. She\u0027s two and a half. I\u0027m snatching screens away from her like they\u0027re poison and then I can\u0027t pee without watching a YouTube video. I\u0027m completely addicted to that poison, so I better get right with my relationship with it before I try to preach to her about what her relationship with it should be. Is that part of why you left social media? I had Twitter for a second and then ran away. I have a private Instagram. I\u0027m addicted like everybody else is. I\u0027m sending people falling down videos to my various group chats. I need to get un-addicted to my private Instagram. You\u0027re playing all these interesting, complicated women, so I have to ask about another complicated role you have coming up, in \u0022Three Women.\u0022 I was obsessed with that book by Lisa Taddeo. For those who don\u0027t know, she is an author who followed three real women in their lives, and it\u0027s about their personal lives, their sex lives, their relationship to desire. I play Lina, who is a woman who lives in Indiana, has two kids and has an affair with her high school flame. Maybe part of the reason that I didn\u0027t work a lot in my 20s is mumblecore was king and being cool and having low stakes and minimalism \u2013 and I\u0027ve never been good at that. I like playing high stakes, playing to the mezzanine, making a thousand choices. Whether it helps or hurts the piece, I don\u0027t know. Lina is such a character who, even though she\u0027s in a minivan in Indiana, is playing to the mezzanine. The stakes are so high for her. One of my favorite characters I\u0027ve ever played. I adored that experience, and I\u0027m so happy it found a home in Starz.",
    "publish_date": "2023-05-02 21:00:01"
  },
  {
    "title": "Google, Microsoft OpenAI CEOs to attend White House AI meeting -official",
    "text": "We know it\u0027s a hassle to switch browsers but we want your experience with CNA to be fast, secure and the best it can possibly be. To continue, upgrade to a supported browser or, for the finest experience, download the mobile app.",
    "publish_date": "2023-05-02 22:05:57"
  },
  {
    "title": "Chegg Shares Drop More Than 40% After Company Says ChatGPT Is Killing Its Business - WorldNewsEra",
    "text": "Otherwise, Chegg beat first-quarter expectations on the top and bottom lines, with earnings per share ex-items of 27 cents above analysts\u2019 26 cent estimate, and revenue of $188 million topping a $185 million consensus. Following the results, Morgan Stanley analyst Josh Baer slashed his price target to $12 from $18. The analyst said that AI \u201Ccompletely overshadowed\u201D the results. Meanwhile, Jefferies downgraded the stock to hold from buy, citing the threat artificial intelligence poses to Chegg. The Wall Street firm slashed its price target to $11 from $25. Chegg is developing its own AI product, CheggMate, which is meant to help students with their homework. The product is built in collaboration with OpenAI, which develops ChatGPT. However, Jefferies analyst Brent Thill says the impact of the product is uncertain. \u201CWhile CHGG plans to launch the CheggMate beta this month to a select few, the timing of a full launch is unclear,\u201D he said. \u201CWe don\u2019t expect there to be any meaningful impact from CheggMate in FY23, believing any potential impact won\u2019t show up until FY24 at the earliest.\u201D \u2014 CNBC\u2019s Michael Bloom and Brian Evans contributed reporting. Correction: Chegg shares fell more than 40% on Tuesday, and CEO Dan Rosensweig spoke during the company\u2019s earnings call Monday evening. A previous version misstated the days of the week.",
    "publish_date": "2023-05-02 22:08:44"
  },
  {
    "title": "AI Vs Governments: What Different Nations Are Doing To Regulate AI Tools - WorldNewsEra",
    "text": "Rapid advances in artificial intelligence (AI) such as Microsoft-backed OpenAI\u2019s ChatGPT are complicating governments\u2019 efforts to agree on laws governing the use of the technology. Also Read \u2013 \u2018Godfather of AI\u2019 Geoffrey Hinton quits Google to warn the world about the dangers of AI Here are the latest steps national and international governing bodies are taking to regulate AI tools: Also Read \u2013 Windows 11 hacks: How to disable ChatGPT in Windows 11 search bar AUSTRALIA Also Read \u2013 Bill Gates believes ChatGPT-like chatbots will have teachers\u2019 capability in future * Seeking input on regulations The government requested advice on how to respond to AI from Australia\u2019s main science advisory body and is considering next steps, a spokesperson for the industry and science minister said in April. BRITAIN * Planning regulations Britain said in March it planned to split responsibility for governing AI between its regulators for human rights, health and safety, and competition, rather than creating a new body. CHINA * Planning regulations China\u2019s cyberspace regulator in April unveiled draft measures to manage generative AI services, saying it wanted firms to submit security assessments to authorities before they launch offerings to the public. China\u2019s capital Beijing will support leading enterprises in building AI models that can challenge ChatGPT, its economy and information technology bureau said in February. EUROPEAN UNION * Planning regulations The European Consumer Organisation (BEUC) has joined the chorus of concern about ChatGPT and other AI chatbots, calling on EU consumer protection agencies to investigate the technology and the potential harm to individuals. Twelve EU lawmakers urged world leaders in April to hold a summit to find ways to control the development of advanced AI systems, saying they were developing faster than expected. The European Data Protection Board, which unites Europe\u2019s national privacy watchdogs, said in April it had set up a task force on ChatGPT, a potentially important first step towards a common policy on setting privacy rules on AI. EU lawmakers are also discussing the introduction of the European Union AI Act that will govern anyone who provides a product or a service that uses AI. Lawmakers have proposed classifying different AI tools according to their perceived level of risk, from low to unacceptable. FRANCE * Investigating possible breaches France\u2019s privacy watchdog CNIL said in April it was investigating several complaints about ChatGPT after the chatbox was temporarily banned in Italy over a suspected breach of privacy rules. France\u2019s National Assembly approved in March the use of AI video surveillance during the 2024 Paris Olympics, overlooking warnings from civil rights groups that the technology posed a threat to civil liberties. IRELAND * Seeking input on regulations Generative AI, such as OpenAI\u2019s ChatGPT, needs to be regulated, but governing bodies must figure out how to do so properly before rushing into prohibitions that \u201Creally aren\u2019t going to stand up\u201D, Ireland\u2019s data protection chief said on April 20. ITALY * Lifted ban ChatGPT is available again to users in Italy, a spokesperson for OpenAI said on April 28. Italy temporarily banned ChatGPT in March after its data protection authority raised concerns over possible privacy violations and for failing to verify that users were aged 13 or above, as it had requested. JAPAN * Seeking input on regulations Digital transformation minister Taro Kono said in April he wanted a G7 digital ministers\u2019 meeting set for April 29-30 to discuss AI technologies including ChatGPT and issue a unified G7 message. SPAIN * Investigating possible breaches Spain\u2019s data protection agency said in April it was launching a preliminary investigation into potential data breaches by ChatGPT. It has also asked the EU\u2019s privacy watchdog to evaluate privacy concerns surrounding ChatGPT, the agency told Reuters on April 11. The US * Seeking input on regulations The Biden administration said in April it was seeking public comments on potential accountability measures for AI systems. President Joe Biden had earlier told science and technology advisers that AI could help address disease and climate change, but it was also important to address potential risks to society, national security, and the economy. \u2014 Reuters",
    "publish_date": "2023-05-02 22:35:41"
  },
  {
    "title": "Box Adds Generative AI Capabilities Through OpenAI Partnership - WorldNewsEra",
    "text": "Box is getting set to launching Box AI, comprising generative AI models that will be natively integrated into the company\u2019 Content Cloud to help users surface information faster and generate new content in order to boost productivity. The new Box AI capabilities will be powered by OpenAI\u2019s ChatGPT API and at launch, will only be available inside the Box Content Cloud, although the company said it has plans to embed Box AI across the Box product suite and support more complex use cases. Use of Box AI will be controlled by Box\u2019s built-in permissions, designed so that users can only see and interact with the files and content they are allowed to access. The capabilities announced by Box fall largely into two categories \u2014 finding the right information and creating new content. Box AI will help users find the exact information they need, working with anorganization\u2019s files, resulting in improved accuracy. when they need it, the company said in an announcement Tuesday. When viewing a document in preview, a user can ask questions, and Box AI will be able to answer by, for example, pulling out related insights from other content, including spreadsheets, or summarizing a presentation. For example, customer service teams will be able to use Box AI to surface insights from hundreds of customer feedback surveys to identify key areas for improvement, while legal teams will be able to ask Box AI to identify key clauses, terms, and obligations from a contract to speed up review cycles. Using AI to generate new content For customers wanting to use Box AI to create new content, users will be able to provide a simple prompt within Box Notes and have the technology create content from scratch or generate new material from existing information. For example, agendas, manuals, and reports that build upon information that is already in Box can also be generated using Box AI. Creatable content types include emails, newsletters, or blog posts, which can then be altered to edit the tone, length, and style. At the highest level, there are several ways that customers will benefit from companies like Box adding AI capabilities to their product portfolio, including knowledge-sharing, augmenting routine or mundane tasks like writing summaries, and generating new and revised content, said Holly Muscolino, a group vice president at analyst firm IDC. Muscolino also noted that Box is not alone in wanting to improve its current offerings with AI integrations. She cited a recent IDC global survey where 37% of respondents said that they are doing some initial exploration of potential use cases, with 24% investing in generative AI technologies in 2023. \u201CMany folks already use AI in day-to-day tasks and are unaware, however, this technology is a significant leap forward,\u201D Muscolino said. \u201C[AI] will improve the work experience, but it is early days with governance and other issues still to be resolved.\u201D Initial access to Box AI will be granted to select Box customers through an upcoming Design Partner Program, with specific pricing and packaging to be announced upon general availability.",
    "publish_date": "2023-05-02 22:34:55"
  },
  {
    "title": "Google, Microsoft CEOs called to AI meeting at White House",
    "text": "WASHINGTON :The chief executives of Alphabet Inc\u0027s Google, Microsoft, OpenAI and Anthropic will meet with Vice President Kamala Harris and top White House officials to discuss key artificial intelligence (AI) issues on Thursday, said a White House official on Tuesday. The invitation obtained by Reuters to the CEOs noted President Joe Biden\u0027s \u0022expectation that companies like yours must make sure their products are safe before making them available to the public.\u0022 Concerns about fast-growing AI technology include privacy violations, bias and worries it could proliferate scams and misinformation. In April, the Biden administration said it was seeking public comments on proposed accountability measures for AI systems, as concerns grow about its impact on national security and education. The meeting will be attended by Biden\u0027s Chief of Staff Jeff Zients, Deputy Chief of Staff Bruce Reed, National Security Adviser Jake Sullivan, Director of the National Economic Council Lael Brainard and Secretary of Commerce Gina Raimondo among others, said the White House official who did not wish to be named. The meeting will emphasize the importance of driving innovation \u0022with safeguards that mitigate risks and potential harms,\u0022 the official said. The companies did not immediately respond to a request for comment. ChatGPT, an AI program that recently grabbed the public\u0027s attention for its ability to write answers quickly to a wide range of queries, in particular has attracted U.S. lawmakers\u0027 attention as it has grown to be the fastest-growing consumer application in history with more than 100 million monthly active users.",
    "publish_date": "2023-05-02 22:52:56"
  },
  {
    "title": "New LinkedIn AI Feature Might Actually Help Get You Hired",
    "text": "LinkedIn is exploring a rare, seemingly useful application of generative AI. The platform is testing out a feature for its paying subscribers that auto-generates personalized messages to hiring managers based on a user\u2019s specific profile, according to a Tuesday blog post on the site by company exec, Ora Levit. The brief, cover letter-esque messages appear to collate information from a user\u2019s LinkedIn bio and present it in the form of a straightforward, professional-sounding appeal. \u201CHi Sarah, Hope you are having a good week. I am excited to reach out about the Premium Account Manager position at Oustia. As an Account executive at Mintome, I have 5\u002B years of experience managing accounts for brands...,\u201D reads the example provided by LinkedIn. The illustrative sample message goes on to reference the user\u2019s educational background and ends with a request to \u201Cconnect and find a time to chat.\u201D \u201CUsing generative AI with information from your profile, the hiring manager\u2019s profile, the job description, and the company of interest, we create a highly personalized draft message to get a conversation started,\u201D wrote Levit in the afternoon press release. Though, the feature announcement included a disclaimer that\u2019s basically a given with all AI tools: double check the work. \u201CCustomization is still important, so take the time to review and edit the draft to make it your own and convey your voice, then send onwards to the hiring manager, getting one step closer to your next opportunity,\u201D the exec added. Then, there\u2019s the biggest caveat: The messaging feature is only being piloted among LinkedIn\u2019s Premium customers, who pay a rather hefty subscription fee. In 2023, the paid membership tier starts at $39.99 per month. Reminder: accessing ChatGPT itself is free if you sign up through OpenAI\u2019s website. Moreover, LinkedIn\u2019s AI messaging upgrades aren\u2019t yet available to all Premium users. The rollout is beginning this week, and will take time per Levit\u2019s post. As a journalist, I have free LinkedIn Premium access. Yet I don\u2019t currently see the option to \u201Clet AI draft a message to the hiring team\u201D among my recommended job listings. \u201CWe\u2019re initially testing this feature with a select group of Premium members as we collect feedback,\u201D a LinkedIn spokesperson, Abby Semcken, told Gizmodo in an email. In many ways, rumors of artificial intelligence\u2019s intelligence have been greatly exaggerated. Large language models like ChatGPT can produce fluid text quickly on virtually any subject in a wide range of tones. But the chatbots cannot yet achieve accuracy nor write particularly inspiring prose. Basically, these AIs aggregate concepts and language from their training and regurgitate what may or may not be a factually correct, finely sorted alphabet soup. It\u2019s impressive in many ways, but it\u2019s probably not the end of all human writing and creative endeavor\u2014as much as media CEOs and studio execs might want it to be. All that said, cover letters are inarguably one of the lowest forms of the written word. Composing a cover letter and any similar professional communication often amounts to little more than repeating information from your clear, bulleted resume in overwrought paragraph form. It is a tedious exercise that only really demonstrates your willingness to undertake tedious exercises. In other words: it\u2019s a perfect task for ChatGPT. People across the internet have already discovered generative AI\u2019s use for lessening the work of job applications. LinkedIn is officially on board with the idea. The professional networking site has been owned by Microsoft since 2016. Then, this year, Microsoft spent billions on a partnership with OpenAI, the company behind ChatGPT. Microsoft has been integrating OpenAI\u2019s buzzy chatbot tool across its platforms and properties, LinkedIn included. Already, the site had introduced a feature to help users build their profiles with AI and improved its suggested messaging response aid. Now, LinkedIn\u2019s beefed-up generative AI tools extend to drafting complete, direct communications. Assuming the feedback goes well and LinkedIn expands the feature to all paid users, it could be a useful timesaver for a chunk of people on the platform. The work of introducing oneself to a prospective employer might be reduced to just a couple of clicks. On the flip side, the feature could just end up filling hiring managers\u2019 LinkedIn inboxes with endless spam. Messaging through LinkedIn is mercifully limited via a credit system, but the ability for individuals to send even five AI-generated messages a month could easily snowball into a problem for those on the receiving end. But LinkedIn doesn\u2019t see this as an issue. \u201CThe tool was built to solve the blank page problem and help everyone put their best step forward,\u201D Semcken wrote. \u201CRather than lead to an overload of messages for hiring managers, initial outreach will be more informed and contextual,\u201D she assured. Which suggests that hiring managers are currently being inundated by uninformed, irrelevant drivel. Based on the state of my LinkedIn inbox, with no jobs to offer anyone, it seems possible. In other words: Maybe AI can help make LinkedIn messages less spammy.",
    "publish_date": "2023-05-02 23:00:19"
  },
  {
    "title": "Amid a Heated A.I. Race, Apple Struggles to Retain Top Talent",
    "text": "The heated Big Tech artificial intelligence race is making Apple very nervous. Stagnant product development and lagging research in large language models (LLMs), the underlying technology powering applications like ChatGPT, have hampered the iPhone maker\u2019s ability to retain top talent and introduce a meaningful A.I. product to compete with Microsoft and Google. Although Apple\u2019s business is more focused on hardware and less reliant on web search\u2014a key area of generative A.I. application\u2014than Google and Microsoft, the opportunities afforded by recent breakthroughs in A.I. are apparently too important to miss for the world\u2019s most valuable tech company. In recent months, Apple engineers, including members of the \u200CSiri\u200C team, have been testing GPT-like language-generation concepts on a weekly basis, the New York Times reported in March. At the center of Apple\u2019s A.I. effort is a team led by John Giannandrea, the company\u2019s head of machine learning and A.I. strategy. Giannandrea, a former tech executive at Google, has been leading Apple\u2019s A.I. projects, including voice assistant Siri, since April 2018. However, as A.I. competition intensifies among Big Tech companies, Giannandrea\u2019s team is embattled in a talent war with competitors. Late last year, the Siri team lost three star engineers to Google, the Information reported on April 27. Srinivasan Venkatachary, Steven Baker and Anand Shukla all joined Apple in November 2018 under Giannandrea\u2019s leadership. They left between October and November 2022 to work on Google\u2019s A.I. projects, according to their LinkedIn profiles. Top engineers believe Google is a better place to work Google CEO Sundar Pichai personally wooed the group. While Apple CEO Tim Cook tried to persuade them to stay, Venkatachary, Baker and Shukla believed Google was a better place to work on LLMs, according to anonymous sources speaking to the Information. Apple hasn\u2019t responded to an inquiry to comment on the three engineers\u2019 departures. Venkatachary and Baker now both hold the title of VP of engineering at Google. Venkatachary\u2019s work focuses on \u201CA.I. product expansion,\u201D while Baker is working on \u201Cnew stuff,\u201D according to their LinkedIn pages. Shukla has assumed the title of distinguished engineer, a high-level engineering position at Google. John Burkey, a former Apple engineer on the \u200CSiri\u200C team between 2014 and 2015, told the New York Times in March that the Siri\u200C voice assistant is built on \u201Cclunky code\u201D that made it very difficult for engineers to add new features. As a result, there was no path for \u200CSiri\u200C to become a \u201Ccreative assistant\u201D like ChatGPT, Burkey said. Microsoft CEO Satya Nadella expressed similar views on voice assistant products in general. Voice assistants are \u201Cdumb as a rock,\u201D Nadella said in an interview with the Financial Times in March. A.I. hype fuels the talent war in tech The past 12 months have been marked by unprecedented cost-cutting measures across the tech sector, with Google and many large tech companies laying off tens of thousands of employees and cutting back on office perks. Interestingly, Apple is the only Big Tech firm that has avoided massive layoffs, and yet stability hasn\u2019t stopped its top engineers from leaving for more rewarding jobs. It\u2019s not just Apple losing talent to Google. Earlier this year, the Information reported Google\u2019s top A.I. scientists were quitting to join OpenAI because they believed what OpenAI was working on was more promising. Yesterday (May 1), University of Toronto professor Geoffrey Hinton, who is known in the industry as \u201Cthe godfather of A.I.,\u201D left his part-time advisory role at Google, fearing the tech company was moving too quickly without considering the social impact of A.I. A.I. scientists and engineers are often among the highest-paid roles at tech companies. And large firms in the industry offer similarly lucrative compensation packages. At Apple, for example, a median software engineer makes $287,000 a year, including salary, bonus and stock awards, according to levels.fyi, a tech salary tracking site. But money is usually not the top consideration when they choose where to work. \u201CScientists and engineers do go after high compensations,\u201D Kyunghyun Cho, a data science professor at New York University and former research scientist at Facebook AI Research, told Observer. \u201CBut, at the end of the day, what they are looking for is an environment where they can flourish and contribute to a success.\u201D",
    "publish_date": "2023-05-02 20:55:46"
  },
  {
    "title": "Generative A.I. Start-Up Cohere Valued At About $2 Billion In Funding Round - WorldNewsEra",
    "text": "Cohere was founded by Aidan Gomez and Nick Frosst, two Canadian researchers who had worked on artificial intelligence at Google, and Ivan Zhang, a Toronto entrepreneur. Mr. Gomez was among the Google researchers who published a key research paper that helped lead to ChatGPT and similar technologies. ChatGPT has captured the imagination of millions of people with its ability to do things like answer questions, write term papers and poetry, and generate computer code. As the chatbot\u2019s popularity has grown, the tech industry has focused on generative artificial intelligence \u2014 technologies that can generate text, images and other media in response to short prompts. Many companies are exploring the fringes of this new area, but only a few have the resources to build the technologies from the ground up. These companies have an unusual blend of experienced researchers, enormous ambition and large amounts of money. Though investors have been reluctant to fund other start-ups, they have been pouring money into the few companies at the forefront of generative A.I. In February, Microsoft invested $10 billion in OpenAI, bringing its total investment in the company to $13 billion. And in March, Character.ai, another start-up that builds online chatbots, raised $150 million in a funding round that valued the company at $1 billion.",
    "publish_date": "2023-05-02 23:13:41"
  },
  {
    "title": "\u2018Godfather of AI\u2019 leaves Google, warns of \u2018scary\u2019 technology he helped create",
    "text": "Harris and administration officials plan to tell the corporate leaders that they have a responsibility to mitigate potential harm from AI tools, according to a White House official. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools \u2014 including Google\u2019s \u201CBard.\u201D Some of the dangers of AI chatbots are \u201Cquite scary,\u201D Hinton told the BBC. \u201CRight now, they\u2019re not more intelligent than us, as far as I can tell. But I think they soon may be.\u201D In an interview with MIT Technology Review, Hinton also pointed to \u201Cbad actors\u201D that may use AI in ways that could have detrimental impacts on society \u2014 such as manipulating elections or instigating violence.",
    "publish_date": "2023-05-02 21:47:57"
  },
  {
    "title": "Hoffman and Suleyman\u0027s AI startup Inflection launches ChatGPT-like chatbot",
    "text": "Inflection AI, the artificial intelligence startup founded by LinkedIn co-founder Reid Hoffman and Google DeepMind co-founder Mustafa Suleyman, has released its first AI chatbot product, the company said on Tuesday. Similar to OpenAI\u0027s viral chatbot ChatGPT, Inflection\u0027s AI chatbot, named Pi, uses generative AI technology to interact with users through dialogues, in which people can ask questions and share feedback. Suleyman, Inflection AI\u0027s CEO, said the startup developed the technology in-house and its Pi chatbot was built on prioritizing human-like conversations with a high level of emotional intelligence, including being kind and supportive. \u0022It\u0027s very balanced and even-handed on political issues or sensitive topics, but also sometimes it can be funny and silly and creative,\u0022 Suleyman said. The chatbot is suitable for personal day-to-day tasks, but not for generating code or essays, he added. Suleyman said the company had also spent time on boundary training to make sure the AI did not violate its behavior policies, including engaging in romantic conversations. \u0022The goal is to make sure that the AI always knows it\u0027s an AI and never tries to imitate a human. So it reminds the human user that it is an AI frequently,\u0022 he said. Users can interact with Pi across platforms including its website, app and social media platforms like Instagram. The service is free, and the startup may launch premium subscriptions in the future, Suleyman added. Pi uses user data, including conversational content, to train its AI systems, according to its terms of service. The chatbot is not currently connected to the internet. Chatbots powered by generative AI technology has become a crowded field since OpenAI\u0027s ChatGPT burst into the scene last November. Using large language models, which mine vast amounts of text to summarize information and generate content, chatbots like Google\u0027s Bard and Character.AI enable people to have in-depth conversations for both professional and personal needs. Founded in 2022, Inflection was incubated by VC firm Greylock, which led a $225 million investment in the startup. Co-founder Hoffman, a partner at Greylock who is also a Microsoft Corp board member, resigned from OpenAI\u0027s board in March, citing potential conflicts due to his work with AI startups.",
    "publish_date": "2023-05-02 23:46:00"
  },
  {
    "title": "Godfather of AI\u0027 quits Google to warn of the tech\u0027s dangers",
    "text": "A computer scientist often dubbed \u0022the godfather of artificial intelligence\u0022 has quit his job at Google to speak out about the dangers of the technology.. Geoffrey Hinton, who created a foundation technology for AI systems, told The New York Times that advancements made in the field posed \u0022profound risks to society and humanity\u0022. \u0022Look at how it was five years ago and how it is now,\u0022 he was quoted as saying in the piece, which was published on Monday. \u0022Take the difference and propagate it forwards. That\u0027s scary.\u0022 Hinton said that competition between tech giants was pushing companies to release new AI technologies at dangerous speeds, risking jobs and spreading misinformation. \u0022It is hard to see how you can prevent the bad actors from using it for bad things,\u0022 he told the Times. In 2022, Google and OpenAI -- the start-up behind the popular AI chatbot ChatGPT -- started building systems using much larger amounts of data than before. Hinton told the Times he believed that these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. \u0022Maybe what is going on in these systems is actually a lot better than what is going on in the brain,\u0022 he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \u0022takes away the drudge work\u0022 but \u0022might take away more than that\u0022, he told the Times. The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will \u0022not be able to know what is true anymore.\u0022 Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to U.S. media. \u0022As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\u0022 the statement added. \u0022We\u0027re continually learning to understand emerging risks while also innovating boldly.\u0022 In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not \u0022scale this up more until they have understood whether they can control it.\u0022 \u00A9 2023 AFP",
    "publish_date": "2023-05-02 23:21:40"
  },
  {
    "title": "Chegg CEO calls 48% stock plunge over ChatGPT fears \u0027extraordinarily overblown\u0027",
    "text": "Chegg\u0027s 48% stock price plunge on Tuesday, driven by comments in the company\u0027s earnings report about the risks of artificial intelligence, was \u0022extraordinarily overblown,\u0022 CEO Dan Rosensweig told CNBC Tuesday. The shares rose as much as 8% in extended trading during Rosensweig\u0027s TV interview, which followed the historic drop during regular market hours. On Monday\u0027s earnings call, Rosensweig said ChatGPT, the suddenly popular chatbot from startup OpenAI, was \u0022having an impact on our new customer growth rate.\u0022 The company, which initially became well known for developing a textbook rental model for college students, has expanded into homework and exam help products. Chegg said it was only providing guidance for the coming quarter and not for the full year because it\u0027s \u0022too early to tell how this will play out.\u0022 Rosensweig reminded investors, during the CNBC interview, that Chegg generates free cash flow and earnings, on an adjusted basis, and has \u0022more than enough cash to pay off our debt.\u0022 The company also reported better-than-expected earnings and revenue for the first quarter. \u0022I think this is extraordinarily overblown, and I don\u0027t normally say that, I don\u0027t really talk about the stock price much,\u0022 Rosensweig said. Chegg is slated to launch Cheggmate, its GPT-4 powered AI platform, in May. Rosensweig said the combination of GPT and Chegg\u0027s trove of academic data could be transformative. Rosensweig noted that ChatGPT struggles with delivering accurate answers, a phenomenon known as hallucination, and a problem in the academic world. \u0022Students can\u0027t be wrong when they do homework or when they learn things,\u0022 he said. \u0022ChatGPT is often wrong, and it\u0027s not going to be right anytime soon.\u0022",
    "publish_date": "2023-05-02 23:14:36"
  },
  {
    "title": "\u2018Godfather of AI\u2019 leaves Google, warns of \u2018scary\u2019 technology he helped create",
    "text": "Harris and administration officials plan to tell the corporate leaders that they have a responsibility to mitigate potential harm from AI tools, according to a White House official. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools \u2014 including Google\u2019s \u201CBard.\u201D Some of the dangers of AI chatbots are \u201Cquite scary,\u201D Hinton told the BBC. \u201CRight now, they\u2019re not more intelligent than us, as far as I can tell. But I think they soon may be.\u201D In an interview with MIT Technology Review, Hinton also pointed to \u201Cbad actors\u201D that may use AI in ways that could have detrimental impacts on society \u2014 such as manipulating elections or instigating violence.",
    "publish_date": "2023-05-02 21:47:57"
  },
  {
    "title": "ChatGPT returns to Italy after OpenAI tweaks privacy disclosures, controls",
    "text": "ChatGPT is again available to users in Italy, after being temporarily banned by the country\u0027s data privacy authority for possible violations of the EU\u0027s General Data Protection Regulation (GDPR). Italy\u0027s Guarantor for the Protection of Personal Data announced the reinstatement of ChatGPT Friday, after Microsoft-backed OpenAI, the creator of the generative AI service, made changes requested by the government body. At the end of March, the Guarantor ordered OpenAI to stop processing ChatGPT data in Italy, effectively causing the service to shut down in the country. On April 11, the data privacy agency notified OpenAI of specific changes it would have to make in order to offer ChatGPT in Italy. Most of those changes have been met, and OpenAI now has conditional approval to offer ChatGPT in Italy, with the expectation that it will make further changes specified by the Guarantor, and continue to adhere to EU data privacy rules. The changes OpenAI has implemented include: The publication on OpenAI\u0027s website of a description of the personal data that is processed for its AI model training algorithms, and a reminder that everyone has the right to opt-out from such processing, The posting of a form designed to let European users opt-out from the processing of their personal data, and The addition of a button on a welcome back page for Italian users that asks for confirmation that they are 18 years old or are above the age of 13 and have obtained consent from their parents or guardians to use the service. \u0022The Italian SA acknowledges the steps forward made by OpenAI to reconcile technological advancements with respect for the rights of individuals and it hopes that the company will continue in its efforts to comply with European data protection legislation,\u0022 the Guarantor said in its announcement Friday. In its March notification of the ChatGPT ban, the data privacy authority noted that a bug in an open-source library \u2014 disclosed by OpenAI in March but since fixed \u2014 allowed some ChatGPT users to see titles from another active user\u2019s chat history. It also noted that \u0022information made available by ChatGPT does not always match factual circumstances, so that inaccurate personal data are processed.\u0022 While OpenAI\u0027s changes have gone far enough for Italy\u0027s Guarantor to allow it operate in Italy, it expects the company to comply with additional requests, including implementation of a stronger age-confirmation mechanism, and an information campaign about the rights of Italians to opt out of the processing of their personal data for the purpose of training AI algorithms. The Italian ban and subsequent reinstatement of ChatGPT come as governments around the world consider regulations on AI. In the West, Europeans have been the first to take concrete steps to rein in AI. Last week, members of the European Parliament (MEPs) agreed on compromise amendments to the AI Act proposed by the European Commission. The act focuses on classifying AI systems into risk-based categories \u2014 banning those that are considered high risk from being used for certain purposes, such as government social-scoring systems and real-time biometric identification systems in public spaces. Earlier in April, The European Data Protection Board (EDPB) said it planned to launch a dedicated task force to investigate ChatGPT after a number of European privacy watchdogs \u2014 including Italy \u2014 raised concerns about whether the technology is compliant with the GDPR. In its announcement about the reinstatement of ChatGPT in Italy, the Guarantor said that it would \u0022carry on its fact-finding activities regarding OpenAI also under the umbrella of the ad-hoc task force that was set up by the European Data Protection Board.\u0022",
    "publish_date": "2023-05-01 18:24:00"
  },
  {
    "title": "Samsung bans staff AI use over data leak concerns",
    "text": "Samsung has reportedly banned employee use of generative AI tools like ChatGPT in a bid to stop transmission of sensitive internal data to external servers. The South Korean electronics giant issued a memo to a key division, notifying employees not to use AI tools, according to a report by Bloomberg, which said it reviewed the memo. Bloomberg did not report which division received the memo. In addition, employees using ChatGPT and other AI tools on personal devices were warned to not upload company related data or other information that could compromise the company\u0027s intellectual property. Doing so, the memo said, could result in employment termination. The memo expressed concerns over inputting data such as sensitive code on AI platforms. The worry is that anything that is typed onto an AI tool like ChatGPT will then reside on external servers, which makes retrieving and deleting them very difficult, and also potentially making them accessible by other users. \u201CInterest in generative AI platforms such as ChatGPT has been growing internally and externally,\u201D the memo said. \u201CWhile this interest focuses on the usefulness and efficiency of these platforms, there are also growing concerns about security risks presented by generative AI.\u201D The memo comes in the wake of a March notification by Microsoft-backed OpenAI, the creator of ChatGPT, that a bug in an open-source library \u2014 since fixed \u2014 allowed some ChatGPT users to see titles from another active user\u2019s chat history. Samsung\u2019s ban on the tool also comes a month after an internal survey it conducted to understand the security risks associated with AI. About 65% of employees surveyed said ChatGPT posed serious security threats. In addition, in April, Samsung engineers \u201Caccidentally leaked internal source code by uploading it to ChatGPT,\u201D according to the memo. The memo did not, however, reveal what the code was, precisely, and did not elaborate on whether the code was simply typed into ChatGPT, or whether it was also inspected by anyone external to Samsung. Lawmakers set to regulate AI Fearing the potential ChatGPT and other AI systems to leak private data and spread false information, regulators have begun to consider restrictions on their use. The European Parliament, for instance, is days away from finalizing an AI Act, and the European Data Protection Board (EDPB) is assembling an AI task force, focusing on ChatGPT, to examine potential AI dangers. Last month, Italy imposed privacy-based restrictions on ChatGPT and temporarily banned its operation in the country. OpenAI agreed to make changes requested by Italian regulators, after which it relaunched the service. Companies that offer AI tools are starting to respond to concerns about privacy and data leakage. OpenAI last month announced that it would allow users to turn off the chat history feature for ChatGPT. The \u201Chistory disabled\u201D feature means that conversations marked as such won\u2019t be used to train OpenAI\u2019s underlying models, and won\u2019t be displayed in the history sidebar, the comany said. Samsung, meanwhile, is working on internal AI tools for translating and summarizing documents as well as for software development, according to media reports. It\u2019s also working on ways to block the upload of sensitive company information to external services. \u201CHQ is reviewing security measures to create a secure environment for safely using generative AI to enhance employees\u2019 productivity and efficiency,\u201D the memo said. \u201CHowever, until these measures are prepared, we are temporarily restricting the use of generative AI.\u201D With this move Samsung joins the expanding group of companies that have exercised some form of restriction on this disruptive technology. Among them are Wall Street banks including JPMorgan Chase, Bank of America, and CitiGroup.",
    "publish_date": "2023-05-02 17:39:00"
  },
  {
    "title": "\u2018Godfather Of AI\u2019 Geoffrey Hinton Exits Role At Google To Warn Of Technology\u2019s Dangers - WorldNewsEra",
    "text": "Sounding alarms about artificial intelligence has become a popular pastime in the ChatGPT era, taken up by high-profile figures as varied as industrialist Elon Musk, leftist intellectual Noam Chomsky and the 99-year-old retired statesman Henry Kissinger. But it\u2019s the concerns of insiders in the AI research community that are attracting particular attention. A pioneering researcher and the so-called \u201CGodfather of AI\u201D Geoffrey Hinton quit his role at Google so he could more freely speak about the dangers of the technology he helped create. Over his decades-long career, Hinton\u2019s pioneering work on deep learning and neural networks helped lay the foundation for much of the AI technology we see today. There has been a spasm of AI introductions in recent months. San Francisco-based startup OpenAI, the Microsoft-backed company behind ChatGPT, rolled out its latest artificial intelligence model, GPT-4, in March. Other tech giants have invested in competing tools \u2014 including Google\u2019s \u201CBard.\u201D Some of the dangers of AI chatbots are \u201Cquite scary,\u201D Hinton told the BBC. \u201CRight now, they\u2019re not more intelligent than us, as far as I can tell. But I think they soon may be.\u201D In an interview with MIT Technology Review, Hinton also pointed to \u201Cbad actors\u201D that may use AI in ways that could have detrimental impacts on society \u2014 such as manipulating elections or instigating violence. Hinton, 75, says he retired from Google so that he could speak openly about the potential risks as someone who no longer works for the tech giant. \u201CI want to talk about AI safety issues without having to worry about how it interacts with Google\u2019s business,\u201D he told MIT Technology Review. \u201CAs long as I\u2019m paid by Google, I can\u2019t do that.\u201D Since announcing his departure, Hinton has maintained that Google has \u201Cacted very responsibly\u201D regarding AI. He told MIT Technology Review that there\u2019s also \u201Ca lot of good things about Google\u201D that he would want to talk about \u2014 but those comments would be \u201Cmuch more credible if I\u2019m not at Google anymore.\u201D Google confirmed that Hinton had retired from his role after 10 years overseeing the Google Research team in Toronto. Hinton declined further comment Tuesday but said he would talk more about it at a conference Wednesday. At the heart of the debate on the state of AI is whether the primary dangers are in the future or present. On one side are hypothetical scenarios of existential risk caused by computers that supersede human intelligence. On the other are concerns about automated technology that\u2019s already getting widely deployed by businesses and governments and can cause real-world harms. \u201CFor good or for not, what the chatbot moment has done is made AI a national conversation and an international conversation that doesn\u2019t only include AI experts and developers,\u201D said Alondra Nelson, who until February led the White House Office of Science and Technology Policy and its push to craft guidelines around the responsible use of AI tools. \u201CAI is no longer abstract, and we have this kind of opening, I think, to have a new conversation about what we want a democratic future and a non-exploitative future with technology to look like,\u201D Nelson said in an interview last month. A number of AI researchers have long expressed concerns about racial, gender and other forms of bias in AI systems, including text-based large language models that are trained on huge troves of human writing and can amplify discrimination that exists in society. \u201CWe need to take a step back and really think about whose needs are being put front and center in the discussion about risks,\u201D said Sarah Myers West, managing director of the nonprofit AI Now Institute. \u201CThe harms that are being enacted by AI systems today are really not evenly distributed. It\u2019s very much exacerbating existing patterns of inequality.\u201D Hinton was one of three AI pioneers who in 2019 won the Turing Award, an honor that has become known as tech industry\u2019s version of the Nobel Prize. The other two winners, Yoshua Bengio and Yann LeCun, have also expressed concerns about the future of AI. Bengio, a professor at the University of Montreal, signed a petition in late March calling for tech companies to agree to a 6-month pause on developing powerful AI systems, while LeCun, a top AI scientist at Facebook parent Meta, has taken a more optimistic approach.",
    "publish_date": "2023-05-03 01:08:15"
  },
  {
    "title": "Nextdoor Unveiled New AI Assistant, Built on OpenAI\u0026#039;s ChatGPT Models",
    "text": "The American social networking service operating hyperlocal for neighborhoods, Nextdoor Holdings has introduced its first generative AI feature, an in-app assistant that can help users rewrite potentially unkind posts on the neighborhood social network. According to the firm, the new feature will be rolled out over the next several weeks. The firm has experimented with methods to remind consumers to keep conversations civil in the past. As we know, the firm has occasionally found it difficult to combat the idea that its platform may be poisonous, started employing kindness reminders in 2019, and launched pop-ups asking users to be more sympathetic last year. Nextdoor New AI Assistant According to the social networking service, the app now offers more focused prods to encourage non-racist speech and temperate political debates. The new assistant expands on the strategy. The assistant will urge users to rephrase potentially hurtful statements rather than reminding them in advance and offer new language for the topic. Nextdoor CEO Sarah Friar said the new AI assistant it\u2019s great to tell people, \u2018Hey, be a little bit more constructive,\u2019 or \u2018You don\u2019t always have to respond. \u201CBut now you\u2019re actually helping them reframe it in a way they might not have thought of, She added. The assistant-written posts are optional, and users can edit the suggested content, according to Friar. According to her, it\u2019s stating that if you phrase things in this way, or even if you just add a few additional phrases, some context, or an emoji, you can make someone more likely to understand you. Even if they don\u2019t agree with you, they can at least start to hear you. Nextdoor New AI Assistant is Built on OpenAI\u2019s ChatGPT According to Friar, the new AI assistance is built using the same OpenAI models as ChatGPT and has been trained using all of the information we have amassed throughout our nearly ten-year existence. This, according to her, enables the assistant to make more specific suggestions for various posts on the platform. In a sample given by the firm, the assistant rewrites a user\u2019s post asking for landscaping assistance with fresh language that the app claims may receive more responses than the original. The assistant may be Nextdoor\u2019s first application of generative AI, but it is unlikely to be the last. According to Friar, she is especially curious about how generative AI may be applied to platform suggestions for small enterprises.",
    "publish_date": "2023-05-02 14:04:30"
  },
  {
    "title": "\u64D4\u5FC3\u804A\u5929\u7D00\u9304\u88AB ChatGPT \u51FA\u8CE3\uFF1F\u5B98\u65B9\u6700\u65B0\u300C\u4E00\u8A2D\u5B9A\u300D\u6700\u597D\u8981\u95DC\u9589",
    "text": "\u5373\u6642 \u71B1\u9580 \u653F\u6CBB \u8ECD\u6B66 \u793E\u6703 \u751F\u6D3B \u5065\u5EB7 \u570B\u969B \u5730\u65B9 \u8490\u5947 \u5F71\u97F3 \u8CA1\u7D93 \u5A1B\u6A02 \u6C7D\u8ECA \u6642\u5C1A \u9AD4\u80B2 3 C \u8A55\u8AD6 \u85DD\u6587 \u73A9\u5496 \u98DF\u8B5C \u5730\u7522 \u5C08\u5340 TAIPEI TIMES \u6C42\u8077 \u7206 Search \u81EA\u7531\u96FB\u5B50\u5831 3C\u79D1\u6280 3C\u9996\u9801 \u5F71\u97F3\u5C08\u5340 \u667A\u6167\u624B\u6A5F \u5BE6\u7528\u79D8\u6280 \u79D1\u6280\u8DA3\u805E \u7DB2\u8DEF\u793E\u7FA4 \u597D\u651D\u76F8\u6A5F \u96FB\u8166\u61C9\u7528 \u5BB6\u96FB\u5A1B\u6A02 \u7C89\u7D72\u5718 \u81EA\u7531\u5F71\u97F3 \u5373\u6642 \u71B1\u9580 \u653F\u6CBB \u8ECD\u6B66 \u793E\u6703 \u751F\u6D3B \u5065\u5EB7 \u570B\u969B \u5730\u65B9 \u8490\u5947 \u8CA1\u7D93 \u5A1B\u6A02 \u85DD\u6587 \u6C7D\u8ECA \u6642\u5C1A \u9AD4\u80B2 3 C \u8A55\u8AD6 \u73A9\u5496 \u98DF\u8B5C \u5730\u7522 \u5C08\u5340 \u670D\u52D9 \u81EA\u7531\u96FB\u5B50\u5831APP \u81EA\u7531\u96FB\u5B50\u5831\u7C89\u7D72\u5718 \u81EA\u7531\u96FB\u5B50\u5831Line \u81EA\u7531\u96FB\u5B50\u5831Twitter \u71B1\u9580\u65B0\u8A0A \u5DF2\u7D93\u52A0\u597D\u53CB\u4E86\uFF0C\u8B1D\u8B1D \u6B61\u8FCE\u52A0\u5165\u3010\u81EA\u75313C\u79D1\u6280\u3011 \u6309\u500B\u8B9A\u3000\u5FC3\u60C5\u597D \u5DF2\u7D93\u6309\u8B9A\u4E86\uFF0C\u8B1D\u8B1D\u3002 3C \u3009 \u7DB2\u8DEF\u793E\u7FA4 \u3009 \u7DB2\u8DEF\u670D\u52D9 \u5BB3\u6015ChatGPT\u5916\u6D29\u79C1\u5BC6\uFF1F\u50B3\u5FAE\u8EDF\u6253\u9020\u300C\u79C1\u6709\u7248\u300D\u6536\u8CBB\u9AD810\u500D 2023/05/03 08:02 \u6587\uFF0F\u8A18\u8005\u5433\u4F69\u6A3A \u50B3\u805E\u5FAE\u8EDF\u6E96\u5099\u6253\u9020\u300C\u79C1\u6709\u7248ChatGPT\u300D\u3002(\u5716\uFF0F\u8DEF\u900F\u793E) OpenAI\u7684AI\u804A\u5929\u6A5F\u5668\u4EBAChatGPT\u7206\u7D05\uFF0C\u4F46\u5728\u4FDD\u5BC6\u65B9\u9762\u4E5F\u53D7\u5230\u8CEA\u7591\uFF0C\u56E0\u6B64\u50B3\u51FA\u6709\u4E00\u4E9B\u5927\u578B\u4F01\u696D\u7981\u6B62\u54E1\u5DE5\u4F7F\u7528ChatGPT\uFF0C\u907F\u514D\u6D29\u9732\u516C\u53F8\u6A5F\u5BC6\u3002 \u5916\u5A92The Information\u5F15\u8FF0\u77E5\u60C5\u4EBA\u58EB\u5831\u5C0E\uFF0C\u5FAE\u8EDFAzure\u96F2\u7AEF\u670D\u52D9\u90E8\u9580\u8A08\u5283\u6253\u9020\u300C\u79C1\u6709\u7248ChatGPT\u300D\uFF0C\u64C1\u6709\u5C08\u9580\u7684\u96F2\u7AEF\u4F3A\u670D\u5668\uFF0C\u7528\u4F86\u7368\u7ACB\u4FDD\u5B58\u6578\u64DA\uFF0C\u8207\u5176\u4ED6\u7528\u6236\u7684\u6578\u64DA\u5206\u958B\uFF0C\u85C9\u6B64\u8B93\u7528\u6236\u5B89\u5FC3\uFF0C\u4FDD\u8B49\u79D8\u5BC6\u4E0D\u6703\u6D41\u5230ChatGPT\u4E3B\u7CFB\u7D71\uFF0C\u4E0D\u904E\u50F9\u683C\u53EF\u80FD\u4E26\u4E0D\u4FBF\u5B9C\uFF0C\u9810\u8A08\u5C07\u662F\u73FE\u6709\u4ED8\u8CBB\u7248ChatGPT\u768410\u500D\u4E4B\u591A\u3002 \u8ACB\u7E7C\u7E8C\u5F80\u4E0B\u95B1\u8B80... \u6B64\u5916\uFF0C\u70BA\u4E86\u964D\u4F4E\u5404\u754C\u64D4\u6182\u7684\u96B1\u79C1\u554F\u984C\uFF0CChatGPT\u5728\u4E0A\u6708\u63A8\u51FA\u95DC\u9589\u804A\u5929\u8A18\u9304\u65B0\u529F\u80FD\uFF0C\u7528\u6236\u53EF\u81EA\u884C\u6C7A\u5B9A\u958B\u555F\u6216\u95DC\u9589\uFF0C\u95DC\u9589\u5F8C\u6240\u6709\u7684\u5C0D\u8A71\u53EA\u6703\u7559\u5B5830\u5929\uFF0C\u904E\u5F8C\u6C38\u4E45\u522A\u9664\uFF0C\u5B98\u65B9\u4FDD\u8B49\u555F\u7528\u8A72\u529F\u80FD\uFF0C\u6240\u6709\u5C0D\u8A71\u4E0D\u6703\u88AB\u7528\u4F86\u8A13\u7DF4\u548C\u6539\u9032AI\u3002 \u300A\u4F60\u53EF\u80FD\u9084\u60F3\u770B\u300B \u64D4\u5FC3\u804A\u5929\u7D00\u9304\u88AB ChatGPT \u51FA\u8CE3\uFF1F\u5B98\u65B9\u6700\u65B0\u300C\u4E00\u8A2D\u5B9A\u300D\u6700\u597D\u8981\u95DC\u9589 \u4E0D\u7528\u62BD \u4E0D\u7528\u6436 \u73FE\u5728\u7528APP\u770B\u65B0\u805E \u4FDD\u8B49\u5929\u5929\u4E2D\u734E\u3000 \u9EDE\u6211\u4E0B\u8F09APP\u3000 \u6309\u6211\u770B\u6D3B\u52D5\u8FA6\u6CD5 \u804A\u5929\u6A5F\u5668\u4EBA ChatGPT \u5FAE\u8EDF \u5FAE\u8EDF \u7DB2\u8DEF\u670D\u52D9 \u76F8\u95DC\u65B0\u805E Google \u4E0D\u85CF\u4E86\uFF01\u81EA\u66DD\u672A\u767C\u8868 Pixel \u65B0\u624B\u6A5F\u5BE6\u7167\u3000\u5927\u79C0\u8D85\u7F8E\u65B0\u8272 Google \u4E0D\u85CF\u4E86\uFF01\u5728\u5B98\u65B9\u63A8\u7279\u5927\u65B9\u5730\u79C0\u51FA\u65B0\u624B\u6A5F\u7684\u90E8\u5206\u7167\u7247\uFF0C\u5916\u754C\u9810\u671F\u5C07\u662F\u4E2D\u968E\u5B9A\u4F4D\u7684 Pixel 7a\uFF0C\u4E14\u6B63\u5F0F\u9810\u544A\u5C07\u5728 5...... \u6436\u5148 HTC U23 Pro \u4E0A\u5E02\uFF1F \u5B8F\u9054\u96FB\u795E\u79D8\u65B0\u54C1\u6084\u6084\u901A\u904E NCC \u8A8D\u8B49\u4E86 HTC \u5B8F\u9054\u96FB\u8FD1\u4F86\u9664\u4E86\u5DF2\u906D\u66DD\u5149\u7684\u4E2D\u9AD8\u968E\u8033\u6A5F HTC U23 Pro\uFF0C\u50B3\u51FA\u6E96\u5099\u6B63\u5F0F\u767B\u5834\u4E4B\u5916\uFF0C\u53E6\uFF0C\u9084\u6709\u4E00\u6B3E\u795E\u79D8\u7684\u771F\u7121\u7DDA\u8033\u6A5F\u65B0...... \u6BD4 Xperia 1 V \u66F4\u65D7\u8266\uFF1F\u50B3Sony\u958B\u767C7\u540B\u795E\u79D8\u65B0\u6A5F Sony\u4E0B\u9031\u767C\u8868\u65B0\u65D7\u8266Xperia 1 V\uFF0C\u53EF\u671B\u63DB\u4E0A\u5168\u65B0\u300CLytia\u300D\u611F\u5149\u5143\u4EF6\uFF0C\u5F15\u8D77\u71B1\u70C8\u8A0E\u8AD6\uFF0C\u4E0D\u904E\u537B\u6709\u65B0\u7206\u6599\uFF0CSony\u53EF...... \u71B1\u9580\u6587\u7AE0 Sony\u5931\u5B88\u3001\u5C0F\u7C73\u8DCC\u51FA\u699C\u5916\uFF01\u53F0\u7063\u624B\u6A5F\u5341\u5927\u54C1\u724C\u6392\u884C\u699C\u55AE\u300C\u9ED1\u99AC\u300D\u7AC4\u51FA \u5FA0\u5361\u4E5F\u6551\u4E0D\u4E86\u5C0F\u7C73\uFF01\u5168\u74035\u5927\u624B\u6A5F\u54C1\u724C\u6700\u65B0\u51FA\u8CA8\u91CF\u63ED\u66C9 \u53F0\u7063\u6700\u65B0\u300C\u624B\u6A5F\u5E02\u4F54\u300D\u6392\u884C\u699C\u51FA\u7210\uFF01\u860B\u679C iPhone \u9818\u5148\u512A\u52E2\u5FEB\u6C92\u4E86 \u4E0B\u6708\u767C\u65B0\u65D7\u8266\uFF01Sony\u96C6\u5718\u793E\u9577\u537B\u300C\u60B2\u89C0\u300D\u770B\u5168\u7403\u624B\u6A5F\u5E02\u5834 \u6BD4\u624B\u6A5F\u66F4\u4FBF\u5B9C\uFF01\u83EF\u78A9\u904A\u6232\u638C\u6A5F ROG Ally\u300C\u5E73\u50F9\u6B3E\u552E\u50F9\u300D\u66DD\u5149\u3000\u73A9\u5BB6\u55E8\u7206 HTC \u9996\u5EA6\u5347\u7D1A 1 \u5104\u756B\u7D20\uFF01\u672A\u4E0A\u5E02\u300C\u4E2D\u9AD8\u968E\u65B0\u6A5F\u300D\u771F\u5BE6\u7167\u7247\u73FE\u8EAB \u770B\u66F4\u591A\uFF01\u52A0\u51653C\u79D1\u6280\u7C89\u7D72\u5718 \u7DB2\u53CB\u56DE\u61C9",
    "publish_date": "2023-05-03 02:13:58"
  },
  {
    "title": "\u0027Godfather of AI\u0027 Quits Google to Warn of the Technology\u0027s Dangers - The Chosun Ilbo (English Edition): Daily News from Korea - World",
    "text": "\u25C6 Jobs Could Be at Risk In 2022, Google and OpenAI -- the start-up behind the popular AI chatbot ChatGPT -- started building systems using much larger amounts of data than before. Hinton told the Times he believed these systems were eclipsing human intelligence in some ways because of the amount of data they were analyzing. \u0022Maybe what is going on in these systems is actually a lot better than what is going on in the brain,\u0022 he told the paper. While AI has been used to support human workers, the rapid expansion of chatbots like ChatGPT could put jobs at risk. AI \u0022takes away the drudge work\u0022 but \u0022might take away more than that,\u0022 he told the Times. \u25C6 Concern About Misinformation The scientist also warned about the potential spread of misinformation created by AI, telling the Times that the average person will \u0022not be able to know what is true anymore.\u0022 Hinton notified Google of his resignation last month, the Times reported. Jeff Dean, lead scientist for Google AI, thanked Hinton in a statement to U.S. media. \u0022As one of the first companies to publish AI Principles, we remain committed to a responsible approach to AI,\u0022 the statement added. \u0022We\u0027re continually learning to understand emerging risks while also innovating boldly.\u0022 In March, tech billionaire Elon Musk and a range of experts called for a pause in the development of AI systems to allow time to make sure they are safe. An open letter, signed by more than 1,000 people. including Musk and Apple co-founder Steve Wozniak, was prompted by the release of GPT-4, a much more powerful version of the technology used by ChatGPT. Hinton did not sign that letter at the time, but told The New York Times that scientists should not \u0022scale this up more until they have understood whether they can control it.\u0022",
    "publish_date": "2023-05-03 01:13:03"
  },
  {
    "title": "\u50B3\u5FAE\u8EDF\u7B49\u57F7\u884C\u95774\u65E5\u53D7\u9080\u78CB\u5546AI \u767D\u5BAE\u5927\u5496\u5B98\u54E1\u7686\u51FA\u5E2D",
    "text": "\u5373\u6642 \u71B1\u9580 \u653F\u6CBB \u8ECD\u6B66 \u793E\u6703 \u751F\u6D3B \u5065\u5EB7 \u570B\u969B \u5730\u65B9 \u8490\u5947 \u5F71\u97F3 \u8CA1\u7D93 \u5A1B\u6A02 \u6C7D\u8ECA \u6642\u5C1A \u9AD4\u80B2 3 C \u8A55\u8AD6 \u85DD\u6587 \u73A9\u5496 \u98DF\u8B5C \u5730\u7522 \u5C08\u5340 TAIPEI TIMES \u6C42\u8077 \u7206 Search \u81EA\u7531\u96FB\u5B50\u5831 \u81EA\u7531\u8CA1\u7D93 \u8CA1\u7D93\u9996\u9801 \u8CA1\u7D93\u653F\u7B56 \u5F71\u97F3\u5C08\u5340 \u570B\u969B\u8CA1\u7D93 \u8B49\u5238\u7522\u696D \u623F\u7522\u8CC7\u8A0A \u8CA1\u7D93\u9031\u5831 \u57FA\u91D1\u67E5\u8A62 \u6295\u8CC7\u7406\u8CA1 \u7C89\u7D72\u5718 \u81EA\u7531\u5F71\u97F3 \u5373\u6642 \u71B1\u9580 \u653F\u6CBB \u8ECD\u6B66 \u793E\u6703 \u751F\u6D3B \u5065\u5EB7 \u570B\u969B \u5730\u65B9 \u8490\u5947 \u8CA1\u7D93 \u5A1B\u6A02 \u85DD\u6587 \u6C7D\u8ECA \u6642\u5C1A \u9AD4\u80B2 3 C \u8A55\u8AD6 \u73A9\u5496 \u98DF\u8B5C \u5730\u7522 \u5C08\u5340 \u670D\u52D9 \u81EA\u7531\u96FB\u5B50\u5831APP \u81EA\u7531\u96FB\u5B50\u5831\u7C89\u7D72\u5718 \u81EA\u7531\u96FB\u5B50\u5831Line \u81EA\u7531\u96FB\u5B50\u5831Twitter \u71B1\u9580\u65B0\u8A0A \u9996\u9801 \u003E \u570B\u969B\u8CA1\u7D93 \u50B3\u5FAE\u8EDF\u7B49\u57F7\u884C\u95774\u65E5\u53D7\u9080\u78CB\u5546AI \u767D\u5BAE\u5927\u5496\u5B98\u54E1\u7686\u51FA\u5E2D 2023/05/03 08:55 \u62DC\u767B\u653F\u5E9C\u9080\u8ACBAI\u516C\u53F8\u7684\u57F7\u884C\u9577\u5011\u4E00\u540C\u5546\u8A0E\u4EBA\u5DE5\u667A\u6167\uFF08AI\uFF09\u554F\u984C\u3002\uFF08\u8DEF\u900F\uFF09 \u3014\u8CA1\u7D93\u983B\u9053\uFF0F\u7D9C\u5408\u5831\u5C0E\u3015\u64DA\u767D\u5BAE\u5B98\u54E1\u900F\u9732\uFF0CGoogle\u6BCD\u516C\u53F8Alphabet\u3001\u5FAE\u8EDF\uFF08Microsoft\uFF09\u3001OpenAI\u3001Anthropic\u7684\u57F7\u884C\u9577\u5011\uFF0C\u9031\u56DB\uFF084\u65E5\uFF09\u5C07\u53D7\u9080\u8207\u526F\u7E3D\u7D71\u8CC0\u9326\u9E97\uFF08Kamala Harris\uFF09\u53CA\u5176\u4ED6\u653F\u5E9C\u9AD8\u7D1A\u5B98\u54E1\u5171\u540C\u5546\u8A0E\u4EBA\u5DE5\u667A\u6167\uFF08AI\uFF09\u7684\u554F\u984C\u3002 AI\u6280\u8853\u5FEB\u901F\u767C\u5C55\uFF0C\u96A8\u4E4B\u800C\u4F86\u7684\u554F\u984C\u5305\u62EC\u96B1\u79C1\u3001\u8A50\u9A19\u3001\u5047\u8A0A\u606F\u4EE5\u53CA\u8A0A\u606F\u504F\u5DEE\u7B49\u3002\u6839\u64DA\u300A\u8DEF\u900F\u300B\u770B\u5230\u7684\u9080\u8ACB\u51FD\u986F\u793A\uFF0C\u7E3D\u7D71\u62DC\u767B\u671F\u671B\u50CF\u4E0A\u8FF0\u9019\u6A23\u7684\u516C\u53F8\uFF0C\u5728\u5411\u516C\u773E\u63D0\u4F9BAI\u7522\u54C1\u4E4B\u524D\u53EF\u4EE5\u78BA\u4FDD\u5176\u5B89\u5168\u3002 \u8ACB\u7E7C\u7E8C\u5F80\u4E0B\u95B1\u8B80... \u767D\u5BAE\u5B98\u54E1\u7A31\uFF0C4\u65E5\u7684\u6703\u8B70\u6703\u6709\u767D\u5BAE\u5E55\u50DA\u9577\u9F4A\u5B89\u8332\uFF08Jeff Zients\uFF09\u3001\u526F\u5E55\u50DA\u9577\u5229\u5FB7\uFF08Bruce Reed\uFF09\u3001\u570B\u5BB6\u5B89\u5168\u9867\u554F\u8607\u5229\u6587\uFF08Jake Sullivan\uFF09\u3001\u7D93\u6FDF\u59D4\u54E1\u6703\u4E3B\u5E2D\u5E03\u862D\u7D0D\u5FB7\uFF08Lael Brainard\uFF09\u3001\u5546\u52D9\u90E8\u9577\u96F7\u8499\u591A\uFF08Gina Raimondo\uFF09\u7B49\u91CD\u8981\u5B98\u54E1\u51FA\u5E2D\u3002 \u767D\u5BAE\u4E0A\u6708\u8868\u793A\uFF0C\u56E0\u70BA\u9084\u4E0D\u6E05\u695AAI\u6703\u5C0D\u570B\u5BB6\u5B89\u5168\u548C\u6559\u80B2\u9020\u6210\u4F55\u7A2E\u5F71\u97FF\uFF0C\u6B63\u5C31\u5176AI\u6F5B\u5728\u554F\u8CAC\u63AA\u65BD\u5411\u516C\u773E\u5FB5\u6C42\u610F\u898B\u3002 \u4E00\u624B\u638C\u63E1\u7D93\u6FDF\u8108\u52D5 \u9EDE\u6211\u8A02\u95B1\u81EA\u7531\u8CA1\u7D93Youtube\u983B\u9053 \u4E0D\u7528\u62BD \u4E0D\u7528\u6436 \u73FE\u5728\u7528APP\u770B\u65B0\u805E \u4FDD\u8B49\u5929\u5929\u4E2D\u734E\u3000 \u9EDE\u6211\u4E0B\u8F09APP\u3000 \u6309\u6211\u770B\u6D3B\u52D5\u8FA6\u6CD5 \u5DF2\u7D93\u52A0\u597D\u53CB\u4E86\uFF0C\u8B1D\u8B1D \u6B61\u8FCE\u52A0\u5165\u3010\u81EA\u7531\u8CA1\u7D93\u3011 \u6309\u500B\u8B9A\u3000\u5FC3\u60C5\u597D \u5DF2\u7D93\u6309\u8B9A\u4E86\uFF0C\u8B1D\u8B1D\u3002 \u76F8\u95DC\u65B0\u805E LTN\u7D93\u6FDF\u901A\u300B\u804A\u5929\u6A5F\u5668\u4EBAChatGPT \u5F15\u7206\u8165\u98A8\u8840\u96E8 AI\u6559\u7236\u96E2\u958BGoogle \u8B66\u60D5AI\u5E36\u4F86\u6F5B\u5728\u98A8\u96AA \u6CBF\u8457\u53F086\u7DDA \u7FFB\u8F49\u53F0\u5357\u516C\u8DEF\u7D93\u6FDF \u4E09\u661F\u7981\u751F\u6210\u5F0FAI\u5DE5\u5177 \u88AB\u6293\u5230\u5C31\u958B\u9664 3\u5927\u96F2\u7AEF\u5DE8\u982D \u5728\u53F0\u5DF2\u57F9\u80B2\u903E10\u842C\u6578\u4F4D\u4EBA\u624D \u806F\u624B\u6253\u8A50 \u6797\u53F3\u660C\uFF1A\u8207Google\u90543\u5171\u8B58 \u82F1\u570B\u958B\u767C\u300C\u53EF\u7CBE\u6E96\u8B58\u5225\u764C\u75C7\u300DAI\u6A21\u578B \u76FC\u53CA\u65E9\u767C\u73FE\u638C\u63E1\u9EC3\u91D1\u6CBB\u7642\u6642\u9593 \u770B\u66F4\u591A\uFF01\u8ACB\u52A0\u5165\u81EA\u7531\u8CA1\u7D93\u7C89\u7D72\u5718 \u4ECA\u65E5\u71B1\u9580\u65B0\u805E CNBC\uFF1A\u4E2D\u82AF\u4E0D\u53EF\u80FD\u91CF\u75227\u5948\u7C73\u6676\u7247 \u6676\u7247\u5927\u6230 Google\u524D\u57F7\u884C\u9577\u66DD\u53F0\u7063\u51772\u5927\u512A\u52E2 \u52DE\u52D5\u57FA\u91D1Q1\u5831\u4F73\u97F3 \u5E33\u9762\u5927\u8CFA2190\u5104\u5143 LTN\u7D93\u6FDF\u901A\u300B\u8CA1\u95A5\u4E00\u624B\u906E\u5929 \u5357\u97D3\u8B8A\u5730\u7344\u671D\u9BAE\uFF1F \u5FB7\u5100\u5EAB\u5B58\u7834\u5343\u5104 \u5F6D\u535A\uFF1A\u6676\u7247\u696D\u89F8\u5E95\u5145\u6EFF\u8B8A\u6578 \u6D1B\u514B\u5E0C\u5FB7\u99AC\u4E01\u7372126\u67B6F-35\u5927\u55AE \u50F9\u503C2391\u5104 LTN\u7D93\u6FDF\u901A\u300BFed\u983B\u5347\u606F \u6E2F\u5E63\u88AB\u9001\u7D42\uFF1F \u8DE8\u6027\u5225\u7DB2\u7D05\u696D\u914D\u5F8C \u767E\u5A01\u6DE1\u5564\u92B7\u91CF\u66B4\u8DCC26\uFF05 \u7DB2\u53CB\u56DE\u61C9 \u8CA1\u7D93\u653F\u7B56 \u5F71\u97F3\u5C08\u5340 \u570B\u969B\u8CA1\u7D93 \u8B49\u5238\u7522\u696D \u623F\u7522\u8CC7\u8A0A \u8CA1\u7D93\u9031\u5831 \u57FA\u91D1\u67E5\u8A62 \u6295\u8CC7\u7406\u8CA1",
    "publish_date": "2023-05-03 03:07:58"
  },
  {
    "title": "White House calls in tech firms to talk AI risks",
    "text": "WASHINGTON: The White House plans to meet with top executives from Google, Microsoft, OpenAI and Anthropic on Thursday (May 4) to discuss the promise and risks of artificial intelligence. Vice President Kamala Harris and other US administration officials will discuss ways to ensure consumers benefit from AI while being protected from its harms, according to a copy of an invitation seen by AFP. US President Joe Biden expects tech companies to make sure products are safe before being released to the public, the invitation said. US regulators last month took a step towards drawing up rules on AI that could see the White House put the brakes on new technologies such as ChatGPT. The US Department of Commerce put out a call for input from industry actors that would serve to inform the Biden administration in drafting regulations on AI. \u0022Just as food and cars are not released into the market without proper assurance of safety, so too AI systems should provide assurance to the public, government, and businesses that they are fit for purpose,\u0022 the Commerce Department said in a statement at the time. The United States is home to the biggest innovators in tech and AI - including Microsoft-backed OpenAI, which created ChatGPT - but trails internationally in regulating the industry. Google in March invited users in the United States and Britain to test its AI chatbot, known as Bard, as it continues on its gradual path to catch up with ChatGPT. Biden has urged Congress to pass laws putting stricter limits on the tech sector, but these efforts have little chance of making headway given political divisions among lawmakers. The lack of rules has given Silicon Valley freedom to put out new products rapidly - and stoked fears that AI technologies will wreak havoc on society before the government can catch up. Billionaire Elon Musk in early March formed an AI company called X.AI, based in the US state of Nevada, according to business documents. Musk, who is already the boss of Twitter and Tesla, is listed as a director of X.AI Corporation, a state business filing indicated. Musk\u0027s founding of what appears to be a rival to OpenAI came despite him recently joining tech leaders and AI critics in calling for an overall pause in the development of artificial intelligence. Google, Meta and Microsoft have spent years working on AI systems to help with translations, internet searches, security and targeted advertising. But late last year San Francisco firm OpenAI supercharged the interest in the AI sphere when it launched ChatGPT, a bot that can generate natural-seeming text responses from short prompts.",
    "publish_date": "2023-05-03 03:44:57"
  },
  {
    "title": "\u2018Godfather of AI\u2019 Geoffrey Hinton quits Google, warns of danger of technology to human way of life",
    "text": "The West Australian Perth Now Click to open navigation \u200C\u200C News Chevron Down Icon Breaking News Western Australia National World Technology Opinion Weather Sport Chevron Down Icon AFL Cricket Soccer Basketball Tennis NRL Rugby Motor Racing MMA Golf Netball Cycling Entertainment Chevron Down Icon Confidential Movies Television Music Reviews Books Competitions Business Chevron Down Icon Breaking News Economy Markets Property Commercial Property Workplace Matters Lifestyle Chevron Down Icon Food Personal Finance Health Parenting Fashion Travel Home \u0026 Garden Relationships Stars Real Estate HUH? Local News Chevron Down Icon North Central South Mandurah Competitions Find My Paper Digital Editions Read your local paperNews to your inbox Camera IconGeoffrey Hinton fears AI could change our way of life for the worse. Credit: The West Australian \u2018Godfather of AI\u2019 Geoffrey Hinton quits Google, warns of danger of technology to human way of life Luke AndrewsDaily Mail May 3, 2023 9:48AM Comments TopicsTechnologyBusinessLifestyleNewsWorld News The \u201CGodfather of artificial intelligence\u201D has sensationally resigned from Google and warned the technology could upend life as we know it. Geoffrey Hinton is credited with creating the technology that became the bedrock of AI systems such as ChatGPT and Google Bard. But the Turing prize winner now says a part of him regrets helping to make the systems, fearing it could prompt the proliferation of misinformation and replace people in the workforce. Mr Hinton said he had to tell himself excuses like \u201Cif I didn\u2019t build it, someone else would have\u201D to prevent himself from being overwhelmed by guilt. He drew comparisons with the \u201Cfather of the atomic bomb\u201D Robert Oppenheimer, who was reportedly distraught by his invention and dedicated the rest of his life to stopping its proliferation. Your cookie settings are preventing this third party content from displaying. If you\u2019d like to view this content, please adjust your Cookie Settings. To find out more about how we use cookies, please see our Cookie Guide. Speaking to the New York Times about his resignation, he warned that in the near future AI would flood the internet with false photos, videos and texts. These would be of a standard, he added, where the average person would \u201Cnot be able to know what is true anymore\u201D. The technology also posed a serious risk to \u201Cdrudge\u201D work and could upend the careers of people working as paralegals, personal assistants and translators. Some workers already say they are using it to cover multiple jobs for them, undertaking tasks such as creating marketing materials and transcribing Zoom meetings so that they do not have to listen. \u201CMaybe what is going on in these systems is actually a lot better than what is going on in the (human) brain,\u201D Mr Hinton said, explaining his fears. \u201CThe idea that this stuff could actually get smarter than people \u2014 a few people believed that. \u201CBut most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. \u201CObviously, I no longer think that.\u201DCamera Icon\u2018Godfather of AI\u2019 Geoffrey Hinton. Credit: Noah Berger/AP Asked about why he had helped develop a potentially dangerous technology, Mr Hinton said: \u201CI console myself with the normal excuse: If I hadn\u2019t done it, somebody else would have.\u201D He added that he had previously paraphrased Oppenheimer when posed with the question in the past, saying: \u201CWhen you see something that is technically sweet, you go ahead and do it.\u201D Mr Hinton decided to quit Google in April after a decade at the tech giant amid the proliferation of AI technologies. He had a long conversation with the chief executive of Google\u2019s parent company Alphabet, Sundar Pichai, before departing \u2014 although it is not clear what was said. In a broadside to his former employer, Mr Hinton accused Google of not being a \u201Cproper steward\u201D for AI technologies. In the past, the company has kept potentially dangerous technologies under wraps, he said. But it had now thrown caution to the wind as it competes with Microsoft, which added a ChatBot to its search engine Bing in April. TechnologyGuardrails and guidelines as AI embeds itself deeper TechnologyMusk, experts urge pause on training AI systems Google\u2019s chief scientist Jeff Dean said: \u201CWe remain committed to a responsible approach to AI. \u201CWe\u2019re continually learning to understand emerging risks while also innovating boldly.\u201D Mr Hinton\u2019s warning comes as Silicon Valley descends into a civil war over the advancement of artificial intelligence \u2014 with the world\u2019s greatest minds split over whether it will elevate or destroy humanity. Elon Musk, Apple co-founder Steve Wozniak and the late Stephen Hawking are among the most famous critics of AI who believe it poses a \u201Cprofound risk to society and humanity\u201D and could have \u201Ccatastrophic effects\u201D. They have called for a pause in the \u201Cdangerous race\u201D to roll out advanced AI, saying more risk assessments were needed. But Bill Gates, My Pichai and futurist Ray Kurzweil are on the other side of the debate, hailing the technology as \u201Cour time\u2019s most important innovation\u201D. They argue it could cure cancer, solve climate change and boost productivity. Mr Hinton has not previously added his voice to the debate, saying he did not want to speak out until he had formally left Google. He surged to fame in 2012 when at the University of Toronto in Canada, helping design a neural network that could analyse thousands of photos and teach itself to identify common objects such as flowers, dogs and cars. Mr Hinton started a company based on the technology, which was bought out by Google for $44 million. Advanced AI systems already available include ChatGPT, which now has more than a billion people signed up after its release in November.Camera IconWhat does the future hold for AI and humans? Credit: AAP Data shows that it also has as many as 100 million active monthly users. Launched by OpenAI, based in San Francisco, the platform has become an instant success worldwide. The chatbot is a large language model trained on massive text data, allowing it to generate eerily human-like text in response to a given prompt. The public uses ChatGPT to write research papers, books, news articles, emails and other text-based work and while many see it more like a virtual assistant, many brilliant minds see it as the end of humanity. If humans lose control of AI then it will be considered to have reached singularity, which means it has surpassed human intelligence and has independent thinking. AI would no longer need or listen to humans, allowing it to steal nuclear codes, create pandemics and spark world wars. \u201CIt\u2019s almost akin to a war between chimps and humans,\u201D DeepAI founder Kevin Baragona said. \u201CThe humans obviously win since we\u2019re far smarter and can leverage more advanced technology to defeat them. \u201CIf we\u2019re like the chimps, then the AI will destroy us or we\u2019ll become enslaved to it.\u201D Share to Facebook Share to Twitter Email Us Copy the Link Register and have your say. Register to comment Already have an account? Log in Find out moreThe first solo exhibition of world-renowned artist Yoshitomo Nara is now on at AGWA. From around the site \u2018Was coping poorly\u2019Celebrity chef\u2019s secret health battle revealed Comments Meow Overheard at Met Gala: BEST celeb gossip from event of year Comments Camera Icon SEE THE PICS: Who wore what on fashion\u2019s biggest night Wedding horrorBride killed by drunk driver hours after getting married Your Local News Share to TwitterShare to FacebookShare to InstagramShare to YoutubeEmail UsGet Digital Edition Find out moreThe first solo exhibition of world-renowned artist Yoshitomo Nara is now on at AGWA. Perth Now Email UsNewsletter Chevron Down IconSubmit story tip Camera IconSubmit photos Get Digital EditionDigital edition Chevron Down IconBack to top Follow Us Share to FacebookShare to TwitterShare to InstagramShare to YoutubeGet Digital EditionEmail Us About Us Contact Us Careers Letter to the Editor Place an Ad Privacy Policy Code of Conduct Editorial Complaints Terms of Use Cookie Settings \u00A9 West Australian Newspapers Limited 2023 News Sport Entertainment Business Lifestyle Privacy Policy Code of Conduct Editorial Complaints Terms of Use Cookie Settings Subscribe to the Paper Read your PerthNow Local Newspaper Advertise on PerthNow Our Partners The West Australian 7plus 7NEWS Car Expert Starts at 60 Raiz Invest Home Beautiful Better Homes \u0026 Gardens Marie Claire New Idea That\u2019s Life Beauty Crew SocietyOne HealthEngine iSeekplant Institchu Carbar",
    "publish_date": "2023-05-03 03:48:16"
  },
  {
    "title": "IBM to freeze hiring as CEO expects AI to replace 7,800 jobs",
    "text": "IBM will freeze hiring as it expects about 7,800 jobs to be replaced by Artificial Intelligence (AI) in the coming years, the tech giant\u2019s CEO has said. In an interview with Bloomberg News, IBM CEO Arvind Krishna said he could \u201Ceasily see\u201D nearly one-third of the company\u2019s non-customer-facing roles being replaced in the next five years. \u201CThese non-customer-facing roles amount to roughly 26,000 workers,\u201D Krishna said in the interview published on Tuesday. \u201CI could easily see 30 percent of that getting replaced by AI and automation over a five-year period.\u201D Back-office employees are only a small portion of IBM\u2019s 260,000 or so workers and the company, based in Armonk, New York, has continued to fill roles even after letting go of about 5,000 workers in other areas, according to Bloomberg. IBM did not immediately respond to Al Jazeera\u2019s request for comment. An IBM spokesperson told the AFP there was no blanket hiring pause in place but that the firm was being \u201Cvery selective when filling jobs that don\u2019t directly touch our clients or technology\u201D. Krishna\u2019s comments come as the rapid advancement of AI-power technology, including OpenAI\u2019s ChatGPT, raises the possibility of huge disruption to numerous industries. Some analysts fear that AI could soon lead to mass layoffs, while others argue the technology\u2019s ability to improve productivity and complement human workers will create jobs and whole new industries. On Tuesday, the share price of Chegg, a California-based learning company, plunged by nearly 50 percent after its CEO Dan Rosensweig acknowledged on an earnings call that ChatGPT was \u201Chaving an impact on our new customer growth rate\u201D.",
    "publish_date": "2023-05-03 04:27:15"
  },
  {
    "title": "Google, Microsoft CEOs called to AI meeting at White House",
    "text": "The chief executives of Alphabet Inc\u0027s Google, Microsoft, OpenAI and Anthropic will meet with Vice President Kamala Harris and top administration officials to discuss key artificial intelligence (AI) issues on Thursday, said a White House official. The invitation seen by Reuters to the CEOs noted President Joe Biden\u0027s \u0022expectation that companies like yours must make sure their products are safe before making them available to the public.\u0022 Concerns about fast-growing AI technology include privacy violations, bias and worries it could proliferate scams and misinformation. In April, Biden said it remains to be seen whether AI is dangerous but underscored that technology companies had a responsibility to ensure their products were safe. Social media had already illustrated the harm that powerful technologies can do without the right safeguards, he said. The administration has also been seeking public comments on proposed accountability measures for AI systems, as concerns grow about its impact on national security and education. On Monday, deputies from the White House Domestic Policy Council and White House Office of Science and Technology Policy wrote in a blog post about how the technology can pose a serious risk to workers. The Thursday meeting will be attended by Biden\u0027s Chief of Staff Jeff Zients, Deputy Chief of Staff Bruce Reed, National Security Adviser Jake Sullivan, Director of the National Economic Council Lael Brainard and Secretary of Commerce Gina Raimondo among others, said the White House official who did not wish to be named. The companies did not immediately respond to a request for comment.ChatGPT, an AI program that recently grabbed the public\u0027s attention for its ability to write answers quickly to a wide range of queries, in particular has attracted U.S. lawmakers\u0027 attention as it has grown to be the fastest-growing consumer application in history with more than 100 million monthly active users. \u0022I think we should be cautious with AI, and I think there should be some government oversight because it is a danger to the public,\u0022 Tesla Chief Executive Elon Musk said last month in a television interview.",
    "publish_date": "2023-05-03 05:29:46"
  },
  {
    "title": "Hoffman and Suleyman\u0026#8217;s AI startup Inflection launches ChatGPT-like chatbot",
    "text": "Inflection AI, the artificial intelligence startup founded by LinkedIn co-founder Reid Hoffman and Google DeepMind co-founder Mustafa Suleyman, has released its first AI chatbot product, the company said on Tuesday. Similar to OpenAI\u0027s viral chatbot ChatGPT, Inflection\u0027s AI chatbot, named Pi, uses generative AI technology to interact with users through dialogues, in which people can ask questions and share feedback. Suleyman, Inflection AI\u0027s CEO, said the startup developed the technology in-house and its Pi chatbot was built on prioritizing human-like conversations with a high level of emotional intelligence, including being kind and supportive. \u0022It\u0027s very balanced and even-handed on political issues or sensitive topics, but also sometimes it can be funny and silly and creative,\u0022 Suleyman said. The chatbot is suitable for personal day-to-day tasks, but not for generating code or essays, he added. Suleyman said the company had also spent time on boundary training to make sure the AI did not violate its behavior policies, including engaging in romantic conversations. \u0022The goal is to make sure that the AI always knows it\u0027s an AI and never tries to imitate a human. So it reminds the human user that it is an AI frequently,\u0022 he said. Users can interact with Pi across platforms including its website, app and social media platforms like Instagram. The service is free, and the startup may launch premium subscriptions in the future, Suleyman added. Pi uses user data, including conversational content, to train its AI systems, according to its terms of service. The chatbot is not currently connected to the internet. Chatbots powered by generative AI technology has become a crowded field since OpenAI\u0027s ChatGPT burst into the scene last November. Using large language models, which mine vast amounts of text to summarize information and generate content, chatbots like Google\u0027s Bard and Character. AI enable people to have in-depth conversations for both professional and personal needs. Founded in 2022, Inflection was incubated by VC firm Greylock, which led a $225 million investment in the startup. Co-founder Hoffman, a partner at Greylock who is also a Microsoft Corp board member, resigned from OpenAI\u0027s board in March, citing potential conflicts due to his work with AI startups.",
    "publish_date": "2023-05-03 05:39:27"
  },
  {
    "title": "ChatGPT creator says there\u2019s 50% chance AI ends in \u2018doom\u2019",
    "text": "One of the creators of ChatGPT has added to a growing chorus of researchers warning of the potentially catastrophic consequences of artificial intelligence development. Former OpenAI worker Paul Christiano, who now runs AI research non-profit Alignment Research Center, said he believed there was a significant chance that the technology would lead to the destruction of humanity. The main danger, he claimed, will come when AI systems reach and surpass the cognitive capacity of a human. Dr Christiano predicts there is a \u201C50/50 chance of doom\u201D once this moment arrives. \u201CI tend to imagine something like a year\u2019s transition from AI systems that are a pretty big deal, to kind of accelerating change, followed by further acceleration, et cetera,\u201D he told the Bankless podcast. \u201CI think once you have that view then a lot of things may feel like AI problems because they happen very shortly after you build AI.\u201D He added: \u201CThe most likely way we die involves \u2013 not AI comes out of the blue and kills everyone \u2013 but involves we have deployed a lot of AI everywhere... [And] if for some reason, God forbid, all these AI systems were trying to kill us, they would definitely kill us.\u201D The comments come amid increased concerns surrounding the rapid advancement of artificial intelligence in recent months, with the so-called godfather of AI Geoffrey Hinton quitting Google to sound the alarm about the dangers of AI. Speaking to The New York Times, he said he regretted the work that he had contributed to the field due to the unpredictable future we now face. \u201CThe idea that this stuff could actually get smarter than people \u2013 a few people believed that \u2013 but most people thought it was way off. And I thought it was way off. I thought it was 30-50 years or even longer away. Obviously I no longer think that,\u201D he said. \u201CI don\u2019t think they should scale this up more until they have understood whether they can control it.\u201D His stance has been praised by other researchers, with AnthropicAI\u2019s Catherine Olsson saying it may encourage others within the field to speak up. \u201CIn college I stopped eating meat, on the spot, when a friend asked why I hadn\u2019t yet. Social checks on our ethics can be so influential,\u201D she tweeted. \u201CI often think about when I would quit Anthropic or leave AI entirely. I encourage others to. I can already tell this move will influence me.\u201D Other prominent figures have also urged AI firms to pause development on advanced systems, most recently through an open letter signed by thousands of experts that urged governments to step in if artificial intelligence development was not paused for at least six months. Among the signatories was Elon Musk, who has frequently spoken about the existential threat posed by AI. The tech billionaire, who co-founded OpenAI, tweeted on Monday: \u201CEven benign dependency on AI/Automation is dangerous to civilization if taken so far that we eventually forget how the machines work.\u201D",
    "publish_date": "2023-05-03 07:11:01"
  },
  {
    "title": "Samsung bans AI chatbots after leak - The Korea Herald",
    "text": "Samsung Electronics joined other tech companies in banning the use of ChatGPT and other AI-powered chatbots by its employees, after discovering leaks of sensitive internal codes by its engineers. Taking measures to ban access of generative AI tools on company-owned computers, tablets and phones, Samsung is also reportedly creating its own service tools to support translation and the summarizing of documents, as well as software development. The tech giant confirmed Wednesday it issued a memo last week banning the use of generative AI tools to the staff of its Device eXperience division in charge of consumer appliances and mobile devices. \u0022Using generative AI tools on company PCs will be banned temporarily from May 1,\u0022 the note said, also asking employees to refrain from uploading anything related to the company, themselves or other employees on the AI chatbots while using their personal devices. \u201CWe ask that you diligently adhere to our security guideline and failure to do so may result in a breach or compromise of company information, resulting in disciplinary action up to and including termination of employment,\u201D Samsung added. Earlier in April, the company\u0027s Device Solution division overseeing its semiconductor business found three misuse cases where its engineers uploaded sensitive company information, including their meeting minutes and source codes, on ChatGPT for work. While it is unclear whether the sensitive information has been leaked to other users of the chatbot, the company immediately issued a notice to the division staff following the incident to not use generative AI tools for work. Acknowledging that staff were using external AI chatbots to increase the efficiency of their work, Samsung said it is developing an optimal AI tool for the staff to use in translation, summarizing documents and in source code development. The fear that ChatGPT and other similar chatbots operated by companies such as Microsoft and Google would leak sensitive company information to the public has prompted many companies to ban the use of AI tools. South Korean companies, SK hynix and Posco have also prohibited the use of AI services in the company. Amazon issued a similar warning to its employees and several major banks in the US, including JPMorgan Chase, Bank of America and Citigroup, also introduced similar measures. In a survey Samsung conducted of its DX division staff last month, 65 percent of respondents said they believe security risks can occur from using ChatGPT for work. ChatGPT, in its default mode, saves up the user\u0027s conversation history, and the inputs are used to improve or \u0022train\u0022 the AI and utilize the stored data to generate responses to inquiries made by other users. In response to the concerns, OpenAI, the company that developed the ChatGPT, said it added an \u0022incognito\u0022 mode under which the user can block their chats being used for training, last month. Other companies have been using the generative AI tools for their work, despite the security concerns. Goldman Sachs, which does restrict its employees from using ChatGPT at work, said its software developers still use generative AI tools to write and test code, though it did not reveal which service they use. IBM CEO Arvind Crishna also said in a recent interview that the company will suspend hiring for jobs that could be replaced with AI tools in the coming years.",
    "publish_date": "2023-05-03 07:16:00"
  },
  {
    "title": "Older generations trail the nation on AI know-how: Poll",
    "text": "Artificial intelligence has become wildly popular for many Americans, but people over the age of 45 are trailing those younger than them on AI familiarity, a Fox News poll shows. Fifty-eight percent of registered voters over the age of 45 who were surveyed for the poll say they are not familiar with AI technology such as OpenAI\u2019s ChatGPT. Only 41% of registered voters over 45 reported they are familiar with the technology. The figures stand in stark contrast to younger Americans, with a whopping 65% of registered voters under the age of 45 reporting they are familiar with AI tech, such as ChatGPT. Only 35% of that group reported that they are not familiar AI. The Fox News poll was conducted April 21-24 under the direction of Beacon Research and Shaw \u0026 Company Research among 1,004 registered voters nationwide. The voters were randomly selected and spoke with live interviewers on both landlines and cellphones. The poll has a sampling error of plus or minus 3 percentage points. FOX NEWS POLL: MORE SEE BAD THAN GOOD IN AI OpenAI\u0027s release of ChatGPT in November was a game-changer for artificial intelligence, becoming the fastest-growing user base with 100 million monthly active users in January. People across the world rushed to use the chatbot, which \u200B\u200Bsimulates human-like conversations based on prompts it is given. CLICK HERE FOR TOPLINE AND CROSS TABS The poll found that familiarity with AI technology like ChatGPT has a gender divide. Sixty percent of men reported being familiar with the technology, compared to 42% of women. Only 40% of male registered voters said they were unfamiliar with the technology, compared to 57% of women. REGULATE AI? GOP MUCH MORE SKEPTICAL THAN DEMS THAT GOVERNMENT CAN DO IT RIGHT: POLL Voters with college degrees were more likely to be familiar with AI tech at 57%, compared to 46% of voters with no college degree reporting the same. Voters across the board, by an 8-point margin, reported they believe AI is bad for society instead of good. Voters who were familiar with AI tech, however, were more likely to report artificial intelligence is good for society, at 46%. POLICE USING AI COULD LEAD TO \u2018PREDICTIVE\u2019 CRIME PREVENTION \u2018SLIPPERY SLOPE,\u2019 EXPERTS ARGUE Seventy-six percent of voters polled, including those familiar and unfamiliar with AI, reported that the government should regulate artificial intelligence, but 59% of the voters said they have no confidence or not much confidence the government can properly regulate it. CLICK HERE TO GET THE FOX NEWS APP \u0022Americans are unsure what to think of our new robot overlords,\u0022 Daron Shaw, a Republican who conducted the Fox News poll with Democrat Chris Anderson, said. \u0022They\u2019re skeptical elected leaders are up to the task of placing appropriate limits on this new tech, which probably says something about opinion on the tech and opinion on our leaders.\u0022",
    "publish_date": "2023-05-03 08:00:09"
  },
  {
    "title": "Have we ventured too far?",
    "text": "Last week, Geoffrey Hinton, the expatriate British computer scientist, in an interview with the New York Times (published on Monday), revealed his growing fear with developments in the field of artificial intelligence (AI). Hinton, who is referred to as the \u2018Godfather of AI\u2019 tendered his resignation as the head of the Google Brain research department last week, and joined an ever-expanding list of worried expert voices on the future direction of AI. Initially, Hinton studied experimental psychology at King\u2019s College, Cambridge. As a graduate student at the University of Edinburgh in 1972, Hinton supported the concept of a neural network, an idea which failed to attract the attention of many researchers. The development of the neural network, a mathematical system which learns skills by analyzing data, became his life\u2019s pursuit. After being awarded a PhD from Edinburgh in 1978, Hinton lectured in computer science at Carnegie Mellon University, in Pitts-burgh, USA. In the 1980s, most of the funding available for research in AI came from the US Department of Defense, and, thus, Hinton, who is fervently opposed to the application of artificial intelligence in war, moved to Canada. In 2018, Hinton and two of his colleagues at the University of Toronto, received the Turing Award \u2013 the \u2018Nobel Prize\u2019 of computing \u2013 for their research on neural networks. The trio created a neural network that taught itself to identify common objects such as dogs, cars, and flowers after analyzing thousands of photographs. Five years ago, tech companies such as Google, Microsoft and OpenAI started building neural networks which learned from enormous quantities of digital text called large language models (LLMs) to generate text on their own, including computer programs. This development assists computer programmers and writers to generate and execute ideas more quickly. However, experts have warned that LLMs can learn unwanted and unexpected behaviours, which can spawn false, biased and harmful information. These experts also note that as systems become more powerful they will introduce new risks. Hinton is clearly worried about what his life\u2019s work will generate from herein on. \u201CMaybe what is going on in these systems is a lot better than what is going on in the brain. Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That\u2019s scary,\u201D Hinton opined. So scary that after the San Francisco-based tech company OpenAI released a new version of ChatGPT in March, more than 1,000 (the number has since swollen to over 27,000 the New York Times reported on Monday) technology leaders and researchers signed an open letter calling for a six-month moratorium on the development of new systems because AI presented \u201Cprofound risks to society and humanity. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable,\u201D the letter read in part. A few days later, 19 current and former leaders of the Association for the Advancement of Artificial Intelligence, a 40-year-old academic society, added their voices of concern over the future of AI, in another letter. Now, having departed from Google after ten years, Hinton is heaping his concerns as well on the table. \u201CThe idea that this stuff could actually get smarter than people \u2014 a few people believed that,\u201D Hinton observed. \u201CBut most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\u201D Following up on the NY Times interview, the BBC News on Monday evening, asked Hinton to elaborate on what he meant when he said \u201Cbad players\u201D would try to use AI for \u201Cbad things\u201D. He responded, \u201CThis is just a worst-case scenario, kind of a nightmare scenario. You can imagine, some bad actor like [Russian President Vladimir] Putin decided to give robots the ability to create their own sub-goals.\u201D Worst-case scenario or not, the \u2018Godfather of AI\u2019 has painted a rather bleak picture of the weak grasp humanity has on the reins of control over the immediate direction and distance of AI. Hinton conceded, \u201DI console myself with the normal excuse: If I hadn\u2019t done it, somebody else would have.\u201D Now saddled with the stark reality of his life\u2019s work, Hinton\u2019s words sound like the echoes of a fellow scientist, the American theoretical physicist Julius Robert Oppenheimer, one of the fathers of the invention of the atomic bomb. Just 11 days after the bombing of Hiroshima, on August 17, 1945, Oppenheimer wrote to the US government expressing his wish for the banning of nuclear weapons. He is famously attributed with uttering a quote from the Bhagavad Gita, \u201CNow I become Death, the destroyer of worlds.\u201D Oppenheimer spent the latter decades of his life as a campaigner for nuclear disarmament. Hinton might not enjoy such an epoch, the genie is out of the bottle and it is too late to return it. Humanity now has another woe of its own creation on the horizon to accompany the ills of climate change.",
    "publish_date": "2023-05-03 08:01:07"
  },
  {
    "title": "LG CNS, Microsoft Korea bolster security business ties - The Korea Herald",
    "text": "Information technology service provider LG CNS said Wednesday it will accelerate business cooperation in the IT security sector with the Korean unit of US tech behemoth Microsoft. The Korean firm said it became Microsoft\u2019s managed security service provider, offering integrated security solutions from consulting to operations. Microsoft considers a firm\u2019s security business capabilities and growth potential to determine its MSSP partner qualification. The news came after the top management meeting between LG CNS Chief Executive Officer Hyun Shin-gyoon and Microsoft Executive Vice President and Chief Commercial Officer Judson Althoff at the US company\u2019s headquarters in Washington last month. The meeting\u0027s purpose was to strengthen the two sides\u2019 strategic cooperation in the fields of artificial intelligence and cloud-based digital transformation. Based on the close partnership, LG CNS looks to pursue multifaceted cooperation with Microsoft Korea. The company will design and build security architecture for generative artificial intelligence based on Azure OpenAI services such as ChatGPT. It will also develop a customized managed detection and response platform using Microsoft solutions to analyze and detect threat factors in advance and respond to them, while making aggressive inroads into the cloud security market, LG CNS said. Bae Min, vice president and head of the security and solution business division at LG CNS, said that the company has provided world-class services through its excellent human resources and a comprehensive security system by operating a cyber-control center. Especially since the beginning of this year, LG CNS has increasingly focused on strengthening its security and solutions business. \u201CThrough cooperation with Microsoft, we will further advance our security business and solidify our position as a \u2018leading security company\u2019 that innovates customer experience,\u201D Bae said.",
    "publish_date": "2023-05-03 08:32:59"
  },
  {
    "title": "\u0022He Knows What He\u0027s Talking About\u0022: Elon Musk On AI \u0027Godfather\u0027 And His Warning",
    "text": "Tesla owner and billionaire Elon Musk has backed Geoffrey Hinton, known as the \u0027Godfather of AI\u0027. In a tweet on Tuesday, Mr Musk said the 75-year-old, who has warned about AI chatbots, \u0022knows what he is talking about\u0022. Mr Hinton, who quit Google recently, told the New York Times (NYT) that he did this to speak freely about the technology\u0027s dangers, after realising that computers could become smarter than people far sooner than he and other experts had expected. Mr Hinton nurtured the technology for decades before realising that it will cause serious harm. \u0022Hinton knows what he\u0027s talking about,\u0022 Mr Musk said in his tweet, in response to a news article on the topic. Hinton knows what he\u0027s talking about\u2014 Elon Musk (@elonmusk) May 2, 2023 In the interview with NYT, Mr Hinton said he was worried about AI\u0027s capacity to create convincing false images and texts, creating a world where people will \u0022not be able to know what is true anymore\u0022. \u0022It is hard to see how you can prevent the bad actors from using it for bad things,\u0022 he said. There are fears that the technology could quickly displace workers, and become a greater danger as it learns new behaviours. \u0022The idea that this stuff could actually get smarter than people - a few people believed that,\u0022 said Mr Hinton. \u0022But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\u0022 Mr Musk too has been expressing concern about the rapidly evolving artificial intelligence systems like ChatGPT. Reacting to a conversation a reporter had with the chatbot, in which he called itself \u0022perfect\u0022, the SpaceX CEO said it \u0022sounds eerily like the AI in System Shock that goes haywire and kills everyone\u0022. AI was already gaining prominence, touching the everyday lives of several people after being deployed by companies like e-commerce platforms. But after the launch of OpenAI\u0027s ChatGPT, there has been a huge increase in human interaction, with users asking it to write essays, speeches and even solve exam questions.",
    "publish_date": "2023-05-03 09:14:59"
  },
  {
    "title": "A CEO Is Spending Rs 2 Lakh A Month On ChatGPT For All His Employees",
    "text": "Artificial intelligence has been rapidly evolving over the last few months, especially since the launch of ChatGPT. OpenAI\u0027s chatbot is being widely used with users asking it to write essays, speeches and even solve exam questions. There are fears that AI will take away millions of jobs and many tech entrepreneurs, including Elon Musk, have raised a voice against its spread. But Akash Nigam, the CEO of start-up Genies, has opted to spend $2,400 (Rs 1.96 lakh) a month on ChatGPT Plus subscriptions for every employee at the company, according to Insider. Since March, Mr Nigam has pushed all the departments of his company -Engineering, Product, Finance, Design, R\u0026D and Accounting - to familiarise themselves with ChatGPT to increase productivity, the outlet further said. \u0022I\u0027m a pretty frugal, stingy person. But in my mind, this is for the health and growth of the company,\u0022 he told Insider. Mr Nigam said he has already seen many tasks get accelerated after all 120 employees of his company started using the AI system. Genies\u0027 various departments use ChatGPT to make day-to-day tasks easy. The R\u0026D group, for example, has used the AI-powered chatbot to solve math and coding problems and create presentation scripts. The staff members have also used it to brainstorm ideas for projects, draft legal documents like corporate regulations, and research technological issues. Genies has also been organising unofficial workshops where employees who are pro in using ChatGPT teach their less experienced colleagues how to use AI to complete various tasks. While there are concerns around the rapid spread of AI, a recent research funded by Stanford University and Massachusetts Institute of Technology found that 14 per cent of employees who used ChatGPT in their workflow saw an increase in productivity. The study also found that the least experienced and least skilled workers were able to complete tasks 35 per cent faster.",
    "publish_date": "2023-05-03 08:21:00"
  },
  {
    "title": "AI\u9A5A\u52D5\u767D\u5BAE\uFF01\u7F8E\u570B\u51FA\u624B\u53ECOpenAI\u3001Google\u3001\u5FAE\u8EDF\u958B\u300C\u570B\u5B89\u6703\u8B70\u300D",
    "text": "\u5373\u6642 \u71B1\u9580 \u653F\u6CBB \u8ECD\u6B66 \u793E\u6703 \u751F\u6D3B \u5065\u5EB7 \u570B\u969B \u5730\u65B9 \u8490\u5947 \u5F71\u97F3 \u8CA1\u7D93 \u5A1B\u6A02 \u6C7D\u8ECA \u6642\u5C1A \u9AD4\u80B2 3 C \u8A55\u8AD6 \u85DD\u6587 \u73A9\u5496 \u98DF\u8B5C \u5730\u7522 \u5C08\u5340 TAIPEI TIMES \u6C42\u8077 \u7206 Search \u81EA\u7531\u96FB\u5B50\u5831 3C\u79D1\u6280 3C\u9996\u9801 \u5F71\u97F3\u5C08\u5340 \u667A\u6167\u624B\u6A5F \u5BE6\u7528\u79D8\u6280 \u79D1\u6280\u8DA3\u805E \u7DB2\u8DEF\u793E\u7FA4 \u597D\u651D\u76F8\u6A5F \u96FB\u8166\u61C9\u7528 \u5BB6\u96FB\u5A1B\u6A02 \u7C89\u7D72\u5718 \u81EA\u7531\u5F71\u97F3 \u5373\u6642 \u71B1\u9580 \u653F\u6CBB \u8ECD\u6B66 \u793E\u6703 \u751F\u6D3B \u5065\u5EB7 \u570B\u969B \u5730\u65B9 \u8490\u5947 \u8CA1\u7D93 \u5A1B\u6A02 \u85DD\u6587 \u6C7D\u8ECA \u6642\u5C1A \u9AD4\u80B2 3 C \u8A55\u8AD6 \u73A9\u5496 \u98DF\u8B5C \u5730\u7522 \u5C08\u5340 \u670D\u52D9 \u81EA\u7531\u96FB\u5B50\u5831APP \u81EA\u7531\u96FB\u5B50\u5831\u7C89\u7D72\u5718 \u81EA\u7531\u96FB\u5B50\u5831Line \u81EA\u7531\u96FB\u5B50\u5831Twitter \u71B1\u9580\u65B0\u8A0A \u5DF2\u7D93\u52A0\u597D\u53CB\u4E86\uFF0C\u8B1D\u8B1D \u6B61\u8FCE\u52A0\u5165\u3010\u81EA\u75313C\u79D1\u6280\u3011 \u6309\u500B\u8B9A\u3000\u5FC3\u60C5\u597D \u5DF2\u7D93\u6309\u8B9A\u4E86\uFF0C\u8B1D\u8B1D\u3002 3C \u3009 \u7522\u696D\u52D5\u614B \u3009 \u7522\u696D\u65B0\u805E AI\u9A5A\u52D5\u767D\u5BAE\uFF01\u7F8E\u570B\u51FA\u624B\u53ECOpenAI\u3001Google\u3001\u5FAE\u8EDF\u958B\u300C\u570B\u5B89\u6703\u8B70\u300D 2023/05/03 15:19 \u6587\uFF0F\u8A18\u8005\u5433\u4F69\u6A3A AI\u767C\u5C55\u5C0D\u793E\u6703\u7684\u885D\u64CA\uFF0C\u4E5F\u8B93\u7F8E\u653F\u5E9C\u76F8\u7576\u91CD\u8996\u3002\uFF08\u5716\uFF0F\u7F8E\u806F\u793E\uFF09 \u79D1\u6280\u754C\u5927\u6253AI\u6230\uFF0C\u751F\u6210\u5F0FAI\u5728\u5168\u7403\u6380\u8D77\u71B1\u6F6E\uFF0C\u4F46\u4E5F\u5F15\u767C\u76F8\u7576\u5927\u7684\u722D\u8B70\uFF0C\u9664\u4E86\u5047\u65B0\u805E\u3001\u8CC7\u5B89\u7B49\uFF0C\u9084\u6709\u5931\u696D\u554F\u984C\u66F4\u662F\u8B93\u4EBA\u64D4\u6182\uFF0C\u8DEF\u900F\u793E\u5831\u5C0E\uFF0C\u7F8E\u570B\u767D\u5BAE\u5DF2\u7D93\u51FA\u624B\u53EC\u96C6\u5404\u5927\u79D1\u6280\u5DE8\u982D\u53EC\u958BAI\u6703\u8B70\u3002 \u7F8E\u570B\u5C07\u5728\u672C\u9031\u56DB(\u7F8E\u570B\u6642\u95935\u67084\u65E5)\u96C6\u7D50OpenAI\u3001Google\u3001\u5FAE\u8EDF\u3001Anthropic\u7B49\u516C\u53F8CEO\uFF0C\u4E00\u540C\u53C3\u8207AI\u6703\u8B70\uFF0C\u8A72\u6703\u8B70\u5C07\u7531\u7F8E\u570B\u526F\u7E3D\u7D71\u8CC0\u9326\u9E97\u3001\u767D\u5BAE\u5E55\u50DA\u9577Jeff Zients\u4EE5\u53CA\u653F\u5E9C\u9AD8\u5C64\u8DDF\u5404\u5927CEO\u6703\u9762\u3002 \u8ACB\u7E7C\u7E8C\u5F80\u4E0B\u95B1\u8B80... \u96D6\u7136\u6C92\u6709\u660E\u78BA\u6307\u51FA\uFF0C\u9019\u5834AI\u6703\u8B70\u7684\u4E3B\u8EF8\u70BA\u4F55\uFF0C\u4F46\u5916\u754C\u8A8D\u70BA\u5C07\u570D\u7E5E\u5728AI\u53EF\u80FD\u5E36\u4F86\u7684\u793E\u6703\u885D\u64CA\u3002\u64DA\u6089\uFF0C\u7F8E\u7E3D\u7D71\u62DC\u767B\u5C0D\u79D1\u6280\u516C\u53F8\u7684\u671F\u671B\u662F\u300C\u5411\u5927\u773E\u63D0\u4F9B\u7522\u54C1\u4E4B\u524D\uFF0C\u5FC5\u9808\u78BA\u4FDD\u7522\u54C1\u662F\u5B89\u5168\u7684\u3002\u300D \u6B64\u5916\uFF0C\u88AB\u8B7D\u70BA\u300CAI\u6559\u7236\u300D\u7684Geoffrey Hinton\u4E5F\u6295\u4E0B\u9707\u61BE\u5F48\uFF0C\u5728\u8FD1\u65E5\u8FAD\u53BBGoogle \u526F\u7E3D\u88C1\u7684\u8077\u52D9\uFF0C\u539F\u56E0\u662F\u64D4\u6182AI\u767C\u5C55\u6050\u5C0D\u672A\u4F86\u4EBA\u985E\u5E36\u4F86\u4E0D\u5229\u5F71\u97FF\u3002 \u300A\u4F60\u53EF\u80FD\u9084\u60F3\u770B\u300B Google\u300C\u6559\u7236\u7D1A\u300DAI \u5927\u795E\u9707\u64BC\u96E2\u8077 \uFF01\u64D4\u6182\uFF1A\u4EBA\u5DE5\u667A\u6167\u6703\u6BD4\u4EBA\u985E\u66F4\u8070\u660E \u4E0D\u7528\u62BD \u4E0D\u7528\u6436 \u73FE\u5728\u7528APP\u770B\u65B0\u805E \u4FDD\u8B49\u5929\u5929\u4E2D\u734E\u3000 \u9EDE\u6211\u4E0B\u8F09APP\u3000 \u6309\u6211\u770B\u6D3B\u52D5\u8FA6\u6CD5 AI \u767D\u5BAE \u7F8E\u570B OpenAI Google \u5FAE\u8EDF \u570B\u5B89 \u6703\u8B70 \u7522\u696D\u65B0\u805E \u76F8\u95DC\u65B0\u805E Google Pixel 7a\u5B98\u65B9\u5BA3\u50B3\u7D20\u6750\u5927\u91CF\u6D41\u51FA\uFF01\u4E00\u5F35\u8868\u770B\u61C25\u5927\u5347\u7D1A Google\u65B0\u4EE3\u4E2D\u968EPixel 7\u9810\u8A08\u5728\u4E0B\u9031\u7684Google I\uFF0FO\u5927\u6703\u4E0A\u767C\u8868\uFF0C\u5916\u89C0\u548C\u4E3B\u8981\u898F\u683C\u5DF2\u7D93\u66DD\u5149\u7684\u5DEE\u4E0D\u591A\uFF0C\u73FE\u5728\u5C31\u9023\u5B98...... \u860B\u679C\u3001Google \u7F55\u898B\u806F\u624B\u300C\u8A02\u898F\u5247\u300D\uFF01\u539F\u56E0\u6050\u662F AirTag \u6380\u8D77\u7684\u96B1\u79C1\u722D\u8B70 \u860B\u679C\u3001Google \u7F55\u898B\u806F\u624B\uFF01\u800C\u6700\u5927\u539F\u56E0\u6307\u5411\u860B\u679C\u9632\u4E1F\u5668 AirTag\uFF0C\u5728\u696D\u754C\u5439\u8D77\u6D6A\u6F6E\u5F8C\uFF0C\u537B\u5C62\u6B21\u906D\u5230\u6709\u5FC3\u4EBA\u58EB\u5229\u7528\u5728\u8FFD\u8E64\u3001\u5077...... \u679C\u7C89\u807D\u4E86\u68EE77\uFF01iPhone\u906D\u4E2D\u570B\u624B\u6A5FCEO\u72E0\u6279\u4E09\u5927\u7F3A\u9EDE \u5F9E\u83EF\u70BA\u812B\u96E2\u7684\u69AE\u8000Honor\uFF0C\u9010\u6F38\u6210\u70BA\u4E2D\u570B\u624B\u6A5F\u54C1\u724C\u7684\u65B0\u6A19\u7AFF\uFF0C\u64DA\u4E2D\u570B\u5A92\u9AD4\u5831\u5C0E\uFF0C\u5728\u65E5\u524D\u8209\u884C\u7684Honor\u6DF1\u5733\u7814\u767C\u5BE6\u9A57\u5BA4\u958B\u653E\u65E5\uFF0CH...... \u71B1\u9580\u6587\u7AE0 Sony\u5931\u5B88\u3001\u5C0F\u7C73\u8DCC\u51FA\u699C\u5916\uFF01\u53F0\u7063\u624B\u6A5F\u5341\u5927\u54C1\u724C\u6392\u884C\u699C\u55AE\u300C\u9ED1\u99AC\u300D\u7AC4\u51FA \u5FA0\u5361\u4E5F\u6551\u4E0D\u4E86\u5C0F\u7C73\uFF01\u5168\u74035\u5927\u624B\u6A5F\u54C1\u724C\u6700\u65B0\u51FA\u8CA8\u91CF\u63ED\u66C9 \u6BD4\u624B\u6A5F\u66F4\u4FBF\u5B9C\uFF01\u83EF\u78A9\u904A\u6232\u638C\u6A5F ROG Ally\u300C\u5E73\u50F9\u6B3E\u552E\u50F9\u300D\u66DD\u5149\u3000\u73A9\u5BB6\u55E8\u7206 \u4E0B\u6708\u767C\u65B0\u65D7\u8266\uFF01Sony\u96C6\u5718\u793E\u9577\u537B\u300C\u60B2\u89C0\u300D\u770B\u5168\u7403\u624B\u6A5F\u5E02\u5834 HTC \u9996\u5EA6\u5347\u7D1A 1 \u5104\u756B\u7D20\uFF01\u672A\u4E0A\u5E02\u300C\u4E2D\u9AD8\u968E\u65B0\u6A5F\u300D\u771F\u5BE6\u7167\u7247\u73FE\u8EAB HTC \u672A\u4E0A\u5E02\u300C\u4E2D\u9AD8\u968E\u65B0\u6A5F\u300D\u9996\u6B21\u73FE\u8EAB\u8DD1\u5206\u5E73\u53F0\uFF01\u8DD1\u5206\u6210\u7E3E\u3001\u578B\u865F\u7686\u66DD\u5149 \u770B\u66F4\u591A\uFF01\u52A0\u51653C\u79D1\u6280\u7C89\u7D72\u5718 \u7DB2\u53CB\u56DE\u61C9",
    "publish_date": "2023-05-03 09:19:59"
  }
]